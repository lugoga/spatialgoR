[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Geospatial Technology and Spatial Analysis in R",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Geospatial Technology and Spatial Analysis in R",
    "section": "Welcome",
    "text": "Welcome\nWelcome to the world of spatial data and analysis in R! In today’s data-driven era, the importance of spatial information cannot be overstated. From urban planning and environmental management to transportation logistics and public health, the ability to understand and analyze spatial data has become essential for making informed decisions and solving complex problems.\nThis book serves as your comprehensive guide to harnessing the power of R for working with spatial data. Whether you are a student, researcher, data scientist, or professional in a related field, this resource will equip you with the knowledge and skills to effectively manipulate, visualize, analyze, and model spatial data using R.\nR, a widely used programming language for statistical computing and graphics, offers a rich set of packages and tools specifically tailored for spatial data analysis. With its extensive capabilities, R provides a versatile and efficient environment for working with diverse spatial datasets, ranging from point patterns and geospatial polygons to raster data and satellite imagery.\nIn this book, we will start by introducing the fundamentals of spatial data and its representation in R. You will learn about spatial objects, coordinate reference systems, and the various data structures used to handle spatial information efficiently. We will explore techniques for importing, exporting, and manipulating spatial data, ensuring that you have a solid foundation for subsequent analysis.\nThe core of this book revolves around spatial analysis techniques in R. We will delve into essential topics such as spatial data visualization, spatial statistics, spatial interpolation, spatial regression, and geostatistics. Through step-by-step tutorials, code examples, and real-world applications, you will gain a deep understanding of how to apply these techniques to uncover patterns, explore relationships, and derive meaningful insights from spatial data.\nAdditionally, we will explore advanced topics such as spatial data modeling, spatial clustering, and spatial optimization, which will enable you to tackle complex spatial problems and make informed decisions. Throughout the book, we will emphasize the importance of sound statistical principles and best practices for spatial data analysis.\nEach chapter is carefully crafted to provide a balance between theoretical concepts, practical demonstrations, and hands-on exercises. The code snippets and reproducible examples provided will help you to grasp the concepts effectively and apply them to your own spatial analysis tasks. We encourage you to actively engage with the material, experiment with the code, and explore additional resources to deepen your understanding.\nFinally, we would like to express our sincere appreciation to the open-source community, which has contributed immensely to the development of R and its spatial packages. We are grateful to the countless individuals who have shared their knowledge, expertise, and code, making spatial data analysis in R accessible and empowering users around the world.\nWe hope that this book will serve as your valuable companion in your journey through the fascinating realm of spatial data and analysis in R. By the end, we aim to equip you with the skills and confidence to unlock the hidden insights in spatial data and contribute to the advancement of knowledge in your respective domains.\nLet’s embark on this exciting adventure together and explore the vast possibilities of spatial data analysis in R!\nHappy spatial coding!\nMasumbuko SEMBA"
  },
  {
    "objectID": "index.html#what-is-r",
    "href": "index.html#what-is-r",
    "title": "Geospatial Technology and Spatial Analysis in R",
    "section": "What is R?",
    "text": "What is R?\nR is a programming language and environment, originally developed for statistical computing and graphics. As of October 2020, there are ~16,000 R packages in the official repository CRAN2.\nNotable advantages of R are that it is a full-featured programming language, yet customized for working with data, relatively simple and has a huge collection of over 100,000 functions from various areas of interest.\nR's popularity has been steadily increasing in recent years (Figures 0.1–0.3).\nFigure 0.1: Stack Overflow Trend for the 'r' question tag (https://insights.stackoverflow.com/trends?tags=r)\nFigure 0.2: IEEE Language Rankings 2019 (https://spectrum.ieee.org/computing/software/the-top-programming-languages-2019)\nFigure 0.3: Proportion of research papers citing R (https://www.nature.com/news/programming-tools-adventures-with-r-1.16609)\nA brief overview of the capabilities and packages for several domains of R use, are available in the \"CRAN Task Views\" (Figure 0.4).\nFigure 0.4: CRAN Task Views (http://www.maths.lancs.ac.uk/~rowlings/R/TaskViews/)"
  },
  {
    "objectID": "index.html#r-and-analysis-of-spatial-data",
    "href": "index.html#r-and-analysis-of-spatial-data",
    "title": "Geospatial Technology and Spatial Analysis in R",
    "section": "R and analysis of spatial data",
    "text": "R and analysis of spatial data\n\nIntroduction\nOver time, there was an increasing number of contributed packages for handling and analyzing spatial data in R. Today, spatial analysis is a major functionality in R. As of October 2020, there are at least 185 packages3 specifically addressing spatial analysis in R.\nFigure 0.5: Books on Spatial Data Analysis with R\nSome important events in the history of spatial analysis support in R are summarized in Table 0.1.\n\n\n\n\nTable 0.1: Significant events in the history of R-spatial\n\n\nYear\nEvent\n\n\npre-2003\nVariable and incomplete approaches (MASS, spatstat, maptools, geoR, splancs, gstat, …)\n\n\n2003\nConsensus that a package defining standard data structures should be useful; rgdal released on CRAN\n\n\n2005\nsp released on CRAN; sp support in rgdal (Section 7.1.3\n\n\n2008\nApplied Spatial Data Analysis with R, 1st ed.\n\n\n2010\nraster released on CRAN (Section 5.3.4)\n\n\n2011\nrgeos released on CRAN\n\n\n2013\nApplied Spatial Data Analysis with R, 2nd ed.\n\n\n2016\nsf released on CRAN (Section 7.1.4)\n\n\n2018\nstars released on CRAN (Section 5.3.5)\n\n\n2019\nGeocomputation with R (https://geocompr.robinlovelace.net/)\n\n\n2021(?)\nSpatial Data Science (https://www.r-spatial.org/book/)\n\n\n\nThe question that arises here is: can R be used as a Geographic Information System (GIS), or as a comprehensive toolbox for doing spatial analysis? The answer is definitely yes. Moreover, R has some important advantages over traditional approaches to GIS, i.e., software with graphical user interfaces such as ArcGIS or QGIS.\nGeneral advantages of Command Line Interface (CLI) software include:\n\nAutomation—Doing otherwise unfeasible repetitive tasks\nReproducibility—Precise control of instructions to the computer\n\nMoreover, specific strengths of R as a GIS are:\n\nR capabilities in data processing and visualization, combined with dedicated packages for spatial data\nA single environment encompassing all analysis aspects—acquiring data, computation, statistics, visualization, Web, etc.\n\nNevertheless, there are situations when other tools are needed:\n\nInteractive editing or georeferencing (but see mapedit package)\nUnique GIS algorithms (3D analysis, label placement, splitting lines at intersections)\nData that cannot fit in RAM (but R can connect to spatial databases4 and other softwere for working with big data)\n\nThe following sections (0.3.2–0.3.11) highlight some of the capabilities of spatial data analysis packages in R, through short examples. We are going to elaborate on most of these packages later on in the book, and many of those examples will become clear.\n\n\nInput and output of spatial data\nReading spatial layers from a file into an R data structure, or writing the R data structure into a file, are handled by external libraries:\n\nGDAL/OGR is used for reading/writing vector and raster files, with sf and stars\nPROJ is used for handling Coordinate Reference Systems (CRS), in both sf and stars\nWorking with specialized formats, e.g., NetCDF with ncdf4\n\nPackage sf combined with RPostgreSQL can be used to read from, and write to, a PostGIS spatial database:\nlibrary(sf) library(RPostgreSQL) con = dbConnect(   PostgreSQL(),   dbname = \"gisdb\",   host = \"159.89.13.241\",   port = 5432,   user = \"geobgu\",   password = \"*******\" ) dat = st_read(con, query = \"SELECT name_lat, geometry FROM plants LIMIT 5;\")\ndat ## Simple feature collection with 5 features and 1 field ## Geometry type: POINT ## Dimension:     XY ## Bounding box:  xmin: 35.1397 ymin: 31.44711 xmax: 35.67976 ymax: 32.77013 ## Geodetic CRS:  WGS 84 ##         name_lat                  geometry ## 1    Iris haynei POINT (35.67976 32.77013) ## 2    Iris haynei   POINT (35.654 32.74137) ## 3 Iris atrofusca POINT (35.19337 31.44711) ## 4 Iris atrofusca POINT (35.18914 31.51475) ## 5  Iris vartanii  POINT (35.1397 31.47415)\n###sf: Processing Vector Layers {.unnumbered}\nGEOS is used for geometric operations on vector layers with sf:\n\nNumeric operators—Area, Length, Distance…\nLogical operators—Contains, Within, Within distance, Crosses, Overlaps, Equals, Intersects, Disjoint, Touches…\nGeometry generating operators—Centroid, Buffer, Intersection, Union, Difference, Convex-Hull, Simplification…\n\nFigure 0.6: Buffer function\n###stars: Processing Rasters {.unnumbered}\nGeometric operations on rasters can be done with package stars:\n\nAccessing cell values—As matrix / array, Extracting to points / lines / polygons\nRaster algebra—Arithmetic (+, -, …), Math (sqrt, log10, …), logical (!, ==, &gt;, …), summary (mean, max, …), Masking\nChanging resolution and extent—Cropping, Mosaic, Resampling, Reprojection\nTransformations—Raster &lt;-&gt; Points / Contour lines / Polygons\n\n\n\ngeosphere: Geometric calculations on longitude/latitude\nPackage geosphere implements spherical geometry functions for distance- and direction-related calculations on geographic coordinates (lon-lat).\nFigure 0.7: Points on Great Circle\nFigure 0.8: Visualizing Facebook Friends with geosphere (http://paulbutler.org/archives/visualizing-facebook-friends/)\n\n\ngstat: Geostatistical Modelling\nAs mentioned above, R was initially developed for statistical computing (Section 0.2). Accordingly, there is an extensive set of R packages for spatial statistics. For example, package gstat provides a comprehensive set of functions for univariate and multivariate geostatistics, mainly for the purpose of spatial interpolation:\n\nVariogram modelling\nOrdinary and universal point or block (co)kriging\nCross-validation\n\nFigure 0.9: Predicted Zinc concentration, using Ordinary Kriging\nWe are going to learn about the gstat package in Chapter 12. An introduction to the package can also be found in Chapter 8 of Applied Spatial Data Analysis with R (Bivand, Pebesma, and Gomez-Rubio 2013).\n\n\nspdep: Spatial dependence modelling\nModelling with spatial weights:\n\nBuilding neighbor lists and spatial weights\nTests for spatial autocorrelation for areal data (e.g., Moran's I)\nSpatial regression models (e.g., SAR, CAR)\n\nFigure 0.10: Neighbours list based on regions with contiguous boundaries\nThe spdep package is beyond the scope of this book. An introduction to the package can be found in Chapter 9 of Applied Spatial Data Analysis with R (Bivand, Pebesma, and Gomez-Rubio 2013).\n\n\nspatstat: Spatial point pattern analysis\nPackage spatstat provides a comprehensive collection of techniques for statistical analysis of spatial point patterns, such as:\n\nKernel density estimation\nDetection of clustering using Ripley's K-function\nSpatial logistic regression\n\nFigure 0.11: Distance map for the Biological Cells point pattern dataset\nThe book Spatial point patterns: methodology and applications with R (Baddeley, Rubak, and Turner 2015) provides a thorough introduction to the subject of point pattern analysis using the spatstat package. A more brief introduction can also be found in Chapter 7 of Applied Spatial Data Analysis with R (Bivand, Pebesma, and Gomez-Rubio 2013).\n\n\nosmdata: Access to OpenStreetMap data\nPackage osmdata gives access to OpenStreetMap (OSM) data—the most extensive open-source map database in the worls—using the Overpass API5.\nlibrary(sf) library(osmdata) q = opq(bbox = \"Beer-Sheva, Israel\") q = add_osm_feature(q, key = \"highway\") dat = osmdata_sf(q) lines = dat$osm_lines pol = dat$osm_polygons pol = st_cast(pol, \"MULTILINESTRING\") pol = st_cast(pol, \"LINESTRING\") lines = rbind(lines, pol) lines = lines[, \"highway\"] lines = st_transform(lines, 32636) plot(lines, key.pos = 4, key.width = lcm(4), main = \"\")\nFigure 0.12: Beer-Sheva road types map, using data downloaded from OpenStreetMap (OSM)\n\n\nggplot2: Visualization\nThe ggplot2 package is one of the most popular packages in R. It provides advanced visualization methods through a well-designed and consistent syntax. The package supports visualization of both vector layers6 and rasters7.\nThe ggplot2 package is highly customizable and capable of producing publication-quality figures and maps as well as original and innovative designs (Figure 0.13). One of its strengths is in easy preparation of \"small-multiple\"—or facet, in the terminology of ggplot2—figures (Figure 0.14).\nFigure 0.13: London cycle hire journeys with ggplot2 (http://spatial.ly/2012/02/great-maps-ggplot2/)\nFigure 0.14: Crime density by day with ggplot2\nThe ggplot2 package is beyond the scope of this book. A good place to start is the book ggplot2: Elegant Graphics for Data Analysis, by package author (Wickham 2016). The book is available online8.\n\n\nleaflet, mapview: Web mapping\nPackages leaflet and mapview provide methods to produce interactive maps using the Leaflet JavaScript library.\nPackage leaflet gives more low-level control. Package mapview is a wrapper around leaflet, automating addition of useful features:\n\nCommonly used basemaps\nColor scales and legends\nLabels\nPopups\n\nFunction mapview produces an interactive map given a spatial object. The zcol parameter is used to specify the attribute used for symbology:\nlibrary(sf) library(mapview) states = st_read(\"USA_2_GADM_fips.shp\") mapview(states, zcol = \"NAME_1\")\nFigure 0.15: Intractive map made with mapview"
  },
  {
    "objectID": "index.html#other-materials",
    "href": "index.html#other-materials",
    "title": "Geospatial Technology and Spatial Analysis in R",
    "section": "Other materials",
    "text": "Other materials\nThis section lists some other resources that are relevant for working with spatial data in R.\n\nBooks\n\nModel-based Geostatistics (Diggle and Ribeiro 2007)\nA Practical Guide to Geostatistical Mapping (Hengl 2009)\nSpatial Data Analysis in Ecology and Agriculture using R (1st ed. 2012, 2nd ed. 2018) (Plant 2018)\nLearning R for Geospatial Analysis (Dorman 2014)\nApplied Spatial Data Analysis with R (1st ed. 2008, 2nd ed. 2013) (Bivand, Pebesma, and Gomez-Rubio 2013)\nHierarchical Modeling and Analysis for Spatial Data (1st ed. 2003, 2nd ed. 2014) (Banerjee, Carlin, and Gelfand 2014)\nAn Introduction to R for Spatial Analysis and Mapping (1st ed. 2015, 2nd ed. 2018) (Brunsdon and Comber 2015)\nSpatial Point Patterns: Methodology and Applications with R (2015) (Baddeley, Rubak, and Turner 2015)\nDisplaying Time Series, Spatial, and Space-Time Data with R (1st ed. 2014, 2nd ed. 2018) (Lamigueiro 2014)\nPredictive Soil Mapping with R (Hengl and MacMillan 2019)\nGeocomputation with R (Lovelace, Nowosad, and Muenchow 2019)\nSpatial Data Science (2021?)\n\n\n\nPapers\n\nPebesma, E., Bivand, R. S. (2005). Classes and Methods for Spatial Data: the sp Package. R news, 5(2), 9-13. [PDF]\nPebesma, E. (2018). Simple Features for R: Standardized Support for Spatial Vector Data. The R Journal, 10(1):439-446. [PDF]\n\n\n\nCourses and tutorials\n\nCourses\n\nGEOG 4/595: Geographic Data Analysis\nCP6521 Advanced GIS\nES214 Introduction to GIS and Spatial Analysis\nGEOG 4/590: R for Earth-System Science\nGEOG 4/595: Geographic Data Analysis\nSpatial Data Science with R (Robert J. Hijmans)\nIntroduction to Spatial Data Programming with R (this course)\nGISC 422 Spatial Analysis and Modelling\nCASA0005 Geographic Information Systems and Science\nAnother list here\n\n\n\nTutorials\n\nGeospatial Data Science with R\nData Carpentry Workshops\nGIS in R (Nick Eubank)\nNEON Data Tutorials\nLearn Spatial Analysis (University of Chicago)\nWUR Geoscripting\nMapping in R\nSpatial Analysis notes\nClassifying Satellite Imagery in R\nFundamentals of Spatial Analysis in R\nHandling and Analyzing Vector and Raster Data Cubes with R\nIntroduction to geospatial data analysis in R\n\n\n\nPresentations\n\nUseR! 2017: Tutorial \"Spatial Data in R: New Directions\"\nUseR! 2019: Spatial workshop part I\nUseR! 2019: Spatial workshop part II\nWhy-R 2020: Applied Spatial Data Analysis with R: retrospect and prospect\n\n\n\nOfficial materials\n\nsf Vignettes\nstars Vignettes"
  },
  {
    "objectID": "ch0.html#exploring-spatial-analysis-packages",
    "href": "ch0.html#exploring-spatial-analysis-packages",
    "title": "1  Geospatial Technology in R",
    "section": "1.1 Exploring Spatial Analysis Packages",
    "text": "1.1 Exploring Spatial Analysis Packages\nGeospatial technology has revolutionized the way we collect, analyze and interpret data related to coastal and marine environments. The use of remote sensing, geographic information systems (GIS) and global positioning systems (GPS) has enabled us to gain a better understanding of our oceans, coastlines, and the impact of human activity on these ecosystems.\nOne of the key advantages of geospatial technology is its ability to collect data over large areas in a relatively short period of time. This is particularly important in coastal and marine environments where traditional methods of data collection, such as boat surveys, can be time-consuming and expensive. Remote sensing techniques, such as satellite imagery and aerial photography, can provide a wealth of information on coastal and marine ecosystems, including water quality, sea surface temperature, and the distribution of habitats and species.\nGIS technology has also been instrumental in mapping and analyzing coastal and marine data. By overlaying different layers of information, such as bathymetry, sediment type, and biological data, we can gain a better understanding of the complex interactions that occur within these ecosystems. GIS has also been used to identify areas of high conservation value, such as critical habitat for endangered species or important breeding grounds for fish and other marine organisms.\nIn addition to remote sensing and GIS, GPS technology has also played a key role in coastal and marine data collection. By accurately recording the location of sampling sites and other data points, GPS has enabled researchers to create detailed maps of coastal and marine environments. This information can then be used to monitor changes over time, such as the erosion of shorelines or the expansion of harmful algal blooms.\nGeospatial technology has also been used to address a range of environmental issues in coastal and marine environments. For example, it has been used to monitor the impact of human activity on these ecosystems, such as the discharge of pollutants into coastal waters or the impact of commercial fishing on fish populations. It has also been used to develop tools for coastal zone management, such as predicting the impact of sea level rise on coastal communities or identifying areas at risk of coastal erosion.\nGeospatial technology has revolutionized the way we analyze and interpret spatial data. With the advent of powerful computing systems and sophisticated software packages, it has become easier to explore complex spatial patterns and relationships. In this book chapter, we will focus on exploring the various spatial analysis packages available in R, a popular open-source programming language for statistical computing and graphics.\nR, a powerful programming language and environment for statistical computing and graphics, offers a wide range of packages specifically designed for geospatial analysis. These packages provide tools and functions that enable users to work with spatial data, conduct geospatial analysis, and create informative visualizations. R offers a wide range of geospatial packages that can be used for different types of spatial analysis. Some of the most popular packages include:\n\nsf: This package provides a simple and efficient way to work with vector data in R. It supports a wide range of data formats, including shapefiles, GeoJSON, and KML. The package also includes a set of functions for data manipulation, visualization, and spatial analysis.\nraster: This package is designed for working with raster data in R. It provides functions for reading, writing, and manipulating raster datasets. The package also includes a set of functions for spatial analysis, such as resampling, reclassification, and overlay analysis.\nsp: This is one of the oldest and most widely used geospatial packages in R. It provides a set of classes and methods for working with spatial data in R. The package supports a wide range of spatial data formats, including shapefiles, GeoJSON, and KML. It also includes a set of functions for spatial analysis, such as buffering, distance calculations, and point-in-polygon tests.\nmaptools: This package provides a set of tools for working with spatial data in R. It includes functions for reading and writing spatial data in different formats, such as shapefiles, GeoJSON, and KML. The package also includes a set of functions for spatial analysis, such as clipping, merging, and overlay analysis.\nrgdal: This package provides an interface to the GDAL library, which is a powerful open-source library for working with geospatial data. The package allows R users to read and write different geospatial data formats, such as shapefiles, GeoJSON, and KML. It also includes a set of functions for spatial analysis, such as coordinate transformations and reprojection.\n\nThese are just a few of the many geospatial packages available in R. Each package has its own strengths and weaknesses, and the choice of package will depend on the specific needs of the user.\nSpatial analysis in R typically involves three main steps: data preparation, analysis, and visualization. In the data preparation step, the user loads the spatial data into R and prepares it for analysis. This may involve cleaning the data, transforming it into a different coordinate system, or merging it with other datasets.\nIn the analysis step, the user performs various spatial operations on the data using the functions provided by the geospatial packages. This may involve calculating distances between points, overlaying different layers of data to identify spatial patterns, or clustering points based on their proximity.\nIn the visualization step, the user creates maps and other visualizations to communicate the results of the analysis. R provides a wide range of tools for creating maps and other visualizations, including ggplot2, plotly, and leaflet.\nOverall, geospatial technology in R has greatly expanded our ability to analyze and interpret spatial data. With its wide range of geospatial packages and powerful analytical capabilities, R has become an essential tool for anyone working with spatial data. Whether you are a researcher analyzing complex environmental patterns or a business analyst looking to optimize logistics networks, geospatial technology in R can help you make sense of your data and gain new insights into the world around us.\n\nR, a powerful programming language and environment for statistical computing and graphics, offers a wide range of packages specifically designed for geospatial analysis. These packages provide tools and functions that enable users to work with spatial data, conduct geospatial analysis, and create informative visualizations. In this chapter, we will explore some popular packages for spatial analysis in R, highlighting their features and applications.\n\nsf: Simple Features for R: The “sf” package is a fundamental package for working with spatial data in R. It provides a consistent and efficient framework for representing, manipulating, and analyzing spatial data. The package supports various types of spatial objects, including points, lines, polygons, and grids. With “sf,” users can perform common geospatial operations such as spatial joins, spatial queries, and spatial aggregations. The package also integrates seamlessly with other geospatial packages, making it a versatile choice for spatial analysis in R.\nsp: Spatial Data Classes and Methods: The “sp” package is another essential package for geospatial analysis in R. It offers classes and methods for handling and analyzing spatial data. The package includes classes for spatial points, lines, polygons, and grids, along with functions for spatial data manipulation, transformation, and visualization. “sp” also provides capabilities for spatial interpolation, geostatistics, and spatial regression modeling. While “sf” has gained popularity in recent years, “sp” remains widely used and is compatible with many existing R packages.\nraster: Analysis of Gridded Data: The “raster” package focuses on the analysis of gridded data, such as satellite imagery, digital elevation models, and climate data. It provides functions for reading, writing, and manipulating raster data, as well as tools for raster algebra, resampling, and overlay analysis. With “raster,” users can extract values from raster layers at specific locations, calculate terrain attributes, perform neighborhood operations, and conduct spatial modeling. The package also supports parallel processing to handle large raster datasets efficiently.\nrgdal: Interface to GDAL: The “rgdal” package acts as a bridge between R and the Geospatial Data Abstraction Library (GDAL), a widely used open-source library for reading and writing geospatial data formats. “rgdal” allows users to read, write, and manipulate spatial data in various formats, including shapefiles, GeoTIFFs, and KML files. It provides functions for coordinate system transformations, projection conversions, and geometric operations. “rgdal” enhances the spatial analysis capabilities of R by expanding its data interoperability.\nleaflet: Interactive Web Mapping: The “leaflet” package enables the creation of interactive web maps directly within R. It leverages the Leaflet JavaScript library and provides an R interface for building web-based maps with rich interactivity and customization options. Users can add various layers, including markers, polygons, and heatmaps, to their maps and incorporate interactive controls and pop-ups. “leaflet” also supports the integration of geospatial data with external data sources and enables the creation of dynamic and engaging web map applications.\n\nConclusion: R offers a diverse set of packages for geospatial analysis, catering to different needs and requirements. The packages mentioned in this chapter, including sf, sp, raster, rgdal, and leaflet, provide a solid foundation for conducting spatial analysis, handling spatial data, and creating visually appealing maps. By leveraging these packages, users can unlock the full potential of R for exploring and understanding spatial patterns, conducting geospatial modeling, and communicating results effectively."
  },
  {
    "objectID": "ch1.html#programming",
    "href": "ch1.html#programming",
    "title": "2  Introduction  The R environment",
    "section": "\n2.1 Programming",
    "text": "2.1 Programming\nIn this section, we are going to demontrate the way that programming differs from Graphical User Interfaces (GUI), and its advantages, through two examples. The first example (Section 1.1.1.2) shows how the graphical interface hides essential details about the data we are working with, and ways to interact with them. The second example (Section 1.1.1.3) is similar, but related to spatial data—it shows how a seemingly simple operation is in fact complex when working with a graphical interface, but it is made simple through programming.\n\n2.1.1 Why is programming necessary?\nProgramming is an essential skill for spatial data analysis due to the complexity of the data involved. Spatial data analysis involves working with data that has a geographic component, such as maps, satellite imagery, and GPS coordinates. This type of data can be incredibly large and complex, making it difficult to work with using traditional data analysis tools. Programming allows analysts to automate tasks, manipulate data, and create custom algorithms that can handle large amounts of spatial data. In addition, programming languages such as Python and R have libraries specifically designed for spatial data analysis, making it easier for analysts to work with this type of data. Overall, programming is necessary for spatial data analysis because it allows analysts to efficiently and effectively work with complex spatial data.\n\n2.1.2 What is programming?\nA computer program is a sequence of text instructions that can be “understood” by a computer and executed. A programming language is a machine-readable artificial language designed to express computations that can be performed by a computer. Programming is the preferred way for giving instructions to the computer.\nProgramming is the process of designing, writing, testing, and maintaining computer programs. It involves using various programming languages and tools to create software that can perform specific tasks. When it comes to data analysis, programming plays a crucial role in processing and analyzing large volumes of data. Data analysis is the process of examining data sets to extract meaningful insights and information.\nProgramming provides the necessary tools and techniques to manipulate and analyze data efficiently. This includes using programming languages such as Python, R, and SQL, as well as specialized libraries and frameworks for data analysis. With programming, data analysts can automate repetitive tasks, visualize data, and build complex models to make predictions and identify patterns. In summary, programming is an essential skill for data analysts as it enables them to work with large datasets and extract valuable insights from them.\n\n2.1.3 Computer hardware\nComputing is an essential aspect of modern life, and it comprises of five key elements that are crucial for its functioning (Figure 2.1). The first element is the Central Processing Unit (CPU), which is the brain of a computer. It performs all the processing tasks and manages the computer’s hardware and software components. The second element is Random Access Memory (RAM), which stores data temporarily and enables the CPU to access it quickly. The third element is the Display, which shows the output of the CPU’s processing in a visual format. It allows users to interact with the computer and access information easily.\nThe fourth element is Storage, which stores all the data and programs permanently, even when the computer is turned off. It includes hard drives, solid-state drives, and other types of storage devices. The final element is Network, which enables computers to communicate with each other and share data over a network. It includes wired and wireless networks and is crucial for internet connectivity. Understanding these five key elements of computing is essential for anyone who uses technology in their daily lives.\n\n\n\n\nFIGURE 2.1. Key component of the computing system\n\n\n\n\n\n2.1.4 Object-oriented programming\nIn object-oriented programming, the interaction with the computer takes places through objects. Each object belongs to a class: an abstract structure with certain properties. Objects are in fact instances of a class.\nThe class comprises a template which sets the properties and methods each object of that class should have, while an object contains specific values for that particular instance (Figure 1.12).\nFor example:\n\nAll cars we see in the parking lot are instances of the “car” class\nThe “car” class has certain properties (manufacturer, color, year) and methods (start, drive, stop)\nEach “car” object has specific values for the properties (Suzuki, brown, 2011)\n\nFigure 1.12: An object (https://www.w3schools.com/js/)\nIn R, as we will see later on, everything we work with are objects. For example, a raster—such as rainfall.tif which we mentioned earlier (Section 1.1.1.3)—that we import into R is actually translated to an object of a class named stars. The object has numerous properties, such as the number of rows an columns, the resolution, the Coordinate Reference System (CRS), and so on.\nThe following two expressions are used to import the raster rainfall.tif into the R environment (we will elaborate on that later on, in Sections 2.2 and 5.3.6):\nlibrary(stars) r = read_stars(\"rainfall.tif\")\nOnce imported, an object named r, belonging to the class named stars is used to represent rasters in R, exists in the R environment (more specifically in the RAM, see Section 1.1.3). For example, printing to object displays some of its properties and their specific values, such as a resolution (delta) of 1000 meters:\nr ## stars object with 2 dimensions and 1 attribute ## attribute(s): ##  rainfall.tif    ##  Min.   :200.0   ##  1st Qu.:373.3   ##  Median :500.7   ##  Mean   :483.8   ##  3rd Qu.:585.6   ##  Max.   :908.5   ##  NA's   :20780   ## dimension(s): ##   from  to  offset delta                refsys point values x/y ## x    1 153  616965  1000 WGS 84 / UTM zone 36N FALSE   NULL [x] ## y    1 240 3691819 -1000 WGS 84 / UTM zone 36N FALSE   NULL [y]\nYou don’t need to worry about the meaning of the different properties in the above output yet. Again, we are going to cover these in detail later on (Sections 5.3.8.1–5.3.8.3 and 6.3).\n\n2.1.5 Inheritance\nOne of the implications of object-oriented programming is inheritance. Inheritance makes it possible for one class to “extend” another class, by adding new properties and/or new methods. Using our car example (Figure 1.12):\n\nA “taxi” is an extension of the “car” class, inheriting all of its properties and methods\nIn addition to the inherited properties, a “taxi” has new properties (taxi company name) and new methods (switching the taximeter on and off)\n\nIn R, the idea of inheritance is realized in various ways. For example, every complex object (such as a raster) is actually a collection of smaller components (the properties). Looking at the structure of the raster object r (using the str function, see Section 4.1.4.2) reveals that it is, in fact, a collection of many small objects belonging to simpler classes, each holding a piece of information. For example, the resolution property (named delta) is in fact a numeric vector (class numeric, see Section 2.3) of length 1 (i.e., containing a single value, 1000):\nstr(r) ## List of 1 ##  $ rainfall.tif: num [1:153, 1:240] NA NA NA NA NA NA NA NA NA NA ... ##  - attr(*, \"dimensions\")=List of 2 ##   ..$ x:List of 7 ##   .. ..$ from  : num 1 ##   .. ..$ to    : num 153 ##   .. ..$ offset: num 616965 ##   .. ..$ delta : num 1000 ##   .. ..$ refsys:List of 2 ##   .. .. ..$ input: chr \"WGS 84 / UTM zone 36N\" ##   .. .. ..$ wkt  : chr \"PROJCRS[\\\"WGS 84 / UTM zone 36N\\\",\\n    BASEGEOGCRS[\\\"WGS 84\\\",\\n        DATUM[\\\"World Geodetic System 1984\\\",\\\"| __truncated__ ##   .. .. ..- attr(*, \"class\")= chr \"crs\" ##   .. ..$ point : logi FALSE ##   .. ..$ values: NULL ##   .. ..- attr(*, \"class\")= chr \"dimension\" ##   ..$ y:List of 7 ##   .. ..$ from  : num 1 ##   .. ..$ to    : num 240 ##   .. ..$ offset: num 3691819 ##   .. ..$ delta : num -1000 ##   .. ..$ refsys:List of 2 ##   .. .. ..$ input: chr \"WGS 84 / UTM zone 36N\" ##   .. .. ..$ wkt  : chr \"PROJCRS[\\\"WGS 84 / UTM zone 36N\\\",\\n    BASEGEOGCRS[\\\"WGS 84\\\",\\n        DATUM[\\\"World Geodetic System 1984\\\",\\\"| __truncated__ ##   .. .. ..- attr(*, \"class\")= chr \"crs\" ##   .. ..$ point : logi FALSE ##   .. ..$ values: NULL ##   .. ..- attr(*, \"class\")= chr \"dimension\" ##   ..- attr(*, \"raster\")=List of 3 ##   .. ..$ affine     : num [1:2] 0 0 ##   .. ..$ dimensions : chr [1:2] \"x\" \"y\" ##   .. ..$ curvilinear: logi FALSE ##   .. ..- attr(*, \"class\")= chr \"stars_raster\" ##   ..- attr(*, \"class\")= chr \"dimensions\" ##  - attr(*, \"class\")= chr \"stars\"\nThe benefit of inheritance is that the programmer does not need to write every class from scratch. Instead, new classes can be built on top of existing ones, while re-using their properties and methods."
  },
  {
    "objectID": "ch1.html#sec-r",
    "href": "ch1.html#sec-r",
    "title": "2  Introduction  The R environment",
    "section": "\n2.2 Starting R",
    "text": "2.2 Starting R\nNow that we covered some central theoretical concepts related to programming, we are staring the practical part—writing R code to work with spatial data. In this chapter, we will become familiar with the R environment, its basic operators and syntax rules. To install R on a Windows computer, follow these steps:\n\nVisit the official R website at https://www.r-project.org/.\nClick on the “Download R” link in the “Getting Started” section.\nChoose a CRAN mirror site that is geographically closest to your location.\nSelect the appropriate version of R for your operating system (Windows in this case).\nClick on the download link for the base distribution of R.\nRun the installer and follow the prompts to complete the installation process.\nOnce installed, launch R by clicking on its icon on your desktop or from the Start menu.\n\nThe current version at the time of writing (October 2020) is R version 4.3.0 released on 2023-04-21. Once R is installed, we can open the default interface (RGui) with Start → All Programs → R → R x64 4.3.0 (Figure 2.2).\n\n\n\n\nFIGURE 2.2. R language Graphic User Interaface (GUI)\n\n\n\n\nWe will be working with R through a more advanced interface than the default one, called the RStudio. It can be downloaded from the RStudio company website. The current version is RSTUDIO-2023.03.1-446. Once both R and RStudio are installed, we can open RStudio with Start → All Programs → RStudio → RStudio Figure 2.3.\n\n\n\n\nFIGURE 2.3. Interactive Development Environment of R simplified in RStudio\n\n\n\n\n\n2.2.1 Console\nIn this section, we will only work with the console shown in Figure 2.4. The console in RStudio is an essential tool for coding in R. It is a command-line interface that allows users to interact with the R programming language. The console is where users can enter and execute R commands and see the output of those commands.\nIt is a powerful tool for data analysis and statistical modeling, allowing users to quickly test and iterate on code. The console also displays error messages and warnings, making it easier to debug code. In addition, the console can be used to install and load packages, set working directories, and manage data objects. Overall, the console in RStudio is a valuable tool for any programmer working with R, providing a streamlined way to write, test, and debug code.\n\n\n\n\nFIGURE 2.4. RStudio console\n\n\n\n\n\n2.2.2 Basic R expressions\n\n2.2.3 Console input and output\nThe simplest way to interact with the R environment is typing an expression into the R console, pressing Enter to execute it. For example, let’s type the expression 1+3+5+7:\n\nCode1 + 3 + 5 + 7\n\n[1] 16\n\n\nAfter we press Enter, the expression 1+3+5+7 is sent to the processor. The returned value, 16, is then printed in the console. Note that the value 16 is not kept in in the RAM or Mass Storage, just printed on screen.\nWe can type a number, the number itself is returned:\n\nCode600 \n\n[1] 600\n\n\nWe can type text inside single ' or double \" quotes:\n\nCode\"Hello\" \n\n[1] \"Hello\"\n\n\nThe last two expressions are examples of constant values, numeric or character. These are the simplest type of expressions in R.\n\n2.2.4 Arithmetic operators\nThrough interactive use of the command line, we can experiment with basic operators in R. For example, R includes the standard arithmetic operators (Table 1.1).\n\n\n\nTable 1.1: Arithmetic operators\n\n\nOperator\nMeaning\n\n\n+\nAddition\n\n\n-\nSubtraction\n\n\n*\nMultiplication\n\n\n/\nDivision\n\n\n^\nExponent\n\n\nHere are some examples of expressions that use the arithmetic operators:\n\nCode5 + 3 \n\n[1] 8\n\n\n\nCode4 - 5 \n\n[1] -1\n\n\n\nCode1 * 10 \n\n[1] 10\n\n\n\nCode1 / 10 \n\n[1] 0.1\n\n\n\nCode10 ^ 2 \n\n[1] 100\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWe can use the up ↑ and down ↓ keys to scroll through the executed expressions history. Try to execute several different expressions, then scroll up until you reach one of the previous expressions and re-execute it by pressing Enter. Scrolling through expression history is convenient for going back to previously exceuted code, without re-typing it, possibly making modifications before excecuting it once again.\n\n\nNote that very large or very small numbers are formatted in exponential notation:\n\nCode1 / 1000000\n\n[1] 1e-06\n\n\n\nCode7 * 100000\n\n[1] 7e+05\n\n\nInfinity is treated as a special numeric value, Inf or -Inf:\n\nCode1 / 0\n\n[1] Inf\n\n\n\nCode-1 / 0\n\n[1] -Inf\n\n\n\nCodeInf + 1\n\n[1] Inf\n\n\n-1 *\nWe can control operator precedence with brackets, just like in math:\n\nCode2 * 3 + 1\n\n[1] 7\n\n\n\nCode2 * (3 + 1)\n\n[1] 8\n\n\nIt is recommended to use brackets for clarity, even where not strictly required.\n\n2.2.5 Spaces and comments\nThe interpreter ignores everything to the right of the number symbol #:\n1 * 2 # * 3 \nThe # symbol is therefore used for code comments:\n# Multiplication example 5 * 5 \n\nWhy do you think the code outputs are marked by ## in the code sections (such as ## [1] 25 in the above code section)?\n\nThe interpreter ignores spaces, so the following expressions are treated exactly the same way:\n1 + 1 \n1+1 \n1+1\nWe can type Enter in the middle of an expression and keep typing on the next line. The interpreter displays the + symbol, which means that the expression is incomplete (Figure 1.18):\n5 *  2 #\nFigure 1.18: Incomplete expression\nWe can exit from the “completion” state, or from an ongoing computation, any time, by pressing Esc.\nClearing the console can be done with Ctrl+L.\n\n2.2.6 Conditional operators\nConditions are expressions that use conditional operators and have a yes/no result, i.e., the condition can be either true or false. The result of a condition is a logical value, TRUE or FALSE:\n\nTRUE means the expression is true\nFALSE means the expression is false\n(NA means it is unknown)\n\nThe conditional operators in R are listed in Table 1.2.\n\n\n\nTable 1.2: Conditional operators\n\n\nOperator\nMeaning\n\n\n==\nEqual\n\n\n&gt;\nGreater than\n\n\n&gt;=\nGreater than or equal\n\n\n&lt;\nLess than\n\n\n&lt;=\nLess than or equal\n\n\n!=\nNot equal\n\n\n&\nAnd\n\n\n|\nOr\n\n\n!\nNot\n\n\nFor example, we can use conditional operators to compare numeric values:\n1 &lt; 2\n1 &gt; 2\n2 &gt; 2\n2 &gt;= 2\n2 != 2\n“Equal” (==) and “not equal” (!=) are opposites of each other, since a pair of values can be either equal or not:\n1 == 1 \n1 == 2 \nThe “and” (&) and “or” (|) operators are used to create more complex conditions. “And” (&) returns TRUE when both sides are TRUE:\n(1 &lt; 10) & (10 &lt; 100) \n(1 &lt; 10) & (10 &gt; 100) \n“Or” (|) returns TRUE when at least one of the sides is TRUE:\n\nCode(1 &lt; 10) | (10 &lt; 100) \n\n[1] TRUE\n\n\n\nCode(1 &lt; 10) | (10 &gt; 100) \n\n[1] TRUE\n\n\nThe last conditional operator is “not” (!), which reverses TRUE to FALSE and FALSE to TRUE:\n\nCode1 == 1 \n\n[1] TRUE\n\n\n\nCode!(1 == 1) \n\n[1] FALSE\n\n\n\nCode(1 == 1) & (2 == 2) \n\n[1] TRUE\n\n\n\nCode(1 == 1) & !(2 == 2) \n\n[1] FALSE\n\n\n\nRun the following expression and explain their result:\nFALSE == FALSE\n!(TRUE == TRUE)\n!(!(1 == 1))\n\n\n2.2.7 Special values\nR has several special values, as listed in Table 1.3.\n\n\n\nTable 1.3: Special values in R\n\n\nValue\nMeaning\n\n\nInf\nInfinity\n\n\nNA\nNot Available\n\n\nNaN\nNot a Number\n\n\nNULL\nEmpty object\n\n\nWe already met Inf, and have shown how it can be the result of particular arithmetic operations such as division by zero (Section 1.3.2):\n\nCode1 / 0\n\n[1] Inf\n\n\nNA specifies an unknown, or missing, value. Later on, we are going to encounter several situations where NA values can arise. For example, empty cells in a table imported into R, such as from a CSV file (Section 4.4), are encoded in R as NA values.\nNA values can participate in any arithmetic or logical operation. For example:\n\nCodeNA + 3\n\n[1] NA\n\n\n\nWhy do you think the result of the above expression is NA?\n\n\nWhat do you think will be the result of the expression NA == NA?\n\nNaN is less relevant for the material of this book, but it is important to be familiar with it. NaN values often result from “meaningless” arithmetic operations:\n\nCode0 / 0 \n\n[1] NaN\n\n\nFor most practical purposes, NaN values behave exactly the same way as NA values.\nFinally, the value of NULL specifies an empty object:\n\nCodeNULL\n\nNULL\n\n\nNULL has some uses which we will discuss later on (Section 4.2.3).\n\n2.2.8 Functions\nIn math, a function (Figure 1.19) is a relation that associates each element x of a set X, to a single element y of another set Y. For example, the function y=2x�=2� is a mathematical function that associates any number x� with the number 2x2�.\nFigure 1.19: A function\nThe concept of functions in programming is similar. A function is a code piece that “knows” how to do a certain task. Executing the function is known as a function call. The function accepts zero or more objects as input (e.g., 2) and returns a single object as output (e.g., 4). In addition to the returned value, the function may perform other task(s), known as side effects (for example: writing information to a file, or displaying graphical output).\nThe number and type of inputs the function needs are determined in the function definition; these are known as the function parameters (e.g., a single number). The objects the function received in practice, as part of a particular function call, are known as arguments (e.g., 2).\nA function is basically a set of pre-defined instructions. There are thousands of built-in functions in R. Later on we will learn to define our own functions (Section 3.3).\nA function call is composed of the function name, followed by the arguments inside brackets () and separated by commas ,. For example, the sqrt function calculates the square root of its input. For example, the following expression calls the sqrt function with the argument 4:\n\nCodesqrt(4)\n\n[1] 2\n\n\nHere, the sqrt function received a single argument 4 and returned its square root 2.\nAs a side note, it is interesting to mention that everything we do in R in fact involves functions (Figure 1.20).\nFigure 1.20: From Chambers 2014, Statistical Science (https://arxiv.org/pdf/1409.3531.pdf)\nEven arithmetic operators are functions, just written in a special way. The arithmetic operators can be executed in the “ordinary” function syntax, too, as follows:\n\nCode`+`(5, 5)\n\n[1] 10\n\n\n\n2.2.9 Error messages\nConsider the following three different expressions:\n\nCodesqrt(16)\n\n[1] 4\n\n\nsqrt(\"16\")\nsqrt(a)\nIn last two expressions we got error messages, because the expressions were illegal, i.e., not in agreement with the syntax rules of R. The first error occurred because we tried to run a mathematical operation sqrt on a text value \"a\". The second error occurred because we tried to use a non-existing object a. Any text without quotes is treated as a name of an object, i.e., a label for an actual object stored in RAM. Since we don’t have an object named a we got an error.\n\n2.2.10 Pre-loaded objects\nWhen starting R, a default set of objects is loaded into the RAM, such as TRUE, FALSE, sqrt and pi. For example, type pi and see what happens:\n\nCodepi\n\n[1] 3.141593\n\n\n\n2.2.11 Decimal places\nIs the value of π� stored in memory really equal to the value we see on screen (3.141593)? Executing the following condition reveals that the answer is no:\n\nCodepi == 3.141593\n\n[1] FALSE\n\n\nIf not, what is the difference?\n\nCodepi - 3.141593\n\n[1] -3.464102e-07\n\n\nThe reason for the discrepancy is that, by default, R prints only the first 7 digits:\n\nCodeoptions()$digits\n\n[1] 7\n\n\nWhen working with R we should keep in mind that the printed value and object contents are not always identical, so that the printed output is more convenient for the user.\n\nThe number of digits to print can be changed with an expression such as options(digits=22). Try running the latter expression, then print the value of pi once again.\n\n\n2.2.12 Case-sensitivity\nR is case-sensitive, it distinguishes between lower-case and upper-case letters. For example, TRUE is a logical value, but True and true are undefined:\n\nCodeTRUE\n\n[1] TRUE\n\n\nTrue\ntrue\n\n2.2.13 Classes\nR is an object-oriented language (Section 1.1.5), where each object belongs to a class. The class functions accepts an object and returns the class name:\n\nCodeclass(TRUE)\n\n[1] \"logical\"\n\n\n\nCodeclass(1)\n\n[1] \"numeric\"\n\n\n\nCodeclass(pi)\n\n[1] \"numeric\"\n\n\n\nCodeclass(\"a\")\n\n[1] \"character\"\n\n\n\nCodeclass(sqrt)\n\n[1] \"function\"\n\n\n\nExplain the returned value of the following expressions.\n\nclass(1 &lt; 2)\nclass(\"logical\")\nclass(1) == class(2)\nclass(class)\nclass(class(sqrt))\nclass(class(1))\n\n2.2.14 Using help files\nEvery built-in object is associated with a help document, which can be accessed using the help function or the ? operator:\nhelp(class) ?class ?TRUE ?pi"
  },
  {
    "objectID": "ch1.html#packages",
    "href": "ch1.html#packages",
    "title": "2  Introduction  The R environment",
    "section": "\n2.3 Packages",
    "text": "2.3 Packages\nSo far we’ve been using a couple functions that are included with R out-of-the-box such as search() and q(). We say that these functions are from “Base R”. But, one of the amazing things about R is its users’ community that create new functions and packages. An R package is a collection of functions, data, and documentation that extends the capabilities of base R. Using packages is key to the successful use of R. The majority of the packages that you will learn in this manual are part of the so-called tidyverse, which is an ecosystem of packages that share a common philosophy of data and R programming, and are designed to work together naturally.\nThe tidyverse is a coherent system of packages for data manipulation, exploration and visualization that share a common design philosophy. The Tidyverse (Wickham et al. 2019) packages form a core set of functions that will allow us to perform most any type of data cleaning or analysis we will need to do. We will use the following packages from the tidyverse\n\n\nggplot2—for data visualisation.\n\ndplyr—for data manipulation.\n\ntidyr—for data tidying.\n\nreadr—for data import.\n\npurrr—for functional programming.\n\ntibble—for tibbles, a modern re-imagining of data frames.\n\n\n2.3.1 Installing R packages\nFor us to use tidyverse and any other package that is not included in Base R, we must install them first. The easiest way to install packages is to use the install.packages() command. For example, let’s go ahead and install the tidyverse package on your machine:\ninstall.packages(\"tidyverse\")\nOn your own computer, type that line of code in the console, and then press enter to run it. R will download the packages from CRAN and install it to your computer. If you have problems installing, make sure that you are connected to the internet, and that https://cloud.r-project.org/ isn’t blocked by your firewall or proxy.\n\n2.3.2 Loading installed packages\nNow we’ve installed the package, but we need to tell R that we are going to use some functions within the tidyverse package. With exception to base R package, add on package that are installed must be called with either library or require functions to make their tools accessible in R session. Let’s us load the tidyverse package we just installed\nrequire(tidyverse)\nYou notice that when we load tidyverse, it popup a notification message showing the loaded packages and the conflicts they bring in. These conflicts happen when packages have functions with the same names as other functions. This is OK when you prefer the function in a package like tidyverse rather than some other function. Basically the last package loaded in will mask over other functions if they have common names.\n\n2.3.3 Flavours of R: base and tidyverse\nR is a programming language, and just like any language, it has different dialects. When you read about R online, you’ll frequently see people mentioning the words “base” and “tidyverse.” These are the two most common dialects of R. Base R is just that, R in its purest form. The tidyverse is a collection of add-on packages for working with different types of data. The two are fully compatible, and you can mix and match as much as you like. Both ggplot2 and magrittr are part of the tidyverse.\nIn recent years, the tidyverse has been heavily promoted as being “modern” R which “makes data science faster, easier and more fun.” You should believe the hype. The tidyverse is marvellous. But if you only learn tidyverse R, you will miss out on much of what R has to offer. Base R is just as marvellous, and can definitely make data science as fast, easy and fun as the tidyverse. Besides, nobody uses just base R anyway - there are a ton of non-tidyverse packages that extend and enrich R in exciting new ways. Perhaps “extended R” would be better names for the non-tidyverse dialect.\nAnyone who tells you to just learn one of these dialects is wrong. Both are great, they work extremely well together, and they are similar enough that you shouldn’t limit yourself to just mastering one of them. This book will show you both base R and tidyverse solutions to problems, so that you can decide for yourself which is faster, easier, and more fun.\nA defining property of the tidyverse is that there are separate functions for everything, which is perfect for code that relies on pipes. In contrast, base R uses fewer functions, but with more parameters, to perform the same tasks. If you use tidyverse solutions there is a good chance that there exists a function which performs exactly the task you’re going to do with its default settings. This is great (once again, especially if you want to use pipes), but it means that there are many more functions to master for tidyverse users, whereas you can make do with much fewer in base R. You will spend more time looking up function arguments when working with base R (which fortunately is fairly straightforward using the ? documentation), but on the other hand, looking up arguments for a function that you know the name of is easier than finding a function that does something very specific that you don’t know the name of. There are advantages and disadvantages to both approaches.\n\nExample 2.1 Oh no! After all our analyses and everything we’ve done, our colleague just emailed us that Tanzania and Zanzibar should not be separated but combined to form a single country Tanzania.\n\n\nSolution. \n\nThat request force us to start over the analysis and combine the catch data for mainland Tanzania and Zanzibar. Aaaaah! If we were doing this in Excel, this would be a bummer; we’d have to rebuild our pivot table and click through all of our logic again. And then export our figures and save them into our report.\n\n\n\n\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686."
  },
  {
    "objectID": "ch2.html#rstudio-keyboard-stortcuts",
    "href": "ch2.html#rstudio-keyboard-stortcuts",
    "title": "\n3  Vectors\n",
    "section": "\n3.1 RStudio keyboard stortcuts",
    "text": "3.1 RStudio keyboard stortcuts\nRStudio provides several handy keyboard shortcuts that streamline the process of editing and executing code files. Here are some useful RStudio keyboard shortcuts:\n\nCtrl + Enter: Executes the currently selected line or the highlighted code chunk.\nCtrl + Shift + Enter: Executes the current line or selected code chunk and moves the cursor to the next line.\nCtrl + Shift + Up/Down Arrow: Moves the current line or selected lines up or down within the script.\nCtrl + L: Clears the console.\nCtrl + 1/2/3/4: Switches between the different panes in RStudio (Source, Console, Environment, and Plots).\nCtrl + Shift + M: Inserts a new Markdown chunk in an R Markdown document.\nCtrl + Shift + K: Knits the current R Markdown document to the specified output format.\nCtrl + Shift + C: Comments or uncomments the selected lines or the current line.\nCtrl + Shift + R: Runs the current line or selected code as R code (useful for running lines in R Markdown documents).\nCtrl + Shift + T: Surrounds the selected text with a tag in an R Markdown document.\n\nThese are just a few examples of the keyboard shortcuts available in RStudio. To explore more shortcuts or customize them according to your preferences, you can go to the “Tools” menu in RStudio, select “Modify Keyboard Shortcuts,” and browse through the available options."
  },
  {
    "objectID": "ch2.html#vectors",
    "href": "ch2.html#vectors",
    "title": "\n3  Vectors\n",
    "section": "\n3.2 Vectors",
    "text": "3.2 Vectors\n\n3.2.1 What is a vector?\nThe vector is indeed the simplest data structure in R and serves as our introductory topic in this book. In R, a vector is an ordered collection of values that share the same data type. It can contain elements of various types, such as numeric, character, logical, or factors. Here are a few examples illustrating the concept of a vector:\n\nNumeric vector: c(1.5, 2.7, 3.9, 4.2)\nCharacter vector: c(“apple”, “banana”, “orange”)\nLogical vector: c(TRUE, FALSE, TRUE, FALSE)\nFactor vector: factor(c(“low”, “medium”, “high”, “low”))\n\nEach of these examples represents a vector with elements of the same data type. The elements within a vector maintain their order, allowing for easy indexing and manipulation. Understanding vectors is fundamental as they serve as building blocks for more complex data structures and facilitate data analysis and manipulation in R.\n\n3.2.2 The c function\nVectors can be created with the c function, which combines the given vectors, in the specified order. For example:\n\nCodex = c(1, 2, 3) \nx \n\n[1] 1 2 3\n\n\nThe c() function in R is not limited to combining individual values but can also be used to combine multiple vectors of different lengths into a new vector. Here’s an example that demonstrates this capability:\n\nCodev1 &lt;- c(1, 2, 3)\nv2 &lt;- c(4)\nv3 &lt;- c(5, 6, 7)\nv4 &lt;- c(8, 9)\n\ncombined_vector &lt;- c(v1, v2, v3, v4)\ncombined_vector\n\n[1] 1 2 3 4 5 6 7 8 9\n\n\nThe c() function in R can indeed be used to combine vectors of different lengths into a new vector. Here’s the corrected example:\n\nCodev1 &lt;- c(1, 2, 3)\nv2 &lt;- c(4)\nv3 &lt;- c(5, 6, 7)\nv4 &lt;- c(8, 9)\n\ncombined_vector &lt;- c(v1, v2, v3, v4)\ncombined_vector\n\n[1] 1 2 3 4 5 6 7 8 9\n\n\nThe expression c(x, 84, x, c(-1, -2)) combines the values from multiple vectors into a new vector. However, since the value of x is not provided, I’ll demonstrate the general concept using a placeholder value.\nHere is another example of using the c function, this time to combine four character values into a vector of length 4.\n\nCodey = c(\"cat\", \"dog\", \"mouse\", \"apple\") \ny\n\n[1] \"cat\"   \"dog\"   \"mouse\" \"apple\"\n\n\n\n3.2.3 Vector subsetting (individual elements)\nIn R, individual elements of a vector can be accessed using the [ operator along with a numeric index. This indexing mechanism allows us to retrieve a specific element from the vector, effectively creating a subset containing only that element. By specifying the desired index within square brackets, we can extract the value associated with that position in the vector. For example, my_vector[3] would retrieve the third element of the vector my_vector. This capability provides flexibility in working with vectors, allowing us to selectively access and manipulate individual elements as needed for our data analysis and computations.\n\nCodey[1] \n\n[1] \"cat\"\n\n\n\nCodey[2] \n\n[1] \"dog\"\n\n\n\nCodey[3] \n\n[1] \"mouse\"\n\n\n\nCodey[4]\n\n[1] \"apple\"\n\n\nNote that numeric indices in R start at 110!\nHere is another example:\n\nCodecounts = c(2, 0, 3, 1, 3, 2, 9, 0, 2, 1, 11, 2) \ncounts[4] \n\n[1] 1\n\n\nWe can also make an assignment into a vector subset, for example to replace an individual element:\n\nCodex = c(1, 2, 3)\nx \n\n[1] 1 2 3\n\n\n\nCodex[2] = 300 \nx \n\n[1]   1 300   3\n\n\nIndeed, in the example you provided, we made an assignment into a subset with a single element. However, as we progress in our learning, we will discover that assigning values into subsets can be done for subsets of any length. This flexibility allows us to modify specific elements or even replace entire subsets within a vector. By specifying the desired subset using indexing techniques, we can assign new values to those elements, effectively updating or altering the vector according to our requirements.\nThis capability becomes particularly valuable when performing data manipulations, transformations, or when updating specific data points within a larger dataset. As we delve deeper into R, we will explore various techniques and approaches for assigning values into subsets of different lengths, empowering us to efficiently manipulate and update our data as needed.\n\n3.2.4 Calling functions on a vector\nR provides a wide range of functions that allow us to calculate various properties of vectors. Some commonly used functions include length, min, max, range, mean, and sum. Here’s an example that showcases their usage:\n\nCodex = c(1, 6, 3, -8, 2) \nx \n\n[1]  1  6  3 -8  2\n\n\n\nCodelength(x)  \n\n[1] 5\n\n\n\nCodemin(x)    \n\n[1] -8\n\n\n\nCodemax(x)     \n\n[1] 6\n\n\n\nCoderange(x)   \n\n[1] -8  6\n\n\n\nCodemean(x)    \n\n[1] 0.8\n\n\n\nCodesum(x)     \n\n[1] 4\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn what way is the range function different from the other functions shown above?\n\n\nContrariwise, there are functions that operate on each vector element, separately, returning a vector of results having the same length as the input:\n\nCodeabs(x)   # Absolute value ## [1] 1 6 3 8 2\n\n[1] 1 6 3 8 2\n\n\n\nCodesqrt(x)  \n\n[1] 1.000000 2.449490 1.732051      NaN 1.414214\n\nCodex\n\n[1]  1  6  3 -8  2\n\n\n\n3.2.5 The recycling rule (arithmetic)\nWhen binary operations, such as arithmetic operators (+, -, *, /), are applied to two vectors in R, the operations are performed element-by-element. This means that each corresponding pair of elements from the two vectors is operated upon independently, resulting in a new vector of the respective results. Here’s an example that demonstrates this behavior:\n\nCodevector1 &lt;- c(1, 2, 3)\nvector2 &lt;- c(4, 5, 6)\n\n\n\nCodevector1 + vector2\n\n[1] 5 7 9\n\n\n\nCodevector1 - vector2\n\n[1] -3 -3 -3\n\n\n\nCodevector1 * vector2\n\n[1]  4 10 18\n\n\n\nCodevector1 / vector2\n\n[1] 0.25 0.40 0.50\n\n\n\nCodevector1 &gt; vector2\n\n[1] FALSE FALSE FALSE\n\n\n\nCodevector1 &lt; vector2\n\n[1] TRUE TRUE TRUE\n\n\nWhat happens when the input vector lengths do not match? In such case, the shorter vector gets “recycled”. For example, when one of the vectors is of length 3 and the other vector is of length 6, then the shorter vector (of length 3) is replicated two times, until it matches the longer vector (Figure 2.7). Thus, the expression:\n\nCodec(1, 2, 3)  + c(1, 2, 3, 4, 5, 6) \n\n[1] 2 4 6 5 7 9\n\n\nis similar to the expression:\n\nCodec(1, 2, 3, 1, 2, 3) + c(1, 2, 3, 4, 5, 6) \n\n[1] 2 4 6 5 7 9\n\n\nWhen one of the vectors is of length 1 and the other is of length 4, the shorter vector (of length 1) is replicated 4 times:\n\nCode2  * c(1, 2, 3, 4) \n\n[1] 2 4 6 8\n\n\n\nCodec(2, 2, 2, 2) * c(1, 2, 3, 4) \n\n[1] 2 4 6 8\n\n\nWhen one of the vectors is of length 2 and the other is of length 6, the shorter vector (of length 2) is replicated 3 times:\n\nCodec(10, 100) + c(1, 2, 3, 4, 5, 6) \n\n[1]  11 102  13 104  15 106\n\n\n\nCodec(10, 100, 10, 100, 10, 100) + c(1, 2, 3, 4, 5, 6) \n\n[1]  11 102  13 104  15 106\n\n\nWhat happens when the longer vector length is not a multiple of the shorter vector length? When the longer vector length is not a multiple of the shorter vector length, a situation arises where recycling is incomplete. This means that the elements of the shorter vector cannot be evenly distributed or recycled to match the length of the longer vector. As a result, some elements from the shorter vector will be used to fill up the longer vector, but there will be remaining elements in the shorter vector that do not have corresponding positions in the longer vector.\nIn programming languages like R, this incomplete recycling is indicated by a warning message, which alerts the user to the fact that the two vectors are not compatible for recycling. It is important to ensure that vector lengths are compatible before attempting to recycle elements, as incomplete recycling\n\nCodec(1, 2) * c(1, 2, 3) \n\n[1] 1 4 3\n\n\n\nCodec(1, 2, 1) * c(1, 2, 3) \n\n[1] 1 4 3\n\n\n\n3.2.6 Consecutive and repetitive vectors\nIn addition to the c function, there are three commonly used methods for creating consecutive or repetitive vectors: the seq and rep functions.\nThe seq function generates a sequence of numbers that can be either increasing or decreasing. It takes arguments such as the starting point, ending point, and the increment or decrement value. For example, seq(1, 10, 2) creates a vector with the numbers 1, 3, 5, 7, 9, indicating an increasing sequence with a step of 2.\n\nCodeseq(1, 10, 2)\n\n[1] 1 3 5 7 9\n\n\nGenerate a decreasing sequence from 10 to 1 with a step of 1\n\nCodeseq(10, 1, -1)\n\n [1] 10  9  8  7  6  5  4  3  2  1\n\n\nThe rep function, on the other hand, replicates elements of a vector. It can replicate a single element or an entire vector multiple times. By specifying the times argument, you can control the number of repetitions. For instance, rep(c(1, 2, 3), times = 3) generates a vector containing three repetitions of the sequence 1, 2, 3: 1, 2, 3, 1, 2, 3, 1, 2, 3.\n\nCoderep(1:3, times = 3)\n\n[1] 1 2 3 1 2 3 1 2 3\n\n\nReplicate a vector multiple times\n\nCoderep(c(\"A\", \"B\", \"C\"), each = 3)\n\n[1] \"A\" \"A\" \"A\" \"B\" \"B\" \"B\" \"C\" \"C\" \"C\"\n\n\nThese functions provide flexible ways to create consecutive or repetitive vectors in programming languages like R, allowing for efficient generation of data for various purposes.\n\n3.2.7 Function calls\nBy utilizing the seq function, we will illustrate three characteristics of function calls. Initially, it is possible to exclude parameter names provided that the arguments are provided in the default order. For instance, the subsequent function calls are equivalent as the default order of the first three parameters of the seq function is from, to, and by:\n\nCodeseq(from = 5, to = 10, by = 1) ## [1]  5  6  7  8  9 10\n\n[1]  5  6  7  8  9 10\n\n\n\nCodeseq(5, 10, 1) ## [1]  5  6  7  8  9 10\n\n[1]  5  6  7  8  9 10\n\n\nFurthermore, we can utilize any argument order by explicitly specifying parameter names. This allows for flexibility in function calls. Even if the order of arguments is different, the following three function calls are equivalent due to the usage of parameter names:\n\nCodeseq(to = 10, by = 1, from = 5) \n\n[1]  5  6  7  8  9 10\n\n\n\nCodeseq(by = 1, from = 5, to = 10) \n\n[1]  5  6  7  8  9 10\n\n\n\nCodeseq(from = 5, by = 1, to = 10) \n\n[1]  5  6  7  8  9 10\n\n\nLastly, it is possible to exclude parameters that have a default argument defined in the function. A default argument is a predefined value specified in the function definition. As an illustration, the by parameter in the seq function has a default value of 1. Hence, if the by argument is omitted in the function call, it will automatically be assigned the default value of 1:\n\nCodeseq(5, 10, 1) \n\n[1]  5  6  7  8  9 10\n\n\n\nCodeseq(5, 10) \n\n[1]  5  6  7  8  9 10\n\n\nThe order of parameters, their default values (if any), and other relevant information about a specific function can be found in the help file associated with that function. The help file provides comprehensive documentation that describes the function’s usage, arguments, return values, and often includes examples and additional explanations.\nIt is a valuable resource for understanding and utilizing functions effectively in programming languages like R. To access the help file for a specific function, you can use the ? operator followed by the function name in the R console or refer to the documentation provided by the programming environment or package documentation.\n?seq\n\n3.2.8 Vector subsetting (general)\nVector subsetting refers to the process of selecting specific elements from a vector based on certain criteria or indices. It allows you to extract a subset of data from a vector that meets your desired conditions or requirements. There are different ways to perform vector subsetting in various programming languages, including R.\nIn R, vector subsetting can be done using square brackets [ ] and various indexing methods. Here are a few commonly used approaches:\n\nCodex = c(43, 85, 10) \nx[3] \n\n[1] 10\n\n\nWe can also use a vector of length &gt;1 as an index. For example, the following expression returns the first and second elements of x, since the index is the the vector c(1,2) (which we create using the : operator) (Section 2.3.6.2):\n\nCodex[1:2] \n\n[1] 43 85\n\n\nNote that the vector of indices can consist of any combination of indices whatsoever. It does not have to be consecutive, and it can even include repetitions:\n\nCodex[c(1, 1, 3, 2)] \n\n[1] 43 43 10 85\n\n\nHere is another example:\n\nCodecounts = c(2, 0, 3, 1, 3) \ncounts[1:3] \n\n[1] 2 0 3\n\n\nAnd here is one more example where the index is not consecutive:\n\nCodecounts = c(2, 0, 3, 1, 3, 2, 9, 0, 2, 1, 11, 2) \ncounts[c(1:3, 7:9)] ## [1] 2 0 3 9 0 2\n\n[1] 2 0 3 9 0 2\n\n\nFor the next examples, let’s create a vector of all even numbers between 1 and 100:\n\nCodex = seq(2, 100, 2) \nx ##  [1]   2   4   6   8  10  12  14  16  18  20  22  24  26  28  30  32  34  36  38 ## [20]  40  42  44  46  48  50  52  54  56  58  60  62  64  66  68  70  72  74  76 ## [39]  78  80  82  84  86  88  90  92  94  96  98 100\n\n [1]   2   4   6   8  10  12  14  16  18  20  22  24  26  28  30  32  34  36  38\n[20]  40  42  44  46  48  50  52  54  56  58  60  62  64  66  68  70  72  74  76\n[39]  78  80  82  84  86  88  90  92  94  96  98 100\n\n\n\nWhat is the meaning of the numbers in square brackets when printing the vector?\n\nHow can we check how many elements does x have? Recall the length function :\n\nCodelength(x) ## [1] 50\n\n[1] 50\n\n\nUsing this knowledge, here are two expression that return the value of the last element in x:\n\nCodex[50] \n\n[1] 100\n\n\n\nCodex[length(x)] \n\n[1] 100\n\n\n\nWhich of the last two expressions is preferable? Why?\n\nWhich index can we use to get back the entire vector? We can use an index that contains all of the vector element indices, starting from 1 and up to length(x):\n\nCodex[1:length(x)] ##  [1]   2   4   6   8  10  12  14  16  18  20  22  24  26  28  30  32  34  36  38 ## [20]  40  42  44  46  48  50  52  54  56  58  60  62  64  66  68  70  72  74  76 ## [39]  78  80  82  84  86  88  90  92  94  96  98 100\n\n [1]   2   4   6   8  10  12  14  16  18  20  22  24  26  28  30  32  34  36  38\n[20]  40  42  44  46  48  50  52  54  56  58  60  62  64  66  68  70  72  74  76\n[39]  78  80  82  84  86  88  90  92  94  96  98 100\n\n\n\nWhat index can we use to get the entire vector except for the last two elements?\n\n\nWhat index can we use to get a reversed vector?\n\nNote that there is a special function named rev for reversing a vector:\n\nCoderev(x) ##  [1] 100  98  96  94  92  90  88  86  84  82  80  78  76  74  72  70  68  66  64 ## [20]  62  60  58  56  54  52  50  48  46  44  42  40  38  36  34  32  30  28  26 ## [39]  24  22  20  18  16  14  12  10   8   6   4   2\n\n [1] 100  98  96  94  92  90  88  86  84  82  80  78  76  74  72  70  68  66  64\n[20]  62  60  58  56  54  52  50  48  46  44  42  40  38  36  34  32  30  28  26\n[39]  24  22  20  18  16  14  12  10   8   6   4   2\n\n\nNote that, when requesting an element, or elements, beyond the vector length, we get NA (Not Available) value(s). For example:\n\nCodex[55] ## [1] NA\n\n[1] NA\n\n\n\nCodex[1:80] ##  [1]   2   4   6   8  10  12  14  16  18  20  22  24  26  28  30  32  34  36  38 ## [20]  40  42  44  46  48  50  52  54  56  58  60  62  64  66  68  70  72  74  76 ## [39]  78  80  82  84  86  88  90  92  94  96  98 100  NA  NA  NA  NA  NA  NA  NA ## [58]  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA ## [77]  NA  NA  NA  NA\n\n [1]   2   4   6   8  10  12  14  16  18  20  22  24  26  28  30  32  34  36  38\n[20]  40  42  44  46  48  50  52  54  56  58  60  62  64  66  68  70  72  74  76\n[39]  78  80  82  84  86  88  90  92  94  96  98 100  NA  NA  NA  NA  NA  NA  NA\n[58]  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA\n[77]  NA  NA  NA  NA\n\n\n\n3.2.9 The recycling rule (assignment)\nThe recycling rule in R also applies to assignment operations. When assigning a vector of length 1 to a subset of a longer vector, R will recycle the single value to match the length of the subset. This ensures that the assignment can be performed consistently across the subset.For example, here we assign a vector of length 1 (NA) into a subset of length 6 (c(1:3,7:9)). As a result, NA is replicated six times, to match the subset. Here’s an example illustrating this behavior:\n\nCodecounts = c(2, 0, 3, 1, 3, 2, 9, 0, 2, 1, 11, 2) \ncounts ##  [1]  2  0  3  1  3  2  9  0  2  1 11  2\n\n [1]  2  0  3  1  3  2  9  0  2  1 11  2\n\n\n\nCodecounts[c(1:3, 7:9)] = NA \ncounts ##  [1] NA NA NA  1  3  2 NA NA NA  1 11  2\n\n [1] NA NA NA  1  3  2 NA NA NA  1 11  2\n\n\nHere, c(NA,99) is replicated three times, also to match the subset of length 6:\n\nCodecounts[c(1:3, 7:9)] = c(NA, 99) \ncounts ##  [1] NA 99 NA  1  3  2 99 NA 99  1 11  2\n\n [1] NA 99 NA  1  3  2 99 NA 99  1 11  2\n\n\n\n3.2.10 Logical vectors\n\n3.2.10.1 Creating logical vectors\nThe third common type of vectors in R, alongside numeric and character vectors, is the logical vector. A logical vector consists of logical values, which are primarily represented by TRUE and FALSE. These values are used to denote logical conditions or Boolean expressions in R. Here’s an example of a logical vector:\n\nCode# Create a logical vector\nmy_logical_vector &lt;- c(TRUE, FALSE, TRUE, FALSE, NA)\n\n# Output the logical vector\nmy_logical_vector\n\n[1]  TRUE FALSE  TRUE FALSE    NA\n\n\nn the provided case, the logical vector is composed of five elements: TRUE, FALSE, TRUE, FALSE, and NA (representing missing or undefined values). Logical vectors are commonly used for logical operations, conditional statements, filtering data, and controlling program flow in R. They allow for the representation and manipulation of logical conditions and Boolean values within vectorized operations.\n\nCodec(TRUE, FALSE, FALSE) ## [1]  TRUE FALSE FALSE\n\n[1]  TRUE FALSE FALSE\n\n\n\nCoderep(TRUE, 7) ## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\n[1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\n\nLogical vectors are typically created by applying conditional operators to numeric or character vectors. These conditional operators evaluate each element of the vector and return a logical value (TRUE or FALSE) based on the specified condition.\n\nCodex = 1:10 \nx \n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nCodex &gt;= 7 \n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n\n\nNote how the recycling rule applies to conditional operators in the above expression.\nAn important property of logical vectors is, that, when arithmetic operations are applied, the logical vector is automatically converted to a numeric one, where TRUE becomes 1 and FALSE becomes 0. For example:\n\nCodeTRUE + FALSE ## [1] 1\n\n[1] 1\n\n\n\nCodesum(x &gt;= 7) ## [1] 4\n\n[1] 4\n\n\n\nCodemean(x &gt;= 7) ## [1] 0.4\n\n[1] 0.4\n\n\n::: layout-important What is the meaning of the values 4 and 0.4 in the above example? :::\n\n3.2.10.2 Subsetting with logical vectors\nExactly! In addition to using a numeric vector of indices for subsetting, a logical vector can also be used as an index. When a logical vector is used as an index for subsetting, the resulting subset will include the elements that correspond to the TRUE positions in the index. For example:\n\nCodecounts = c(2, 0, 3, 1, 3)\n\n\n\nCodecounts[c(TRUE, FALSE, TRUE, FALSE, FALSE)] ## [1] 2 3\n\n[1] 2 3\n\n\nHere is another example, where the logical vector of indices is created from the same vector being subsetted:\n\nCodecounts[counts &lt; 3] ## [1] 2 0 1\n\n[1] 2 0 1\n\n\nIn this example, the logical vector counts&lt;3:\n\nCodecounts &lt; 3 ## [1]  TRUE  TRUE FALSE  TRUE FALSE\n\n[1]  TRUE  TRUE FALSE  TRUE FALSE\n\n\nspecifies whether to include each of the elements of counts in the resulting subset (Figure 2.10).\n\nWhat does the expression counts[counts&lt;3] do, in plain language?\n\nFigure 2.10: Subsetting with a logical vector\nHere are some more examples of subsetting with a logical index:\n\nCodex = 1:10 \nx \n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nCodex[x &gt;= 3]         # Elements of 'x' greater or equal than 3 ## [1]  3  4  5  6  7  8  9 10\n\n[1]  3  4  5  6  7  8  9 10\n\n\n\nCodex[x != 2]         # Elements of 'x' not equal to 2  ## [1]  1  3  4  5  6  7  8  9 10\n\n[1]  1  3  4  5  6  7  8  9 10\n\n\n\nCodex[x &gt; 4 | x &lt; 2]  # Elements of 'x' greater than 4 or smaller than 2 ## [1]  1  5  6  7  8  9 10\n\n[1]  1  5  6  7  8  9 10\n\n\n\nCodex[x &gt; 4 & x &lt; 2]  # Elements of 'x' greater than 4 and smaller than 2 ## integer(0)\n\ninteger(0)\n\n\n\nWhat does the output integer(0), which we got in the last expression, mean?\n\nThe next example is slightly more complex; we select the elements of z whose square is larger than 8:\n\nCodez = c(5, 2, -3, 8) \nz[z^2 &gt; 8] ## [1]  5 -3  8\n\n[1]  5 -3  8\n\n\nLet’s go over this step-by-step. First, z^2 gives a vector of squared z values (2 is recycled):\n\nCodez^2 ## [1] 25  4  9 64\n\n[1] 25  4  9 64\n\n\nThen, each of the squares is compared to 8 (8 is recycled):\n\nCodez^2 &gt; 8 ## [1]  TRUE FALSE  TRUE  TRUE\n\n[1]  TRUE FALSE  TRUE  TRUE\n\n\nFinally, the logical vector z^2&gt;8 is used for subsetting z.\n\n3.2.11 Missing values\nMissing values, often represented as NA in R, are used to indicate the absence or unavailability of data for a particular element in a vector. Missing values play a crucial role in handling incomplete or undefined information within datasets. It\n\naccepts a vector, of any type, and\nreturns a logical vector, with TRUE in place of NA values and FALSE in place of non-NA values.\n\nFor example, suppose we have a vector x where some of the values are missing:\n\nCodex = c(28, 58, NA, 31, 39, NA, 9) \nx ## [1] 28 58 NA 31 39 NA  9\n\n[1] 28 58 NA 31 39 NA  9\n\n\nThe is.na function can be used to detect which values are missing:\n\nCodeis.na(x) ## [1] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE\n\n[1] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE\n\n\n\nHow can we use the above expression to subset the non-missing values of x?\n\nMany of the functions that summarize vector properties, such as sum and mean, have a parameter called na.rm. The na.rm parameter is used to determine whether NA values are excluded from the calculation. The default is na.rm=FALSE, meaning that NA values are not excluded. For example:\n\nCodex = c(28, 58, NA, 31, 39, NA, 9) \nmean(x) \n\n[1] NA\n\n\n\nWhy do we get NA in the first expression?\n\n\nWhat do you think will be the result of length(x)?\n\n\nHow can we replace the NA values in x with the mean of its non-NA values?"
  },
  {
    "objectID": "ch2.html#additional-useful-functions",
    "href": "ch2.html#additional-useful-functions",
    "title": "\n3  Vectors\n",
    "section": "\n3.3 Additional useful functions",
    "text": "3.3 Additional useful functions\n\n3.3.1 any and all\n\nSometimes we want to figure out whether a logical vector:\n\ncontains at least one TRUE value; or\nis entirely composed of TRUE values.\n\nWe can use the any and all functions, respectively, to do those things.\nThe any function returns TRUE if at least one of the input vector values is TRUE, otherwise it returns FALSE. For example, let’s take a numeric vector x:\n\nCodex = c(2, 6, 2, 3, 0, 1, 6) \nx ## [1] 2 6 2 3 0 1 6\n\n[1] 2 6 2 3 0 1 6\n\n\nThe expression any(x &gt; 5) returns TRUE, which means that the vector x &gt; 5 contains at least one TRUE value, i.e., at least one element of x is greater than 5:\n\nCodex &gt; 5 \n\n[1] FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE\n\n\nThe expression any(x &gt; 88) returns FALSE, which means that the vector x &gt; 88 contains no TRUE values, i.e., none of the elements of x are greater than 88:\n\nCodex &gt; 88 ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE any(x &gt; 88) ## [1] FALSE\n\n[1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nThe all function returns TRUE if all of the input vector values are TRUE, otherwise it returns FALSE. For example, the expression all(x &gt; 5) returns FALSE, which means that the vector x &gt; 5 contains at least one FALSE value, i.e., not all elements of x are greater than 5:\n\nCodex &gt; 5 ## [1] FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE all(x &gt; 5) ## [1] FALSE\n\n[1] FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE\n\n\nThe expression all(x &gt; -1) returns TRUE, which means that x &gt; -1 is composed entirely of TRUE values, i.e., all elements of x are greater than -1:\n\nCodex &gt; -1 \n\n[1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\n\nIn a way, any and all are inverse:\n\nany determines if the logical vector contains at least one TRUE value.\nall determines if the logical vector contains at least one FALSE value.\n\n3.3.2 which\n\nThe which function converts a logical vector to a numeric one, with the indices of TRUE values. That way, we can find out the index of values that satisfy a given condition. For example, considering the vector x:\n\nCodex ## [1] 2 6 2 3 0 1 6\n\n[1] 2 6 2 3 0 1 6\n\n\nthe expression which(x &gt; 2.3) returns the indices of TRUE elements in x &gt; 2.3, i.e., the indices of x elements which are greater than 2.3:\n\nCodex &gt; 2.3 ## [1] FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE which(x &gt; 2.3) ## [1] 2 4 7\n\n[1] FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE\n\n\n\n3.3.3 which.min and which.max\n\nRelated to the which function, there are two additional functions in R: which.min and which.max. These functions return the index of the first occurrence of the minimum or maximum value in a vector, respectively.\nHere’s an example illustrating the usage of which.min and which.max functions with a vector x:\n\nCodex \n\n[1] 2 6 2 3 0 1 6\n\n\nusing which.min we can find out that the minimal value of x is in the 5th position:\n\nCodewhich.min(x) \n\n[1] 5\n\n\nwhile using which.max we can find out that the maximal value of x is in the 2nd position:\n\nCodewhich.max(x) \n\n[1] 2\n\n\n\n\n\n\n3.3.4 The order function\nThe order function returns ordered vector indices, based on the order of vector values. In other words, order gives the index of the smallest value, the index of the second smallest value, etc., up to the index of the largest value. For example, given the vector x:\n\nCodex ~ [1] 2 6 2 3 0 1 6\n\n\norder(x) returns the indices 1:length(x), ordered from smallest to largest value:\n\nCodeorder(x) ## [1] 5 6 1 3 4 2 7\n\n[1] 5 6 1 3 4 2 7\n\n\nThis result tells us that the 5th element of x is the smallest, the 6th is the second smallest, and so on.\nWe can also get the reverse order with decreasing=TRUE:\norder(x, decreasing = TRUE) ## [1] 2 7 4 1 3 6 5\n\nHow can we get a sorted vector of elements from x, as shown below, using the order function?\n\n\nCode## [1] 0 1 2 2 3 6 6\n\n\n\n3.3.5 paste and paste0\n\nThe paste function is used to “paste” text values. Its sep parameter determines the separating character(s), with the default being sep=\" \" (a space). For example:\n\nCodepaste(\"There are\", \"5\", \"books.\") ## [1] \"There are 5 books.\" paste(\"There are\", \"5\", \"books.\", sep = \"_\") ## [1] \"There are_5_books.\"\n\n[1] \"There are 5 books.\"\n\n\nNon-character vectors are automatically converted to character before pasting:\n\nCodepaste(\"There are\", 80, \"books.\") ## [1] \"There are 80 books.\"\n\n[1] \"There are 80 books.\"\n\n\nThe recycling rule applies in paste too:\n\nCodepaste(\"image\", 1:5, \".tif\", sep = \"\") ## [1] \"image1.tif\" \"image2.tif\" \"image3.tif\" \"image4.tif\" \"image5.tif\"\n\n[1] \"image1.tif\" \"image2.tif\" \"image3.tif\" \"image4.tif\" \"image5.tif\"\n\n\nThe paste0 function is a shortcut for paste with sep=\"\":\n\nCodepaste0(\"image\", 1:5, \".tif\") ## [1] \"image1.tif\" \"image2.tif\" \"image3.tif\" \"image4.tif\" \"image5.tif\"\n\n[1] \"image1.tif\" \"image2.tif\" \"image3.tif\" \"image4.tif\" \"image5.tif\""
  },
  {
    "objectID": "ch3.html#dates",
    "href": "ch3.html#dates",
    "title": "\n4  Time and Date\n",
    "section": "\n4.1 Dates",
    "text": "4.1 Dates\nDates play a crucial role in data analysis and represent an essential component in many datasets. In R, working with dates is made easy through a range of functions and packages specifically designed for handling date-related operations. Dates in R are represented as objects, allowing for convenient manipulation and calculations.\nWith R’s built-in date functions and packages like lubridate, users can create, format, and perform various operations on date objects. These operations include extracting components like year, month, and day, performing arithmetic operations to calculate time intervals, comparing dates, and visualizing temporal data. With the flexibility and functionality offered by R, users can effectively work with dates to gain valuable insights and make informed decisions based on temporal patterns in their data.\n\n4.1.1 Date and time classes in R\nIn R, there are specific classes designed for representing times and time-series data. These classes provide specialized functionalities for handling temporal information. Some of the commonly used classes include:\n\nPOSIXct: This class represents time as the number of seconds since January 1, 1970 (known as the Unix epoch) in the UTC time zone. It offers precision up to the second and is suitable for handling timestamps.\nPOSIXlt: Similar to the POSIXct class, POSIXlt represents time as the number of seconds since the Unix epoch. However, it provides more detailed components like year, month, day, hour, minute, and second. POSIXlt objects are useful when you need to work with individual time components.\nDate: The Date class is used to represent dates without time information. Dates are stored as the number of days since January 1, 1970. This class is useful when you want to work with dates and perform date-specific operations.\nTime-Series: R provides the ts class to handle time-series data, which is a combination of time and corresponding data values. The ts class allows for efficient storage and manipulation of time-series data and supports various time-based operations.\nxts: The xts (eXtensible Time Series) class is an extension of the ts class and provides enhanced functionality for working with time-series data. It includes features like subsetting, merging, and advanced time-based calculations.\n\nThese specialized classes make it easier to work with time and time-series data in R. In this book, we will only be working with the Date class, which is used to represent times of type date.\n\n4.1.2 Working with Date objects\n\n4.1.2.1 Today’s date\nThe Date data structure is the simplest and most basic way to represent dates in R. It is specifically designed to store dates without the inclusion of the time of day. One common use of Date is to represent calendar dates, such as birthdays, holidays, or any event that is not concerned with specific timestamps.\nTo obtain the current date in R, you can use the Sys.Date() function. This function retrieves the current system date and returns it as a Date object. Here’s an example:\n\nCodecurrent_date &lt;- Sys.Date()\ncurrent_date\n\n[1] \"2023-06-19\"\n\n\nThe output will be the current date in the format YYYY-MM-DD, where YYYY represents the year, MM represents the month, and DD represents the day. By utilizing the Sys.Date() function, you can easily retrieve the current date for various purposes, such as tracking data entry, performing date-based calculations, or generating reports based on the current date.\n\nCodecurrent_date |&gt;\n  class()\n\n[1] \"Date\"\n\n\n\n4.1.2.2 Converting character to Date\n\nIn addition to retrieving the current date using Sys.Date(), we can also convert character values to Date objects using the as.Date() function. This allows us to create Date objects representing specific dates of our choice.\nThe as.Date() function takes a character value as input and converts it to a Date object based on the specified format. The format is specified using the format parameter, which follows the conventions of the strftime() function in C. Here’s an example:\n\nCodecustom_date &lt;- as.Date(\"2023-05-24\", format = \"%Y-%m-%d\")\ncustom_date\n\n[1] \"2023-05-24\"\n\n\nIn this example, the character value “2023-05-24” is converted to a Date object using the %Y-%m-%d format, where %Y represents the year with century, %m represents the month, and %d represents the day.\nWhen the character values are in the standard date format (YYYY-MM-DD), such as in the above example, the as.Date function works without any additional arguments. However, when the character values are in a non-standard format, we need to specify the format definition with format, using the various date component symbols. Table [3.1] lists the most commonly used symbols for specifying date formats in R. The full list of symbols can be found in ?strptime.\n\n\n\n\n\n\n\n\n\n\nTable 3.1: Common Date format components\n\n\nSymbol\nExample\nMeaning\n\n\n%d\n\"15\"\nDay\n\n\n%m\n\"08\"\nMonth, numeric\n\n\n%b\n\"Aug\"\nMonth, 3-letter\n\n\n%B\n\"August\"\nMonth, full\n\n\n%y\n14\nYear, 2-digit\n\n\n%Y\n2014\nYear, 4-digit\n\n\n\nBefore going into examples of date formatting, it is useful to set the standard \"C\" locale in R. That way, we make sure that month or weekday names are interpreted in English as intended:\n\nCodeSys.setlocale(\"LC_TIME\", \"C\") ## [1] \"C\"\n\n[1] \"C\"\n\n\nFor example, converting the following character date—which is in a non-standard format—to Date fails when format is not specified:\n\nCodeas.Date(\"07/Aug/12\") \n\n# Error in charToDate(x) \n\n\nSpecifying the right format, which is \"%d/%b/%y\" in this case, leads to a successful conversion:\n\nCodeas.Date(\"07/Aug/12\", format = \"%d/%b/%y\") \n\n[1] \"2012-08-07\"\n\n\n\nWhat will be the result if we used format=\"%y/%b/%d\" (switching %d and %y) in the above expression?\n\nHere is another example with a different non-standart format (\"%Y-%B-%d\"):\n\nCodeas.Date(\"2012-August-07\")\n# Error in charToDate(x) \n\n\nUsing the anytime::anytime() function allows us to handle a broader range of date formats without explicitly specifying the format. It provides convenience and flexibility in parsing date strings and converting them to appropriate date objects.\n\nCodeanytime::anytime((\"2012-August-07\") )\n\n[1] \"2012-08-07 EAT\"\n\n\n\nCodeas.Date(\"2012-August-07\", format = \"%Y-%B-%d\") \n\n[1] \"2012-08-07\"\n\n\n\n4.1.2.3 Converting Date to character\n\nA Date can always be converted back to character using as.character:\n\nCoded = as.Date(\"1961-12-09\") \nd \n\n[1] \"1961-12-09\"\n\n\n\nCodeclass(d) \n\n[1] \"Date\"\n\n\n\nCodeas.character(d) \n\n[1] \"1961-12-09\"\n\n\n\nCodeclass(as.character(d)) ## [1] \"character\"\n\n[1] \"character\"\n\n\nNote that both the Date and the character objects are printed exactly the same way, so we have to use class to figure out which class we are dealing with.\nThe as.character function, by default, returns a text string with all three date components in the standard YYYY-MM-DD (or \"%Y-%m-%d\") format. Using the format argument, however, lets us compose different date formats, or extract individual date components out of a Date object:\n\nCoded \n\n[1] \"1961-12-09\"\n\n\n\nCodeas.character(d, format = \"%m/%Y\") \n\n[1] \"1961-12-09\"\n\n\n{r            as.character(d, \"%d\")\n{r            as.character(d, \"%B\")\n{r            as.character(d, \"%Y\")\nNote that as.character consistently returns a character, even when the result contains nothing but numbers, as in \"%d\" or \"%Y\". We can always convert from character to numeric with as.numeric if necessary:\n{r            as.numeric(as.character(d, \"%Y\"))\n\n4.1.2.4 Arithmetic operations with dates\nArithmetic operations can be performed on Date objects in R, allowing for calculations involving dates and time intervals. Here are some common arithmetic operations that can be applied to Date objects:\n\nAddition and Subtraction:\n\nAdding/Subtracting Days: You can add or subtract a certain number of days to/from a Date object using the + and - operators. For example:\n\n\n\n\nCode     date1 &lt;- as.Date(\"2023-05-24\")\n     date2 &lt;- date1 + 7   # Adds 7 days to date1\n     date3 &lt;- date1 - 3   # Subtracts 3 days from date1\n\n\n\nAdding/Subtracting Weeks/Months/Years: Similarly, you can add or subtract weeks, months, or years using the same operators. R automatically adjusts the resulting date according to the specified time interval. For example:\n\n\nCode     date4 &lt;- date1 + lubridate::weeks(2)   # Adds 2 weeks to date1\n     date5 &lt;- date1 - lubridate::month(6)  # Subtracts 6 months from date1\n     date6 &lt;- date1 + lubridate::years(1)   # Adds 1 year to date1\n\n\n\nCalculating Time Differences: You can calculate the time difference between two Date objects using the - operator. The result will be an object of class difftime, representing the difference in days. For example:\n\n\nCode   date1 &lt;- as.Date(\"2023-05-24\")\n   date2 &lt;- as.Date(\"2023-06-01\")\n   diff &lt;- date2 - date1   # Calculates the difference in days between date2 and date1\n   diff\n\nTime difference of 8 days\n\n\n\nComparing Dates: Dates can be compared using comparison operators (&lt;, &lt;=, &gt;, &gt;=, ==, !=) to check their relative order. This is useful for sorting or filtering data based on dates. For example:\n\n\nCode   date1 &lt;- as.Date(\"2023-05-24\")\n   date2 &lt;- as.Date(\"2023-06-01\")\n   date1 &lt; date2   # Returns TRUE as date1 is earlier than date2\n\n[1] TRUE\n\n\n\n4.1.2.5 Arithmetric operation with lubridate package\nThe lubridate package introduces intuitive functions like ymd() (year-month-day) for creating Date objects with a specified format. It also provides convenient helper functions such asweeks(), months(), and years() to perform arithmetic operations on dates. These functions enhance readability and simplify the code when working with dates and time intervals.\n\nCode   library(lubridate)\n\n\nBy utilizing the lubridate package, you can handle date arithmetic operations more intuitively and efficiently, making it easier to perform calculations and comparisons involving dates in your R code. Here are examples of arithmetic operations with dates using the lubridate package:\n\nAddition and Subtraction:\n\n\nCode   date1 &lt;- ymd(\"2023-05-24\")\n   date2 &lt;- date1 + weeks(2)   # Adds 2 weeks to date1\n   date1 - months(6)  # Subtracts 6 months from date1\n\n[1] \"2022-11-24\"\n\n\n\nCalculating Time Differences:\n\n\nCode   date1 &lt;- ymd(\"2023-05-24\")\n   date2 &lt;- ymd(\"2023-06-01\")\ndate2 - date1   # Calculates the difference in days between date2 and date1\n\nTime difference of 8 days\n\n\n\nComparing Dates:\n\n\nCode   date1 &lt;- ymd(\"2023-05-24\")\n   date2 &lt;- ymd(\"2023-06-01\")\n   date1 &lt; date2   # Returns TRUE as date1 is earlier than date2\n\n[1] TRUE\n\n\nFinally, the seq function, which we are already familiar with, can be used to create a sequence of consecutive dates in R. By specifying the starting date, ending date, and the desired interval, we can generate a sequence of dates.\nFor example, the following expression creates a sequence of dates from January 1, 2023, to January 10, 2023, with a daily interval:\n\nCodeseq(ymd(\"2023-01-01\"), ymd(\"2023-01-10\"), by = \"day\")\n\n [1] \"2023-01-01\" \"2023-01-02\" \"2023-01-03\" \"2023-01-04\" \"2023-01-05\"\n [6] \"2023-01-06\" \"2023-01-07\" \"2023-01-08\" \"2023-01-09\" \"2023-01-10\"\n\n\nThe output is a sequence of Date objects representing consecutive dates from January 1, 2023, to January 10, 2023.\nBy adjusting the starting and ending dates and specifying a different interval (e.g., “week”, “month”, “year”), you can generate sequences of dates that suit your specific needs. This can be useful for generating date ranges, creating time-based indices, or iterating over a series of dates for analysis or visualization purposes.\n\n4.1.3 Time series\nA time series is a sequence of data points collected and recorded over a period of time. In R, time series data can be represented and analyzed using specialized data structures and functions. The two main data structures for handling time series data in R are:\n\n\nts (Time Series): The ts object is a basic data structure in R for representing univariate time series. It is created using the ts() function, which takes the data as input and optionally accepts arguments for specifying the frequency and start time of the time series. Here’s an example of creating a ts object:\n\n\nCodedata &lt;- c(10, 15, 20, 25, 30)\nts_data &lt;- ts(data, start = c(2020, 1), frequency = 1)\nts_data\n\nTime Series:\nStart = 2020 \nEnd = 2024 \nFrequency = 1 \n[1] 10 15 20 25 30\n\n\nIn this example, the ts() function is used to create a time series object ts_data from the numeric vector data. The start argument specifies the start time of the series (January 2020 in this case), and the frequency argument indicates the number of observations per unit of time (1 observation per year).\n\n\nxts (eXtensible Time Series): The xts object is a more powerful and flexible data structure for handling time series data in R. It is part of the xts package, which extends the functionality of ts objects by providing additional features and methods for working with time series data. xts objects are created using the xts() function, and they can store both univariate and multivariate time series data. Here’s an example of creating an xts object:\n\n\nCode# Load the xts package\nlibrary(xts)\n\n# Create an xts object\ndata &lt;- c(10, 15, 20, 25, 30)\ntime &lt;- as.Date(c(\"2020-01-01\", \"2020-02-01\", \"2020-03-01\", \"2020-04-01\", \"2020-05-01\"))\nxts_data &lt;- xts(data, order.by = time)\nxts_data\n\n           [,1]\n2020-01-01   10\n2020-02-01   15\n2020-03-01   20\n2020-04-01   25\n2020-05-01   30\n\n\nIn this example, the xts() function is used to create an xts object xts_data. The data argument contains the time series data, and the order.by argument specifies the corresponding time stamps for each observation.\nBoth ts and xts objects offer various methods and functions for analyzing and manipulating time series data in R. These include plotting, statistical analysis, time-based subsetting, aggregation, and more. Time series analysis techniques such as forecasting, seasonal decomposition, and autocorrelation analysis can also be applied to these data structures.\nHere are a few more examples of working with time series using the ts package in R:\n\nCreating a Monthly Time Series:\n\n\nCode# Generate monthly time series data\n   \nts_data &lt;- rnorm(n = 233, mean = 25, sd = 4) |&gt;\nts(start = c(2004,1), frequency = 12)\nts_data\n\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n2004 27.34499 27.17237 24.62156 21.68477 20.19828 20.58288 29.62347 21.46734\n2005 25.64082 24.50923 32.19372 31.39395 23.30834 31.38716 24.23200 25.25643\n2006 26.73120 20.68608 22.78567 27.82451 25.88936 27.15770 31.47742 20.63695\n2007 18.30026 19.14970 31.61360 19.66005 22.62929 23.90140 23.94396 26.00029\n2008 32.27445 28.59940 24.63599 26.79094 24.47729 21.65715 25.65079 29.17468\n2009 25.07763 26.88618 26.16901 31.86157 21.30014 25.47337 26.19258 23.78133\n2010 22.66916 19.60314 25.49854 30.30652 35.29321 19.35570 24.40690 20.79529\n2011 33.61163 21.58291 31.81660 25.72493 24.47594 24.80412 26.76041 17.74855\n2012 27.09957 23.20298 27.96231 24.02154 21.98870 27.29386 16.72594 22.39374\n2013 21.28865 23.50462 25.39106 28.05074 25.20278 26.33608 26.52864 26.52343\n2014 30.54998 30.54852 27.67911 26.66169 23.94412 24.89157 24.39752 27.32481\n2015 20.05773 15.58428 23.33754 19.03844 24.92283 22.59505 27.21832 32.05114\n2016 26.49166 25.00079 22.65979 29.10551 30.00607 23.42811 30.26670 16.45889\n2017 29.35224 21.08609 29.37130 32.93503 21.61618 17.98721 23.96567 24.41360\n2018 22.19059 25.36744 13.37039 27.78744 27.72380 23.02264 19.81353 30.11909\n2019 21.38398 27.44787 20.29182 25.14146 20.68146 27.02965 24.53692 24.94370\n2020 17.02259 29.37203 25.61750 23.80800 34.74780 19.44434 21.03135 21.72338\n2021 21.92506 32.04422 31.67719 28.24664 31.03517 24.46138 19.79447 26.66287\n2022 22.21422 28.57109 22.22410 20.57094 20.37970 24.35361 26.91857 24.50356\n2023 22.08677 24.01050 25.19480 22.36049 21.97301                           \n          Sep      Oct      Nov      Dec\n2004 24.22153 21.62754 23.86859 21.87370\n2005 23.44570 21.92880 28.60020 23.15030\n2006 19.47601 21.27150 25.68546 23.80836\n2007 27.38850 21.55739 32.18209 21.78651\n2008 25.25811 22.70393 29.54767 20.79304\n2009 33.79045 26.74404 22.57881 24.22942\n2010 28.13136 19.83723 28.42925 24.71715\n2011 24.55229 27.59187 23.74834 25.09575\n2012 23.17996 19.67532 19.64619 24.40616\n2013 22.59562 21.38334 21.14929 20.95980\n2014 23.91062 29.93282 21.85606 27.57417\n2015 23.15978 30.68149 31.88257 27.69774\n2016 24.96479 29.96906 22.34244 20.73868\n2017 28.88316 21.37133 22.12182 21.71374\n2018 20.41044 27.48209 24.01053 18.54786\n2019 21.17700 21.39315 27.53830 15.95588\n2020 30.28193 23.46485 24.80147 24.69073\n2021 26.76542 23.82328 21.18727 21.62664\n2022 23.58284 24.91281 22.19251 22.21999\n2023                                    \n\n\n2.Subsetting Time Series Data\nSubsetting time series data involves extracting a portion of the data based on specific criteria, such as a range of dates or a subset of observations. In R, subsetting time series data can be done using various approaches.\n\nCode   # Subset time series data for a specific period\nsubset_data &lt;- ts_data |&gt;\n  window(start = c(2020, 1), end = c(2021, 6))\nsubset_data\n\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n2020 17.02259 29.37203 25.61750 23.80800 34.74780 19.44434 21.03135 21.72338\n2021 21.92506 32.04422 31.67719 28.24664 31.03517 24.46138                  \n          Sep      Oct      Nov      Dec\n2020 30.28193 23.46485 24.80147 24.69073\n2021                                    \n\n\n\nPlotting Time Series Data\n\nPlotting time series data is a common task in analyzing and visualizing temporal patterns and trends. In R, there are various packages and functions available for creating plots of time series data. Let’s explore two commonly used plotting methods: base R plots and ggplot2 package.\n\n\n\n\nFIGURE 4.1. Time series plotted with base-R\n\n\n\n\n\n\n\n\nFIGURE 4.2. Time series plotted with ggplot\n\n\n\n\n\n\nCalculating Summary Statistics: When working with time series data, calculating summary statistics often involves considering specific aspects related to the temporal dimension. Here are some common summary statistics for time series data\n\n\nCode   # Calculate summary statistics of the time series data\n   summary(ts_data)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  13.37   21.79   24.41   24.70   27.29   35.29 \n\n\n\n\nApplying Smoothing Techniques: Smoothing techniques are commonly used in time series analysis to reduce noise, identify patterns, and reveal underlying trends in the data. In R, there are several smoothing techniques available that can be applied to time series data. Let’s explore some commonly used smoothing techniques and their implementation in R\n\n\nCode   # Apply moving average smoothing to the time series data\n   smoothed_data &lt;- filter(ts_data, filter = rep(1/3, 3), sides = 2)\n\n\n\n\n\n\nFIGURE 4.3. Smoothed time series\n\n\n\n\n\n\nDecomposing Time Series Components:\n\n\nCode   # Decompose the time series data into trend, seasonal, and random components\n   decomposition &lt;- decompose(ts_data)\n   trend &lt;- decomposition$trend\n   seasonal &lt;- decomposition$seasonal\n   random &lt;- decomposition$random\n\n\n\n4.1.4 Applying Smoothing Techniques\nSmoothing techniques are commonly used in time series analysis to reduce noise, identify patterns, and reveal underlying trends in the data. In R, there are several smoothing techniques available that can be applied to time series data. Let’s explore some commonly used smoothing techniques and their implementation in R:\n\n\nMoving Average: The moving average is a simple smoothing technique that calculates the average of a fixed window of observations. It helps to smooth out short-term fluctuations and highlight the overall trend. In R, you can apply the moving average using the ma() function from the forecast package. For example:\n\n   # Apply moving average smoothing\n   smoothed_data &lt;- ma(ts_data, order = 5)  # Moving average with a window of size 5\n\n\nExponential Smoothing: Exponential smoothing is a popular smoothing technique that assigns exponentially decreasing weights to past observations. It places more emphasis on recent observations and is suitable for data with a changing trend. In R, you can apply exponential smoothing using the ets() function from the forecast package. For example:\n\n\nCode   # Apply exponential smoothing\nsmoothed_data &lt;- ts_data |&gt;\n  forecast::ets()\n\n\n\n\n\n\nFIGURE 4.4. ETS smoothed time series\n\n\n\n\n\n\nLoess Smoothing: Loess smoothing is a non-parametric technique that fits a smooth curve through the data using local weighted regression. It is effective in capturing complex patterns and is particularly useful for data with irregular or nonlinear trends. In R, you can apply loess smoothing using the loess() function. For example:\n\n\nCode   # Apply loess smoothing\n   smoothed_data &lt;- loess(ts_data ~ time(ts_data))\n\n\n\n\n\n\nFIGURE 4.5. loess smoothed time series\n\n\n\n\n\n\nSpline Smoothing: Spline smoothing is a flexible technique that fits a piecewise polynomial function to the data. It is useful for capturing both local and global trends in the time series. In R, you can apply spline smoothing using the smooth.spline() function. For example:\n\n\nCode# Apply spline smoothing\n   smoothed_data &lt;- smooth.spline(time(ts_data), ts_data)\n\n\n\n\n\n\nFIGURE 4.6. Spline smoothed"
  },
  {
    "objectID": "ch3Edition.html#importing-data",
    "href": "ch3Edition.html#importing-data",
    "title": "5  Data types and data frame",
    "section": "\n5.1 Importing Data",
    "text": "5.1 Importing Data\nSo far, we’ve looked at several dataset in previous chapter and we have also created ourselves some datasets. While you can do all your data entry work in R or Excel, it is much more common to load data from other sources. Local and international organization have been collecting fisheries dependent and fisheries independent data for years. These historical dataset with fisheries information like fish catch, effort, landing sites, fishing ground and critical habitats can be obtained from several databases—some are open and other closed. Much of the data we download or receive from is either comma-separated value files .csv or and Excel spreadsheets, .xlsx. .csv files are spreadsheets stored as text files - basically Excel files stripped down to the bare minimum - no formatting, no formulas, no macros. You can open and edit them in spreadsheet software like LibreOffice Calc, Google Sheets or Microsoft Excel. Many devices and databases can export data in .csv format, making it a commonly used file format that you are likely to encounter sooner rather than later.\nWhether that be a comma separated (csv) or a tab delimited file, there are multiple functions that can read these data into R. We will stick to loading these data from the tidyverse packages but be aware these are not the only methods for doing this. We will use the tidyverse functions just to maintain consistency with everything else we do. The first package in tidyverse we will use is called readr (Wickham, Hester, and Bryan 2022), which is a collection of functions to load the tabular data from working directory in our machine into R session. Some of its functions include:\n\n\nread_csv(): comma separated (CSV) files\n\nread_tsv(): tab separated files\n\nread_delim(): general delimited files\n\nread_fwf(): fixed width files\n\nread_table(): tabular files where columns are separated by white-space.\n\nread_log(): web log files\n\nreadxl reads in Excel files.\n\nBefore we import the data, we need to load the packages that we will use their functions in this chapter\n\nCoderequire(tidyverse)\nrequire(magrittr)\n\n\n\n5.1.1 Importing csv files\nA CSV file is a type of file where each line contains a single record, and all the columns are separated from each other via a comma. In order to load data from a file into R, you need its path - that is, you need to tell R where to find the file. Unless you specify otherwise, R will look for files in its current working directory. You can read .csv file using read_csv() function of the readr package (Wickham, Hester, and Bryan 2022) as shown in the chunk below;\n\nCodeimported.lfq = read_csv(\"dataset/project/tidy_LFQ_sample_4.csv\")\n\nlf4\n\n\nWe imported tidy_LFQ_sample_4.csv from working directory into R using read_csv() and specify the path to the file in your working directory and store as imported.lfq. If you get an error message, it means thattidy_LFQ_sample_4.csvis not in your working directory. Either move the file to the right directory (remember, you can use rungetwd()` to see what your working directory is) or change your working directory.\n\nCodeimported.lfq = read_csv(\"data/tidy/tidy_LFQ_sample_4.csv\")\n\n\nIf you glimpse the dataframe with glimpse() function, you should see the internal structure of the imported.lfq object we just loaded;\n\nCodeimported.lfq %&gt;% \n  glimpse()\n\nRows: 6,185\nColumns: 6\n$ site  &lt;chr&gt; \"Mombasa\", \"Mombasa\", \"Mombasa\", \"Mombasa\", \"Mombasa\", \"Mombasa\"…\n$ date  &lt;date&gt; 2019-04-05, 2019-04-05, 2019-04-05, 2019-04-05, 2019-04-05, 201…\n$ tl_mm &lt;dbl&gt; 184, 185, 145, 189, 175, 165, 181, 176, 164, 154, 188, 186, 179,…\n$ fl_mm &lt;dbl&gt; 169, 169, 134, 173, 161, 153, 165, 163, 148, 142, 173, 173, 164,…\n$ wt_gm &lt;dbl&gt; 59.50, 54.71, 24.15, 61.36, 49.31, 38.54, 49.68, 45.27, 36.26, 3…\n$ sex   &lt;chr&gt; \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\",…\n\n\nThe dataset contains six variables and 6,185 records. The variables site and sex both contain text, and have been imported as character vectors4. The date column has been imported as date format, the variable tl_mm and fl_mm are measured length and have been imported as numeric vector measured in millimeters. The variable wt_gm is the weight of fish measured in grams and also have been imported as numeric vector.\nSo, what can you do in case you need to import data from a file that is not in your working directory? This is a common problem, as many of us store script files and data files in separate folders (or even on separate drives). One option is to use file.choose, which opens a pop-up window that lets you choose which file to open using a graphical interface:\nimported.lfq2 = read_csv(file.choose())\nThis solution work just fine if you just want to open a single file once. But if you want to reuse your code or run it multiple times, you probably don’t want to have to click and select your file each time. Instead, you can specify the path to your file in the call to read_csv.\n\n5.1.2 Importing Excel files\nCommonly our data is stored as a Excel file. There are several packages that can be used to import Excel files to R. I prefer the readxl package (Wickham and Bryan 2022), so let’s install that:\ninstall.packages(\"readxl\")\nThe package has read_exel() function that allows us to specify which sheet within the Excel file to read. The function automatically convert the worksheet into a .csv file and read it. Let’s us import the the data in first sheet of the tidy_LFQ_sample_4.xlsx. Is a similar dataset that just imported in the previous section, but is in Excel format. We will use this file to illustrate how to import the excel file into R workspace with readxl package (Wickham and Bryan 2022).\n\nCodeimported.lfq = readxl::read_excel(\"data/tidy/tidy_LFQ_sample_4.xlsx\", sheet = 1)\n\n\n\nCodeimported.lfq\n\n# A tibble: 6,185 × 6\n   site    date                tl_mm fl_mm wt_gm sex  \n   &lt;chr&gt;   &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n 1 Mombasa 2019-04-05 00:00:00   184   169  59.5 M    \n 2 Mombasa 2019-04-05 00:00:00   185   169  54.7 M    \n 3 Mombasa 2019-04-05 00:00:00   145   134  24.2 M    \n 4 Mombasa 2019-04-05 00:00:00   189   173  61.4 M    \n 5 Mombasa 2019-04-05 00:00:00   175   161  49.3 M    \n 6 Mombasa 2019-04-05 00:00:00   165   153  38.5 M    \n 7 Mombasa 2019-04-05 00:00:00   181   165  49.7 M    \n 8 Mombasa 2019-04-05 00:00:00   176   163  45.3 M    \n 9 Mombasa 2019-04-05 00:00:00   164   148  36.3 M    \n10 Mombasa 2019-04-05 00:00:00   154   142  31.9 M    \n# ℹ 6,175 more rows\n\n\n\nCodeimported.lfq %&gt;% \n  skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n6185\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\nPOSIXct\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nsite\n0\n1\n3\n7\n0\n2\n0\n\n\nsex\n0\n1\n1\n1\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\ntl_mm\n0\n1\n170.77\n21.08\n97.0\n157.00\n171.00\n183.00\n269.00\n▁▅▇▁▁\n\n\nfl_mm\n0\n1\n156.00\n19.26\n18.1\n144.00\n156.00\n168.00\n241.00\n▁▁▅▇▁\n\n\nwt_gm\n0\n1\n46.03\n19.51\n7.0\n32.77\n43.59\n55.28\n194.18\n▇▆▁▁▁\n\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\ndate\n0\n1\n2016-03-31\n2020-09-11\n2020-02-25\n42"
  },
  {
    "objectID": "ch3Edition.html#saving-and-exporting-your-data",
    "href": "ch3Edition.html#saving-and-exporting-your-data",
    "title": "5  Data types and data frame",
    "section": "\n5.2 Saving and exporting your data",
    "text": "5.2 Saving and exporting your data\nIn many a case, data manipulation is a huge part of statistical work, and of course you want to be able to save a data frame after manipulating it. There are two options for doing this in R - you can either export the data as e.g. a .csv or a .xlsx file, or save it in R format as an .RData file.\n\n5.2.1 Exporting data\nJust as we used the functions read_csv and read_excel to import data, we can use write_csvto export it. The code below saves the bookstore data frame as a .csv file file, which will be created in the current working directory. If you wish to store\n\nimported.lfq %&gt;%  write_csv(\"assets/fao_paul_dataset/tidy/tidy_lfq.csv\")\n\n5.2.2 Saving and loading R data\nBeing able to export to different spreadsheet formats is very useful, but sometimes you want to save an object that can’t be saved in a spreadsheet format. For instance, you may wish to save a multiple processed data, functions and formula that you’ve created. .RData files can be used to store one or more R objects. To save the objects bookstore and age in a .Rdata file, we can use the save function:\n\nsave.image(\"assets/fao_paul_dataset/tidy/myData.RData\")\n\n\n\n\nMüller, Kirill, and Hadley Wickham. 2022. Tibble: Simple Data Frames. https://CRAN.R-project.org/package=tibble.\n\n\nWickham, Hadley, and Jennifer Bryan. 2022. Readxl: Read Excel Files. https://CRAN.R-project.org/package=readxl.\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2022. Readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr."
  },
  {
    "objectID": "ch4.html#why-use-dplyr",
    "href": "ch4.html#why-use-dplyr",
    "title": "6  Data frame Manipulation",
    "section": "\n6.1 Why use dplyr?",
    "text": "6.1 Why use dplyr?\nUsing this package’s functions will allow you to code expressively—code that are easy to write and read, which make you effective and efficient data scientists.\n\nGreat for data exploration and manipulation\nIntuitive to write and easy to read, especially when using the chaining syntax\nFast on data frame—tabular dataset"
  },
  {
    "objectID": "ch4.html#core-dplyr-functions",
    "href": "ch4.html#core-dplyr-functions",
    "title": "6  Data frame Manipulation",
    "section": "\n6.2 Core dplyr Functions",
    "text": "6.2 Core dplyr Functions\nI will not go through all of the dplyr functions in this chapter. I will demonstrate the core functions that are used regularly for manipulating data. The five core functions also called verbs include:\n\n\nselect() to select columns based on their names\n\nfilter() to rows in data frame\n\narrange() to re-order or arrange the rows in ascending or descending order\n\nmutate() to create new columns—add new variable\n\nsummarise() to make a summary of variable(s)\n\ngroup_by() to group observation\n\nsample_n() and rename()to make random sample from the data set\n\nThe group_by() function perform other common task which are related to the split-apply-combine concept. You can use these verbs when you describe the algorithm or process for interrogating data, and then use dplyr verbs to write code that will closely follow your “plain language” description because it uses functions and procedures that share the same language.\nFor most of us who are familiar with the R base function, you will find that most dplyr functions on data frames can be expressed succinctly because you don’t need to repeat the name of the data frame. This becomes handy in operation, because dplyr package comes with the pipe operateor %&gt;% from the magrittr package (Bache and Wickham 2022), which allows to combine several functions in a chain to manipulate data.\nTo use dplyr functions to manipulate your data, it must be already installed in your machine so that you can load with a require () function. Once the package is loaded, its functions are available for use. dplyr is a key package of the tidyverse ecosystem—a collection of R packages, which also includes other packages like, readr (Wickham, Hester, and Bryan 2022), purr,tibble (Müller and Wickham 2022), stringr (Wickham 2019), forcats, tidyr (Wickham and Girlich 2022) and ggplot2 (Wickham 2016).\n\nCoderequire(tidyverse)"
  },
  {
    "objectID": "ch4.html#data",
    "href": "ch4.html#data",
    "title": "6  Data frame Manipulation",
    "section": "\n6.3 Data",
    "text": "6.3 Data\nData frames are ideal for representing data where each row is an observations and each column is a variable. Nearly all packages in a tidyverse work on data frames new version called tibble. A tibble provides stricter checking and better formatting than the traditional data frame.\nTo demonstrate the usefulness of the dplyr package for manipulating data, we will use the CTD data of 22 stations casted along the coastal water of Tanzania. I have prepared the data, cleaned and align the profile into 5 meter standard depth for each cast and merged them into a single .csv file. You need to load the file into your R session. We can import the file with read_csv() function from the readr package (Wickham, Hester, and Bryan 2022). The read_csv() function gives out a tibble (Müller and Wickham 2022)."
  },
  {
    "objectID": "ch4.html#choosing-rows-filtering-observations",
    "href": "ch4.html#choosing-rows-filtering-observations",
    "title": "6  Data frame Manipulation",
    "section": "\n6.4 Choosing rows: Filtering observations",
    "text": "6.4 Choosing rows: Filtering observations\nThe first dplyr verb we’ll explore is filter(). This function is primarily used to create a subset of observations that meet a specified conditions. The filter() function lets you pick out rows based on logical expressions. You give the function a predicate, specifying what a row should satisfy to be included. For instance, take a look at the chunk below:\n\nCodectd %&gt;% \n  filter(pressure &lt; 150)\n\n# A tibble: 637 × 12\n   station time                  lon   lat pressure temperature salinity oxygen\n   &lt;chr&gt;   &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n 1 st1     2004-08-18 15:27:46  40.6 -10.5        5        25.2     33.9   3.93\n 2 st1     2004-08-18 15:27:46  40.6 -10.5       10        25.1     34.9   4.49\n 3 st1     2004-08-18 15:27:46  40.6 -10.5       15        25.1     34.9   4.50\n 4 st1     2004-08-18 15:27:46  40.6 -10.5       20        25.0     34.9   4.51\n 5 st1     2004-08-18 15:27:46  40.6 -10.5       25        24.9     34.9   4.51\n 6 st1     2004-08-18 15:27:46  40.6 -10.5       30        24.9     34.9   4.50\n 7 st1     2004-08-18 15:27:46  40.6 -10.5       35        24.9     34.9   4.49\n 8 st1     2004-08-18 15:27:46  40.6 -10.5       40        24.9     34.9   4.48\n 9 st1     2004-08-18 15:27:46  40.6 -10.5       45        24.8     34.9   4.46\n10 st1     2004-08-18 15:27:46  40.6 -10.5       50        24.6     34.9   4.44\n# ℹ 627 more rows\n# ℹ 4 more variables: fluorescence &lt;dbl&gt;, spar &lt;dbl&gt;, par &lt;dbl&gt;, density &lt;dbl&gt;\n\n\nThe expression calls the ctd dataset and feed into the filter()and pick all observations with pressure below 150meters and create a new datase called surface. This is an expression where a single conditional statement is used.\nWe can also limit the of the variable of interest by combining multiple conditional expressions as part of the filter(). Each expression (argument) is combined with an “AND” clause by default. This means that all expressions must be matched for a recorded to be returned. For instance we want to pick observations that were measured between 5 and 10 meters water only. We combine theses expressions with & operator;\n\nCodectd %&gt;% \n  filter(pressure &gt;= 0 & pressure &lt;= 10)\n\n# A tibble: 44 × 12\n   station time                  lon   lat pressure temperature salinity oxygen\n   &lt;chr&gt;   &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n 1 st1     2004-08-18 15:27:46  40.6 -10.5        5        25.2     33.9   3.93\n 2 st1     2004-08-18 15:27:46  40.6 -10.5       10        25.1     34.9   4.49\n 3 st2     2004-08-18 17:00:01  40.8 -10.5        5        25.2     34.8   4.47\n 4 st2     2004-08-18 17:00:01  40.8 -10.5       10        25.2     34.8   4.47\n 5 st3     2004-08-18 20:32:54  41.0 -10.5        5        NA       NA    NA   \n 6 st3     2004-08-18 20:32:54  41.0 -10.5       10        25.0     34.9   4.49\n 7 st4     2004-08-18 22:44:56  41.1 -10.5        5        NA       NA    NA   \n 8 st4     2004-08-18 22:44:56  41.1 -10.5       10        NA       NA    NA   \n 9 st5     2004-08-19 00:59:59  41.3 -10.5        5        NA       NA    NA   \n10 st5     2004-08-19 00:59:59  41.3 -10.5       10        NA       NA    NA   \n# ℹ 34 more rows\n# ℹ 4 more variables: fluorescence &lt;dbl&gt;, spar &lt;dbl&gt;, par &lt;dbl&gt;, density &lt;dbl&gt;\n\n\nWe can also use the between() function, which is equivalent to pressure &gt;= 0 & pressure &lt;= 10 in above chunk to achive the same result.\n\nCodectd %&gt;% \n  filter(between(pressure, 5,10))\n\n# A tibble: 44 × 12\n   station time                  lon   lat pressure temperature salinity oxygen\n   &lt;chr&gt;   &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n 1 st1     2004-08-18 15:27:46  40.6 -10.5        5        25.2     33.9   3.93\n 2 st1     2004-08-18 15:27:46  40.6 -10.5       10        25.1     34.9   4.49\n 3 st2     2004-08-18 17:00:01  40.8 -10.5        5        25.2     34.8   4.47\n 4 st2     2004-08-18 17:00:01  40.8 -10.5       10        25.2     34.8   4.47\n 5 st3     2004-08-18 20:32:54  41.0 -10.5        5        NA       NA    NA   \n 6 st3     2004-08-18 20:32:54  41.0 -10.5       10        25.0     34.9   4.49\n 7 st4     2004-08-18 22:44:56  41.1 -10.5        5        NA       NA    NA   \n 8 st4     2004-08-18 22:44:56  41.1 -10.5       10        NA       NA    NA   \n 9 st5     2004-08-19 00:59:59  41.3 -10.5        5        NA       NA    NA   \n10 st5     2004-08-19 00:59:59  41.3 -10.5       10        NA       NA    NA   \n# ℹ 34 more rows\n# ℹ 4 more variables: fluorescence &lt;dbl&gt;, spar &lt;dbl&gt;, par &lt;dbl&gt;, density &lt;dbl&gt;\n\n\nIn the next example, two conditional expressions are passed. The first is used to filter surface water below 200 m, and the second statement filter records that above latitude 6°S\n\nCodectd %&gt;% \n  filter(pressure &lt; 200 & lat &gt; -6)\n\n# A tibble: 223 × 12\n   station time                  lon   lat pressure temperature salinity oxygen\n   &lt;chr&gt;   &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n 1 st17    2004-08-23 19:42:30  40.1 -5.49        5        25.6     35.2   4.40\n 2 st17    2004-08-23 19:42:30  40.1 -5.49       10        25.4     35.1   4.45\n 3 st17    2004-08-23 19:42:30  40.1 -5.49       15        25.3     35.1   4.48\n 4 st17    2004-08-23 19:42:30  40.1 -5.49       20        25.4     35.2   4.48\n 5 st17    2004-08-23 19:42:30  40.1 -5.49       25        25.4     35.2   4.48\n 6 st17    2004-08-23 19:42:30  40.1 -5.49       30        25.4     35.2   4.48\n 7 st17    2004-08-23 19:42:30  40.1 -5.49       35        25.4     35.2   4.46\n 8 st17    2004-08-23 19:42:30  40.1 -5.49       40        25.4     35.2   4.48\n 9 st17    2004-08-23 19:42:30  40.1 -5.49       45        25.4     35.2   4.47\n10 st17    2004-08-23 19:42:30  40.1 -5.49       50        25.4     35.2   4.45\n# ℹ 213 more rows\n# ℹ 4 more variables: fluorescence &lt;dbl&gt;, spar &lt;dbl&gt;, par &lt;dbl&gt;, density &lt;dbl&gt;\n\n\nIn this case, the surface.transect dataset has records where both conditions are met—the pressure is blow 200 meter and latitude above -6. Note that when two or more conditions are paased, the & operator is used.\nYou may sometimes want to know stations and at what depth a particular variable has missing values. You can pick all variable in the data frame using is.na() function.\n\nCodectd %&gt;% \n  filter(is.na(fluorescence))\n\n# A tibble: 7 × 12\n  station time                  lon    lat pressure temperature salinity oxygen\n  &lt;chr&gt;   &lt;dttm&gt;              &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 st3     2004-08-18 20:32:54  41.0 -10.5         5          NA       NA     NA\n2 st4     2004-08-18 22:44:56  41.1 -10.5         5          NA       NA     NA\n3 st4     2004-08-18 22:44:56  41.1 -10.5        10          NA       NA     NA\n4 st5     2004-08-19 00:59:59  41.3 -10.5         5          NA       NA     NA\n5 st5     2004-08-19 00:59:59  41.3 -10.5        10          NA       NA     NA\n6 st10    2004-08-19 19:36:50  39.7  -8.83        5          NA       NA     NA\n7 st10    2004-08-19 19:36:50  39.7  -8.83       10          NA       NA     NA\n# ℹ 4 more variables: fluorescence &lt;dbl&gt;, spar &lt;dbl&gt;, par &lt;dbl&gt;, density &lt;dbl&gt;\n\n\nYou can also drop the observation with missing values in the data frame using the !is.na() operator\n\nCodectd %&gt;% \n  filter(!is.na(fluorescence))\n\n# A tibble: 2,789 × 12\n   station time                  lon   lat pressure temperature salinity oxygen\n   &lt;chr&gt;   &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n 1 st1     2004-08-18 15:27:46  40.6 -10.5        5        25.2     33.9   3.93\n 2 st1     2004-08-18 15:27:46  40.6 -10.5       10        25.1     34.9   4.49\n 3 st1     2004-08-18 15:27:46  40.6 -10.5       15        25.1     34.9   4.50\n 4 st1     2004-08-18 15:27:46  40.6 -10.5       20        25.0     34.9   4.51\n 5 st1     2004-08-18 15:27:46  40.6 -10.5       25        24.9     34.9   4.51\n 6 st1     2004-08-18 15:27:46  40.6 -10.5       30        24.9     34.9   4.50\n 7 st1     2004-08-18 15:27:46  40.6 -10.5       35        24.9     34.9   4.49\n 8 st1     2004-08-18 15:27:46  40.6 -10.5       40        24.9     34.9   4.48\n 9 st1     2004-08-18 15:27:46  40.6 -10.5       45        24.8     34.9   4.46\n10 st1     2004-08-18 15:27:46  40.6 -10.5       50        24.6     34.9   4.44\n# ℹ 2,779 more rows\n# ℹ 4 more variables: fluorescence &lt;dbl&gt;, spar &lt;dbl&gt;, par &lt;dbl&gt;, density &lt;dbl&gt;\n\n\nWhen you have string variable in the data frame with character or factor format, you can filter the certain observation with %in% statement. For example, to obtain profiles from three stations: st1, st8, and st13, we can write the code as;\n\nCodectd %&gt;% \n  filter(station %in% c(\"st1\", \"st8\", \"st13\"))\n\n# A tibble: 347 × 12\n   station time                  lon   lat pressure temperature salinity oxygen\n   &lt;chr&gt;   &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n 1 st1     2004-08-18 15:27:46  40.6 -10.5        5        25.2     33.9   3.93\n 2 st1     2004-08-18 15:27:46  40.6 -10.5       10        25.1     34.9   4.49\n 3 st1     2004-08-18 15:27:46  40.6 -10.5       15        25.1     34.9   4.50\n 4 st1     2004-08-18 15:27:46  40.6 -10.5       20        25.0     34.9   4.51\n 5 st1     2004-08-18 15:27:46  40.6 -10.5       25        24.9     34.9   4.51\n 6 st1     2004-08-18 15:27:46  40.6 -10.5       30        24.9     34.9   4.50\n 7 st1     2004-08-18 15:27:46  40.6 -10.5       35        24.9     34.9   4.49\n 8 st1     2004-08-18 15:27:46  40.6 -10.5       40        24.9     34.9   4.48\n 9 st1     2004-08-18 15:27:46  40.6 -10.5       45        24.8     34.9   4.46\n10 st1     2004-08-18 15:27:46  40.6 -10.5       50        24.6     34.9   4.44\n# ℹ 337 more rows\n# ℹ 4 more variables: fluorescence &lt;dbl&gt;, spar &lt;dbl&gt;, par &lt;dbl&gt;, density &lt;dbl&gt;"
  },
  {
    "objectID": "ch4.html#select",
    "href": "ch4.html#select",
    "title": "6  Data frame Manipulation",
    "section": "\n6.5 select",
    "text": "6.5 select\nThe second verb we are going to demonstrate is the select() function. Often you work with large datasets with many columns but only a few are actually of interest to you. The select() function selects columns of the data frame. select() function allows you to choose variables that are of interest. You can use it to pick out a some columns from the dataset. For instance, fi we want pressure, temprature, salinity, fluorescence and ovygen variables from the data frame, we can simply write a code as;\n\nCodectd %&gt;% \n  select (pressure, temperature, salinity, fluorescence, oxygen)\n\n# A tibble: 2,796 × 5\n   pressure temperature salinity fluorescence oxygen\n      &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n 1        5        25.2     33.9        0.560   3.93\n 2       10        25.1     34.9        0.599   4.49\n 3       15        25.1     34.9        0.650   4.50\n 4       20        25.0     34.9        0.678   4.51\n 5       25        24.9     34.9        0.760   4.51\n 6       30        24.9     34.9        0.729   4.50\n 7       35        24.9     34.9        0.740   4.49\n 8       40        24.9     34.9        0.693   4.48\n 9       45        24.8     34.9        0.703   4.46\n10       50        24.6     34.9        0.752   4.44\n# ℹ 2,786 more rows\n\n\nBesides just selecting columns, you can use a minus sign to remove variables you do not need from the data frame.\n\nCodectd %&gt;% \n  select(-spar, -par, -density, -time) \n\n# A tibble: 2,796 × 8\n   station   lon   lat pressure temperature salinity oxygen fluorescence\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;\n 1 st1      40.6 -10.5        5        25.2     33.9   3.93        0.560\n 2 st1      40.6 -10.5       10        25.1     34.9   4.49        0.599\n 3 st1      40.6 -10.5       15        25.1     34.9   4.50        0.650\n 4 st1      40.6 -10.5       20        25.0     34.9   4.51        0.678\n 5 st1      40.6 -10.5       25        24.9     34.9   4.51        0.760\n 6 st1      40.6 -10.5       30        24.9     34.9   4.50        0.729\n 7 st1      40.6 -10.5       35        24.9     34.9   4.49        0.740\n 8 st1      40.6 -10.5       40        24.9     34.9   4.48        0.693\n 9 st1      40.6 -10.5       45        24.8     34.9   4.46        0.703\n10 st1      40.6 -10.5       50        24.6     34.9   4.44        0.752\n# ℹ 2,786 more rows\n\nCode## or you can bind the variable you want to remove\nctd %&gt;% \n  select(-c(spar, par, density, time))\n\n# A tibble: 2,796 × 8\n   station   lon   lat pressure temperature salinity oxygen fluorescence\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;\n 1 st1      40.6 -10.5        5        25.2     33.9   3.93        0.560\n 2 st1      40.6 -10.5       10        25.1     34.9   4.49        0.599\n 3 st1      40.6 -10.5       15        25.1     34.9   4.50        0.650\n 4 st1      40.6 -10.5       20        25.0     34.9   4.51        0.678\n 5 st1      40.6 -10.5       25        24.9     34.9   4.51        0.760\n 6 st1      40.6 -10.5       30        24.9     34.9   4.50        0.729\n 7 st1      40.6 -10.5       35        24.9     34.9   4.49        0.740\n 8 st1      40.6 -10.5       40        24.9     34.9   4.48        0.693\n 9 st1      40.6 -10.5       45        24.8     34.9   4.46        0.703\n10 st1      40.6 -10.5       50        24.6     34.9   4.44        0.752\n# ℹ 2,786 more rows\n\n\nYou can drop a range of variables in the data frame with select() function. For instance, the code below drop all variables beween temperature to fluorescence. You can also select those variables in range by removing the negative sign\n\nCode# hide a range of columns\nctd %&gt;% \n  select(-(temperature:fluorescence))\n\n# A tibble: 2,796 × 8\n   station time                  lon   lat pressure  spar   par density\n   &lt;chr&gt;   &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 st1     2004-08-18 15:27:46  40.6 -10.5        5 1177. 53.9    1022.\n 2 st1     2004-08-18 15:27:46  40.6 -10.5       10 1151. 40.3    1023.\n 3 st1     2004-08-18 15:27:46  40.6 -10.5       15 1135. 31.3    1023.\n 4 st1     2004-08-18 15:27:46  40.6 -10.5       20 1124. 25.6    1023.\n 5 st1     2004-08-18 15:27:46  40.6 -10.5       25 1111. 21.1    1023.\n 6 st1     2004-08-18 15:27:46  40.6 -10.5       30 1103. 17.2    1023.\n 7 st1     2004-08-18 15:27:46  40.6 -10.5       35 1097. 13.9    1023.\n 8 st1     2004-08-18 15:27:46  40.6 -10.5       40 1091. 11.2    1023.\n 9 st1     2004-08-18 15:27:46  40.6 -10.5       45 1087.  9.05   1024.\n10 st1     2004-08-18 15:27:46  40.6 -10.5       50 1084.  7.30   1024.\n# ℹ 2,786 more rows\n\n\nJust like you can pick columns with the matching name, you can also drop any column with a matching name\n\nCodectd %&gt;% \n  select(-contains(\"t\"))\n\n# A tibble: 2,796 × 6\n     lon pressure oxygen fluorescence  spar   par\n   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  40.6        5   3.93        0.560 1177. 53.9 \n 2  40.6       10   4.49        0.599 1151. 40.3 \n 3  40.6       15   4.50        0.650 1135. 31.3 \n 4  40.6       20   4.51        0.678 1124. 25.6 \n 5  40.6       25   4.51        0.760 1111. 21.1 \n 6  40.6       30   4.50        0.729 1103. 17.2 \n 7  40.6       35   4.49        0.740 1097. 13.9 \n 8  40.6       40   4.48        0.693 1091. 11.2 \n 9  40.6       45   4.46        0.703 1087.  9.05\n10  40.6       50   4.44        0.752 1084.  7.30\n# ℹ 2,786 more rows\n\n\nBecause of the naming conventions, many of the column names that you import dont make sense. You will often need to change the name of the variable. select() function allows you to accomplish that. For example, we want to select station, pressure and fluoresence, but we need also change the name of station to be Cast, pressure to Depth and fluorescence to Chlorophyll. You can achieve that with code written as;\n\nCodectd %&gt;% \n  select(Cast = station, \n         Depth = pressure, \n         Chlorophyll = fluorescence)\n\n# A tibble: 2,796 × 3\n   Cast  Depth Chlorophyll\n   &lt;chr&gt; &lt;dbl&gt;       &lt;dbl&gt;\n 1 st1       5       0.560\n 2 st1      10       0.599\n 3 st1      15       0.650\n 4 st1      20       0.678\n 5 st1      25       0.760\n 6 st1      30       0.729\n 7 st1      35       0.740\n 8 st1      40       0.693\n 9 st1      45       0.703\n10 st1      50       0.752\n# ℹ 2,786 more rows\n\n\n\nThere are also a number of handy helper functions that you can use with the select() function to filter the returned columns. These include starts_with(), ends_with(), contains(), matches(), and num_range(). I wont illustrate them here, however, you can consult the help document for more information.\n\n\n6.5.1 Adding new variables: mutate, transmute, add_rownames\nBesides selecting sets of existing columns, it’s often useful to add new columns that are functions of existing columns. This is the job of mutate(): Any new variable created with the mutate() function will be added to the end of the data frame. For example, raw fluorescence values are often skewed (Figure 6.1 a) and we often transform them to have normal distribution (Figure 6.1 b).\n\n\n\n\nFIGURE 6.1. ?(caption)\n\n\n\n\nAt this situation, its handy to add a new column with transformed values in the data frame as shown in the code;\n\nCodectd %&gt;% \n  select(pressure, fluorescence) %&gt;%\n  mutate(log.fluorescence = fluorescence %&gt;% log10())\n\n# A tibble: 2,796 × 3\n   pressure fluorescence log.fluorescence\n      &lt;dbl&gt;        &lt;dbl&gt;            &lt;dbl&gt;\n 1        5        0.560           -0.251\n 2       10        0.599           -0.223\n 3       15        0.650           -0.187\n 4       20        0.678           -0.169\n 5       25        0.760           -0.119\n 6       30        0.729           -0.138\n 7       35        0.740           -0.131\n 8       40        0.693           -0.159\n 9       45        0.703           -0.153\n10       50        0.752           -0.124\n# ℹ 2,786 more rows\n\n\nThe code tells important two steps: the first steps involved selecting the pressure and fluorescence variables, once these variables were selected fromt he ctd data frame were fed into a mutate() function, which computed the logarithmic of fluorescence and assign a new log.fluorescence variable into the data frame.\nIn a similar way above, we can create a new variable of anomaly as the code below shows;\n\nCodectd %&gt;% \n  select(pressure, fluorescence) %&gt;%\n  mutate(anomaly = fluorescence - mean(fluorescence, na.rm = TRUE))\n\n# A tibble: 2,796 × 3\n   pressure fluorescence anomaly\n      &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;\n 1        5        0.560   0.425\n 2       10        0.599   0.464\n 3       15        0.650   0.515\n 4       20        0.678   0.542\n 5       25        0.760   0.624\n 6       30        0.729   0.593\n 7       35        0.740   0.604\n 8       40        0.693   0.557\n 9       45        0.703   0.568\n10       50        0.752   0.617\n# ℹ 2,786 more rows\n\n\n\n6.5.2 Arranging rows\nThe arrange() function in the dplyr package can be used to order the rows in a data frame. This function accepts a set of columns to order by with the default row ordering being in ascending order.\n\nCodectd %&gt;% \n  arrange(pressure)\n\n# A tibble: 2,796 × 12\n   station time                  lon    lat pressure temperature salinity oxygen\n   &lt;chr&gt;   &lt;dttm&gt;              &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n 1 st1     2004-08-18 15:27:46  40.6 -10.5         5        25.2     33.9   3.93\n 2 st2     2004-08-18 17:00:01  40.8 -10.5         5        25.2     34.8   4.47\n 3 st3     2004-08-18 20:32:54  41.0 -10.5         5        NA       NA    NA   \n 4 st4     2004-08-18 22:44:56  41.1 -10.5         5        NA       NA    NA   \n 5 st5     2004-08-19 00:59:59  41.3 -10.5         5        NA       NA    NA   \n 6 st6     2004-08-19 11:49:08  40.3  -8.83        5        25.2     34.9   4.48\n 7 st7     2004-08-19 13:33:31  40.2  -8.83        5        25.3     34.9   4.52\n 8 st8     2004-08-19 15:28:18  40.0  -8.83        5        25.0     34.9   4.59\n 9 st9     2004-08-19 17:39:39  39.8  -8.83        5        25.1     34.9   4.64\n10 st10    2004-08-19 19:36:50  39.7  -8.83        5        NA       NA    NA   \n# ℹ 2,786 more rows\n# ℹ 4 more variables: fluorescence &lt;dbl&gt;, spar &lt;dbl&gt;, par &lt;dbl&gt;, density &lt;dbl&gt;\n\n\nBy default, it orders numerical values in increasing order, but you can ask for decreasing order using the desc() function:\n\nCodectd %&gt;% \n  arrange(pressure %&gt;% desc())\n\n# A tibble: 2,796 × 12\n   station time                  lon   lat pressure temperature salinity oxygen\n   &lt;chr&gt;   &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n 1 st3     2004-08-18 20:32:54  41.0 -10.5     1015        6.43     34.8   2.13\n 2 st3     2004-08-18 20:32:54  41.0 -10.5     1010        6.45     34.8   2.13\n 3 st3     2004-08-18 20:32:54  41.0 -10.5     1005        6.45     34.8   2.13\n 4 st3     2004-08-18 20:32:54  41.0 -10.5     1000        6.45     34.8   2.13\n 5 st3     2004-08-18 20:32:54  41.0 -10.5      995        6.46     34.8   2.13\n 6 st3     2004-08-18 20:32:54  41.0 -10.5      990        6.48     34.8   2.13\n 7 st3     2004-08-18 20:32:54  41.0 -10.5      985        6.55     34.8   2.11\n 8 st3     2004-08-18 20:32:54  41.0 -10.5      980        6.60     34.8   2.14\n 9 st3     2004-08-18 20:32:54  41.0 -10.5      975        6.60     34.8   2.17\n10 st3     2004-08-18 20:32:54  41.0 -10.5      970        6.62     34.8   2.17\n# ℹ 2,786 more rows\n# ℹ 4 more variables: fluorescence &lt;dbl&gt;, spar &lt;dbl&gt;, par &lt;dbl&gt;, density &lt;dbl&gt;"
  },
  {
    "objectID": "ch4.html#summarizing-and-grouping",
    "href": "ch4.html#summarizing-and-grouping",
    "title": "6  Data frame Manipulation",
    "section": "\n6.6 Summarizing and Grouping",
    "text": "6.6 Summarizing and Grouping\nSummary statistics for a data frame can be produced with the summarise() function. The summarise() function produces a single row of data containing summary statistics from a data frame. For example, you can compute for the mean of fluorescence values:\n\nCodectd %&gt;% \n  summarise(fl.mean = mean(fluorescence, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  fl.mean\n    &lt;dbl&gt;\n1   0.118\n\n\nBy itself, it’s not that useful until chained with the group_by() verb to compute summary statistics. There you can split the data into different groups and compute the summaries for each group.For example, you can ask for the mean of and standard deviation values of fluorescence for each station in the data frame:\n\nCodectd %&gt;% group_by(station) %&gt;%\n  summarise(Mean = mean(fluorescence, na.rm = TRUE),\n           STD = sd(fluorescence, na.rm = TRUE))\n\n# A tibble: 5 × 3\n  station   Mean   STD\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 st1     0.304  0.319\n2 st13    0.0897 0.179\n3 st18    0.101  0.287\n4 st4     0.0970 0.233\n5 st8     0.125  0.381\n\n\nYou can group by one or more variables; you just specify the columns you want to separate into different subsets to the function. It works best when grouping by factors or discrete numbers; there isn’t much fun in grouping by real numbers.\n\nCodectd %&gt;% group_by(station, lon)%&gt;%\n  summarise(Mean = mean(fluorescence, na.rm = TRUE),\n           STD = sd(fluorescence, na.rm = TRUE))\n\n# A tibble: 5 × 4\n# Groups:   station [5]\n  station   lon   Mean   STD\n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 st1      40.6 0.304  0.319\n2 st13     40.1 0.0897 0.179\n3 st18     39.9 0.101  0.287\n4 st4      41.1 0.0970 0.233\n5 st8      40.0 0.125  0.381\n\n\nsummarise() can be used to count the number of rows in each group with nc()—which just counts how many observations you have in a subset of your data: You only need to parse the argument n() in the summarise()` function as;\n\nCodectd %&gt;% \n  group_by(station) %&gt;% \n  summarise(frequency = n())\n\n# A tibble: 5 × 2\n  station frequency\n  &lt;chr&gt;       &lt;int&gt;\n1 st1            50\n2 st13          135\n3 st18          163\n4 st4           186\n5 st8           162\n\n\n\n\n\n\nBache, Stefan Milton, and Hadley Wickham. 2022. Magrittr: A Forward-Pipe Operator for r. https://CRAN.R-project.org/package=magrittr.\n\n\nMüller, Kirill, and Hadley Wickham. 2022. Tibble: Simple Data Frames. https://CRAN.R-project.org/package=tibble.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\n———. 2019. Stringr: Simple, Consistent Wrappers for Common String Operations. https://CRAN.R-project.org/package=stringr.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2022. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, and Maximilian Girlich. 2022. Tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2022. Readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr."
  },
  {
    "objectID": "ch41.html#data-models",
    "href": "ch41.html#data-models",
    "title": "7  Simple Features",
    "section": "7.1 Data models",
    "text": "7.1 Data models\nGeographic Information Systems (GIS) play a pivotal role in capturing, analyzing, and visualizing spatial data. Central to the functioning of GIS is the concept of data models, which provide a structured framework for organizing and representing geographic information. In this article, we will embark on a thorough exploration of data models in GIS, delving into their significance, types, and applications. By the end, readers will have a comprehensive understanding of data models and their role in enabling efficient spatial analysis."
  },
  {
    "objectID": "ch41.html#the-significance-of-data-models",
    "href": "ch41.html#the-significance-of-data-models",
    "title": "7  Simple Features",
    "section": "7.2 The Significance of Data Models",
    "text": "7.2 The Significance of Data Models\nData models serve as the backbone of GIS, offering a structured approach to organizing and representing spatial data. They define how real-world objects and phenomena are conceptualized and stored within GIS databases. Data models facilitate data integration, standardization, and interoperability, enabling efficient data management and analysis. By understanding data models, GIS professionals can effectively capture, store, and retrieve spatial data, facilitating informed decision-making and analysis."
  },
  {
    "objectID": "ch41.html#types-of-data-models",
    "href": "ch41.html#types-of-data-models",
    "title": "7  Simple Features",
    "section": "7.3 Types of Data Models",
    "text": "7.3 Types of Data Models\nVector Data Model\nThe vector data model represents geographic features as points, lines, and polygons. It utilizes coordinates and topological relationships to define the spatial properties of objects. Vector data models are commonly used for representing discrete and continuous phenomena, such as roads, rivers, land parcels, and vegetation. They offer precise representation and are suitable for detailed analysis and visualization.\nRaster Data Model\nThe raster data model organizes spatial data into a grid of cells or pixels. Each cell represents a discrete unit of space and contains a value that represents a specific attribute or property. Raster data models are suitable for continuous data, such as satellite imagery, elevation data, and climate information. They excel in spatial analysis operations like overlay analysis, surface modeling, and terrain analysis.\nData models form the foundation of GIS, providing a structured framework for organizing, representing, and analyzing spatial data. Understanding the types and applications of data models is essential for GIS professionals to effectively manage and utilize spatial information. By leveraging appropriate data models, users can unlock the full potential of GIS for spatial analysis, visualization, and informed decision-making in diverse domains. The continual advancement of data modeling in GIS ensures that the field remains dynamic and responsive to evolving spatial data requirements."
  },
  {
    "objectID": "ch41.html#simple-feature",
    "href": "ch41.html#simple-feature",
    "title": "7  Simple Features",
    "section": "7.4 Simple feature",
    "text": "7.4 Simple feature\nHave you ever found yourself struggling to work with spatial data in R? Well, fear not, because the introduction of the simple features (sf) package has made handling spatial data a breeze. In this article, we will explore what the sf package is, how it works, and some of its key features. R, a widely used programming language for statistical computing and graphics, offers the powerful “sf” package, which provides a comprehensive framework for working with spatial data. In this chapter, we will delve into the basics of the “sf” package in R, exploring its key features, data structures, and functions. By the end of this chapter, readers will gain a solid understanding of how to leverage “sf” to perform efficient and insightful spatial analysis."
  },
  {
    "objectID": "ch41.html#what-is-simple-feature-sf",
    "href": "ch41.html#what-is-simple-feature-sf",
    "title": "7  Simple Features",
    "section": "7.5 What is Simple Feature (sf)?",
    "text": "7.5 What is Simple Feature (sf)?\nThe “sf” package is an essential package in R for working with spatial data using the Simple Features standard. Simple Features is an open standard developed by the Open Geospatial Consortium (OGC) to represent and manipulate spatial data consistently across different software platforms. “sf” in R provides a unified framework for representing, managing, and analyzing spatial data, making it easier for users to perform a wide range of spatial operations."
  },
  {
    "objectID": "ch41.html#key-features-of-the-sf-package",
    "href": "ch41.html#key-features-of-the-sf-package",
    "title": "7  Simple Features",
    "section": "7.6 Key Features of the “sf” Package",
    "text": "7.6 Key Features of the “sf” Package\nThe “sf” package offers several key features that make it a powerful tool for spatial analysis in R:\n\nUnified Spatial Data Objects: “sf” introduces the “sf” class, which is used to represent spatial objects in R. The “sf” class can handle various spatial data types, including points, lines, polygons, and grids, allowing users to work with different types of spatial data seamlessly.\nAttribute Management: In addition to spatial data, “sf” provides a flexible framework for managing attribute data associated with spatial objects. Users can easily access and manipulate attribute data using familiar data manipulation techniques in R. One of the most useful features is the ability to easily manipulate spatial data using dplyr-style verbs. For example, you can use the filter function to select specific features based on their attributes, or the arrange function to sort features based on their attributes. This makes it easy to perform common data manipulation tasks on spatial data without having to write complex code.\nAnother useful feature of the sf package is its ability to handle projections and coordinate reference systems (CRS). The package provides functions for converting between different CRSs, as well as for reprojecting spatial data to a different CRS. This means that you can easily work with spatial data from different sources that may have different projections or CRSs.\nGeometric Operations: “sf” provides a wide range of geometric operations to manipulate spatial data. Users can perform operations such as buffer, intersection, union, and difference on spatial objects, allowing for complex spatial analysis and data manipulation.\nIntegration with Tidyverse: The “sf” package seamlessly integrates with other popular packages in the Tidyverse, such as dplyr and ggplot2. This integration allows users to combine spatial analysis with data manipulation, visualization, and statistical modeling in a consistent and efficient workflow.\n\n\nWorking with “sf” Spatial Objects: The “sf” package introduces the concept of “sf” spatial objects, which combine geometry and attribute data into a unified structure. Users can create “sf” objects from various data sources, including shapefiles, GeoJSON files, and data frames with spatial information. This chapter will cover the process of reading and writing spatial data, as well as performing basic operations on “sf” objects, such as subsetting, filtering, and summarizing attribute data.\nSpatial Operations and Analysis: The true power of the “sf” package lies in its ability to perform complex spatial operations and analysis. This section will explore some common spatial operations, including spatial joins, spatial queries, and spatial aggregations. Users will learn how to combine spatial datasets, query spatial relationships, and summarize spatial patterns using “sf” functions. Examples will be provided to illustrate the application of these operations in real-world scenarios.\nVisualizing Spatial Data with “sf”: Effective visualization is crucial for communicating spatial information. The “sf” package integrates smoothly with popular visualization packages such as ggplot2 and leaflet, allowing users to create informative and visually appealing maps. This section will cover the basics of visualizing spatial data using “sf” and demonstrate techniques for creating thematic maps, interactive web maps, and customized map layouts.\n\nThe sf package also provides a variety of functions for visualizing spatial data. For example, you can use the ggplot2 framework to create beautiful and informative maps of your spatial data. Additionally, the package provides functions for creating interactive maps using leaflet or mapview.\n\nSpatial data in R has a reputation for being tedious and time consuming. With so many different spatial file types (.shp, .nc, .gpkg, .geojson, and .tif to name a few) with various resolutions and coordinate reference systems, it can be challenging to produce accurate maps. Most data scientists have historically utilized the raster package to calculate cell values across stacked layers and recognize patterns over space and time. In pursuit of improving methodology and keeping up with the hip trends in environmental science, many scientists are motivated to make the spatial switch from raster to terra. The terra package is essentially the modern version of raster, but with faster processing speeds and more flexible functions."
  },
  {
    "objectID": "ch5_matrix.html#matrices",
    "href": "ch5_matrix.html#matrices",
    "title": "8  Introduction to Matrices",
    "section": "\n8.1 Matrices",
    "text": "8.1 Matrices\nMatrices are a fundamental data structure in R that allows you to store and manipulate data in a two-dimensional format. They consist of rows and columns, where each element is identified by its row and column index. Matrices are useful for organizing and working with structured data, such as numerical or categorical values.\nMatrices are useful for performing various mathematical operations, such as matrix multiplication, addition, and transposition. They are also commonly used in statistical analysis and data manipulation tasks. R provides many built-in functions and operators for working with matrices, allowing you to perform calculations, subsetting, and other operations efficiently."
  },
  {
    "objectID": "ch5_matrix.html#what-is-a-matrix",
    "href": "ch5_matrix.html#what-is-a-matrix",
    "title": "8  Introduction to Matrices",
    "section": "\n8.2 What is a matrix?",
    "text": "8.2 What is a matrix?\nA matrix is a two-dimensional data structure in R that consists of rows and columns. It is similar to a table or a spreadsheet where each element in the matrix is identified by its row and column index. Matrices in R can contain elements of the same data type, such as numeric values, character strings, or logical values.\nUnlike a data.frame, the number of values in all columns of a matrix is equal, and the same can be said about the rows. It is important to know how to work with matrices because it is a commonly used data structure, with many uses in data processing and analysis, including spatial data. For example, many R function accept a matrix as an argument, or return a matrix as a returned object. Moreover, a matrix is used to store raster object."
  },
  {
    "objectID": "ch5_matrix.html#creating-a-matrix",
    "href": "ch5_matrix.html#creating-a-matrix",
    "title": "8  Introduction to Matrices",
    "section": "\n8.3 Creating a matrix\n",
    "text": "8.3 Creating a matrix\n\nIn R, you can create a matrix using the matrix() function. The function takes in a vector of values and parameters specifying the number of rows and columns. A matrix function accepts the following arguments:\n\n\ndata—A vector of the values to fill into the matrix\n\nnrow—The number of rows\n\n\nncol—The number of columns\n\n\nbyrow—Whether the matrix is filled by column (FALSE, the default) or by row (TRUE)\n\nFor example, to create a 3x3 matrix with numeric values, you can use the following code:\n\nCodemy_matrix &lt;- matrix(\n  c(1, 2, 3, 4, 5, 6, 7, 8, 9), \n  nrow = 3, \n  ncol = 3\n  )\n\n\nThis will create a matrix my_matrix with 3 rows and 3 columns, where the elements are filled in column-wise order.\n\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\nNote that the class of matrix objects is a vector of length two, with the values \"matrix\" and \"array\":\n\nCodemy_matrix |&gt;\n  class()\n\n[1] \"matrix\" \"array\" \n\n\nThis implies the fact that the matrix class inherits from the more general array class. The nrow and ncol parameters determine the number of rows and number of columns, respectively. When only one of them is specified, the other is automatically determined based on the length of the data vector:\n\nCodemy_matrix &lt;- matrix(\n  c(1, 2, 3, 4, 5, 6, 7, 8, 9), \n  ncol = 3\n  )\n\nmy_matrix\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\n\nExample 8.1 What do you think will happen when we try to create a matrix with less, or more, data values than matrix size nrow*ncol? Run the following expressions to find out.\n\n\nSolution. \n\nCodematrix(12:1, ncol = 4, nrow = 4)\n\n     [,1] [,2] [,3] [,4]\n[1,]   12    8    4   12\n[2,]   11    7    3   11\n[3,]   10    6    2   10\n[4,]    9    5    1    9\n\n\n\n\nExample 8.2 Create a 3×33×3 matrix where all values are 1/91/9.\n\nFinally, the byrow parameter determines the direction of filling the matrix with data values. In both cases the filling starts from the top-left corner (i.e., row 1, column 1), however with byrow=FALSE the matrix is filled one column at a time (the default), while with byrow=TRUE the matrix is filled one row at a time. For example:\n\nCodematrix(1:12, nrow = 3) \n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12"
  },
  {
    "objectID": "ch5_matrix.html#matrix-properties",
    "href": "ch5_matrix.html#matrix-properties",
    "title": "8  Introduction to Matrices",
    "section": "\n8.4 matrix properties",
    "text": "8.4 matrix properties\nIn R, a matrix is a two-dimensional data structure that contains elements of the same data type organized in rows and columns. Matrices are useful for various mathematical and statistical operations.\n\n8.4.1 Dimensions\nA matrix has a defined number of rows and columns, which determine its dimensions. The dimensions of a matrix can be obtained using the dim() function.\n\nCodex = matrix(1:6, nrow = 2) \nx \n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\nFor example, if mat is a matrix, dim(mat) will return a vector containing the number of rows and columns.\n\nCodex |&gt;\n  dim()\n\n[1] 2 3\n\n\nThe length function returns the number of values in a matrix:\n\nCodex |&gt;\n  length()\n\n[1] 6\n\n\nJust like with a data.frame, the nrow and ncol functions return the number of rows and columns in a matrix, respectively:\n\nCodex |&gt;\n  nrow()\n\n[1] 2\n\n\n\nCodex |&gt;\n  ncol()\n\n[1] 3\n\n\nAlso like with a data.frame, the dim function gives both dimensions of the matrix as a vector of length 2, i.e., number of rows and columns, respectively:\n\nCodex |&gt;\n  dim()\n\n[1] 2 3\n\n\nFor example, R has a built-in dataset named volcano, which is a matrix of surface elevation. The sample script volcano.R, used in Section 2.1.1 to demontrate working with R code files, creates a 3D image of elevation based on that matrix (Figure 2.2).\n\nExample 8.3 Find out what are the number of elements, rows and columns in the built-in matrix named volcano.\n\n\n8.4.2 Row and column names\nLike a data.frame, matrix objects also have row and column names which can be accessed or modified using the rownames and colnames functions, respectively. Unlike data.frame row and column names, which are mandatory, matrix row and column names are optional. For example, matrices created with matrix initially do not have row and column names:\n\nCoderownames(x) \n\nNULL\n\n\n\nCodecolnames(x) \n\nNULL\n\n\nThe matrix row and column names can be initialized, or modified, by assignment to the rownames and colnames properties:\n\nCoderownames(x) = c(\"a\", \"b\") \ncolnames(x) = c(\"var1\", \"var2\", \"var3\") \nx \n\n  var1 var2 var3\na    1    3    5\nb    2    4    6\n\n\n\n8.4.3 matrix conversions\nIn R, you can convert between different data structures, such as vectors, data frames, and matrices, using various functions and operations. Here are some common ways to convert data into matrices:\n\n8.4.4 matrix → vector\nIn R, you can convert a matrix into a vector using the as.vector() or c() function. Here’s how you can perform the conversion:\n\nCodex = matrix(\n  data = 1:6, \n  ncol = 3, \n  byrow = TRUE)\nx \n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n\n\n\nCodex |&gt; \n  as.vector()\n\n[1] 1 4 2 5 3 6\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that the matrix values are always arranged by column in the resulting vector!\n\n\n\nExample 8.4 Does the volcano matrix contain any NA values? How can we check?\n\n\n8.4.5 matrix → data.frame\n\nTo convert a matrix into a data frame in R, you can use the as.data.frame() function. Here’s how you can perform the conversion:\n\nCodex |&gt;\n  as.data.frame() \n\n  V1 V2 V3\n1  1  2  3\n2  4  5  6\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that row and column names are automatically generated (if they do not exist) as part of the conversion, since they are mandatory in a data.frame\n\n\n\n8.4.6 Transposing a matrix\nIn R, you can transpose a matrix by interchanging its rows and columns. The transpose operation flips the matrix along its diagonal. R provides the t() function specifically designed for transposing matrices. The t function transposes a matrix. In other words, the matrix rows and columns are \"switched\"—rows become columns and columns become rows. Here’s how you can transpose a matrix in R:\n\nCodex = matrix(1:6, ncol = 3) \nx \n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\n\nCodet(x) \n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n\n\n\nExample 8.5 What will be the result of t(t(x))?"
  },
  {
    "objectID": "ch5_raster.html#what-is-a-raster",
    "href": "ch5_raster.html#what-is-a-raster",
    "title": "9  Introduction to Raster",
    "section": "\n9.1 What is a raster?",
    "text": "9.1 What is a raster?\nA raster is basically a matrix or an array, representing a rectangular area on the surface of the earth. To associate the matrix or the array with the particular area it represents, the raster has some additional spatial properties, on top of the non-spatial properties that any ordinary matrix or array has:\n\n\nNon-spatial properties\n\nValues\nDimensions (rows, columns, layers)\n\n\n\nSpatial properties\n\nExtent\nCoordinate Reference System (CRS)\n(Resolution)\n\n\n\nRaster extent is the range of x- and y-axis coordinates that the raster occupies. The Coordinate Reference System (CRS) is the particular system that \"associates\" the raster coordinates (which are just pairs of x/y values) to geographic locations. Raster resolution is the size of a raster cell, in the x and y directions. The resolution is listed in parentheses because it can be calculated given the extent and the number of rows and columns."
  },
  {
    "objectID": "ch5_raster.html#commmon-raster-file-formats",
    "href": "ch5_raster.html#commmon-raster-file-formats",
    "title": "9  Introduction to Raster",
    "section": "\n9.2 Commmon Raster file formats",
    "text": "9.2 Commmon Raster file formats\nCommonly used raster file formats (Table [5.2] can be divided in two groups. \"Simple\" raster file formats, such as GeoTIFF, are single-band or multi-band rasters (Figure [5.10] with a geo-referenced extent, as discussed above (Section [5.3.1]. \"Complex\" raster file formats, such as HDF, contain additional complexity, such as more than three dimensions (Figure [5.11], and/or metadata, such as band names, time stamps, units of measurement, and so on. | | | | |————————————|—————————————————————–|—————————————| | | | Table 5.2: Common raster file formats | | Type | Format | File extension | | \"Simple\" | GeoTIFF | .tif | | | Erdas Imagine Image | .img | | \"Complex\" (&gt;3D and/or metadata) | HDF | .hdf, he5, and other | | | NetCDF | .nc |"
  },
  {
    "objectID": "ch5_raster.html#raster-with-terra",
    "href": "ch5_raster.html#raster-with-terra",
    "title": "9  Introduction to Raster",
    "section": "\n9.3 Raster with terra",
    "text": "9.3 Raster with terra\nIn this section, our focus will be on the terra package, which serves as a replacement for the raster package in R when working with raster-based geospatial data. Over the years, the raster package has been widely used as a primary tool for geospatial data manipulation in R. However, the creators of the raster package have recently introduced the terra package as a more efficient alternative.\nterra is a geospatial data processing library in R that provides efficient and scalable tools for working with raster data. It is designed to handle large and complex spatial datasets, allowing for high-performance analysis and processing. The terra package introduces three new classes for working with geospatial data:\n\nSpatRaster: The SpatRaster class is designed for representing raster data in a geospatial context. It provides efficient memory usage compared to the data models used in the raster package. SpatRaster objects store the spatial extent, resolution, and values of a raster dataset. They support various operations and functions for raster data manipulation, analysis, and visualization.\nSpatVector: The SpatVector class represents vector-based geospatial data, including points, lines, and polygons, along with their associated attributes. SpatVector objects store the geometries and attribute data of vector features. They support common vector operations such as subsetting, spatial queries, spatial joins, and attribute manipulation.\nSpatExtent: The SpatExtent class is used to represent the spatial extent of a geospatial object, such as a SpatRaster or SpatVector. It captures the minimum and maximum coordinates along each dimension (e.g., x, y, and potentially z for 3D data). SpatExtent objects provide methods for comparing extents, calculating overlaps, and extracting subsets of data based on spatial extent.\n\nTo get started using the terra package for working with geospatial data and performing various tasks, such as raster preprocessing, install the terra package from the CRAN repository using the following command:\n\nCodeinstall.packages(\"terra\")\n\n\nThen, load the package into your R session:\n\nCoderequire(terra)\nrequire(tidyterra)\nrequire(sf)\nrequire(tidyverse)\nrequire(patchwork)"
  },
  {
    "objectID": "ch5_raster.html#reading-raster-file",
    "href": "ch5_raster.html#reading-raster-file",
    "title": "9  Introduction to Raster",
    "section": "\n9.4 Reading raster file",
    "text": "9.4 Reading raster file\nTo read a raster dataset from an existing file, you can use the rast() function along with the file path and name. For instance, you can read bathymetry data covering the Indian Ocean region from a file provided by NASA.\n\nCodebathy = rast(\"data/wioregio-7753.asc\")\n\nbathy\n\nclass       : SpatRaster \ndimensions  : 3181, 2641, 1  (nrow, ncol, nlyr)\nresolution  : 0.01666667, 0.01666667  (x, y)\nextent      : 25.99167, 70.00834, -40.00833, 13.00834  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource      : wioregio-7753.asc \nname        : wioregio-7753 \n\n\n\nCodebathy  |&gt;\n  class()\n\n[1] \"SpatRaster\"\nattr(,\"package\")\n[1] \"terra\"\n\n\nThe spatRaster object in R provides a range of functions to obtain information about the data it contains. Simply calling the object using its variable name will return basic information, but additional functions are available for more specific details. For instance, ncol() returns the number of columns, nrow() returns the number of rows, and ncell() returns the total number of cells in the data.\n\nCodebathy |&gt;\n  ncol()\n\n[1] 2641\n\n\n\nCodebathy |&gt;\n  nrow()\n\n[1] 3181\n\n\n\nCodebathy |&gt;\n  nlyr()\n\n[1] 1\n\n\n\nCodebathy |&gt;\n  res()\n\n[1] 0.01666667 0.01666667\n\n\nThe nlyr() function returns the number of layers or bands, while res() provides the data’s resolution relative to the map projection’s units. The names assigned to each band can be obtained using the names() function. Finally, inMemory() returns a Boolean value indicating whether the data is stored in memory or RAM. These functions allow for a more detailed understanding of the spatRaster object’s contents and properties.\n\nCodebathy |&gt;\n  ncell()\n\n[1] 8401021\n\n\n\nCodebathy |&gt;\n  names()\n\n[1] \"wioregio-7753\"\n\n\n\nCodebathy |&gt;\n  inMemory()\n\n[1] FALSE"
  },
  {
    "objectID": "ch5_raster.html#explore-raster",
    "href": "ch5_raster.html#explore-raster",
    "title": "9  Introduction to Raster",
    "section": "\n9.5 Explore raster",
    "text": "9.5 Explore raster\nIn the next section, we will delve into raster processing tasks such as cropping, masking, and merging using the terra package. These operations allow us to manipulate raster data before further analysis. In this section, let us explore the raster dataset we just imported into our session.\n\nCodebathy %&gt;% \n  sf::st_bbox()\n\n     xmin      ymin      xmax      ymax \n 25.99167 -40.00833  70.00834  13.00834 \n\n\nThe command bathy %&gt;% sf::st_bbox() is used for obtaining the bounding box of the Western Indian Ocean (WIO) region in the bathymetry data. This command utilizes the st_bbox() function from the sf package in R programming language. The output of this command will provide the minimum and maximum values for longitude and latitude that define the rectangular boundary of the WIO region. This information can be useful for a variety of spatial analyses, such as selecting a subset of data within the WIO region or visualizing the extent of the region on a map. Overall, this command provides a convenient and efficient way to obtain the bounding box for the WIO region from the bathymetry data.\nTo extract a rectangular extent from a larger raster dataset, you can utilize the crop() function in the terra package. This requires specifying the desired extent using the spatExtent object, which is created using the ext() function and defining the xmin, xmax, ymin, and ymax coordinates relative to the map projection.\n\nCodecrop_extent &lt;- terra::ext(30,65,-30,10)\n\n\n\nCodebathy.aoi = bathy %&gt;% \n  terra::crop(crop_extent)\n\n\n\nCodebathy.aoi %&gt;% \n  sf::st_bbox()\n\n      xmin       ymin       xmax       ymax \n 29.991667 -30.008331  64.991674   9.991677"
  },
  {
    "objectID": "ch5_raster.html#visualizing-raster",
    "href": "ch5_raster.html#visualizing-raster",
    "title": "9  Introduction to Raster",
    "section": "\n9.6 Visualizing raster",
    "text": "9.6 Visualizing raster\nIn addition to the well-known raster package, which is set to retire soon, the R ecosystem offers several new tools for visualizing raster data. Notably, the terra and tidyterra packages have been introduced, significantly enhancing the capabilities and ease of raster visualization. These tools provide advanced features and improved functionalities, making the process of visualizing raster data even better and more accessible. With terra and tidyterra, users can explore and visualize raster datasets more effectively, enabling a seamless and efficient workflow for raster visualization tasks in R.\nOne notable tool in the R ecosystem for raster visualization is the geom_spatraster() function from the tidyterra package. This function provides a convenient and efficient way to integrate raster data visualization into the grammar of graphics framework provided by the tidyverse.\n\nCodeggplot()+\n  tidyterra::geom_spatraster(data = bathy.aoi)+\n  tidyterra::scale_fill_hypso_tint_c(\n    palette = \"gmt_globe\",\n    labels = scales::label_number(),\n    breaks = seq(-6000, 6000,3000), \n    # limits = c(-6000, 6500),\n    guide = guide_colorbar(\n      direction = \"horizontal\",\n      title = \"Bathymetry & altitude (meters)\",\n      title.position = \"top\",\n      barwidth = 12, barheight = 0.8\n    ))+\n  coord_sf(expand = FALSE) +\n  theme_bw(base_size = 14) +\n  theme(legend.position = \"top\")\n\n\n\nFIGURE 9.1. Bathymetry and altitude in the WIO region\n\n\n\n\nBy leveraging geom_spatraster(), we can easily create aesthetically pleasing and customizable plots that represent raster data in a visually appealing manner. This function offers various options for styling and mapping raster values to visual properties, such as color gradients or contours.\n\nCodeiron.wio = terra::rast(\"f:/2023/dataAll/wekeza/Present.Benthic.Max.Depth.Iron.Range.tif\") |&gt;\n  terra::crop(terra::ext(25,70,-40,15)) %&gt;% \n  tidyterra::rename(iron = 1)\n\n\niron.wio %&gt;% \n  terra::values()  %&gt;%\n  quantile(na.rm = T, c(0.05,0.95))\n\n     5%     95% \n1.3e-05 6.8e-05 \n\n\n\nCodeggplot() +\n  tidyterra::geom_spatraster(data = iron.wio)+\n  scale_fill_gradientn(colours = hcl.colors(n = 120, palette = \"Spectral\", rev = TRUE),\n    guide = guide_colorbar(\n      direction = \"horizontal\",\n      title = \"Iron distribution\",\n      title.position = \"top\",\n      barwidth = 12, barheight = 0.8\n    ), \n                       limits = c(0.000013, 0.000068), na.value = \"white\", \n                       breaks = seq(0.000013, 0.000068, length.out = 5), label = seq(0.13,0.6,0.1))+\n  theme_bw(base_size = 14)+\n  coord_sf(expand = FALSE)+\n  theme_bw(base_size = 14) +\n  theme(legend.position = \"top\")\n\n\n\nFIGURE 9.2. Iron distribution in the WIO region\n\n\n\n\nAdditionally, you can crop a raster relative to the rectangular extent or bounding box of another geospatial layer by using the extent of that layer in the crop operation. In the provided code block, an example is shown where the spatial extent of a watershed boundary is used for cropping. It is worth noting that plotting spatVector objects with the tmap package may not be possible, so the data is also read in as an sf object using the sf package for visualization purposes.\n\nCodeggplot()+\n  tidyterra::geom_spatraster(data = bathy.aoi)+\n  tidyterra::geom_spatraster_contour(data = bathy.aoi, breaks = -200)+\n  tidyterra::scale_fill_hypso_tint_c(\n    palette = \"gmt_globe\",\n    labels = scales::label_number(),\n    breaks = seq(-6000, 6000,3000), \n    # limits = c(-6000, 6500),\n    guide = guide_colorbar(\n      direction = \"horizontal\",\n      title = \"Bathymetry & altitude (meters)\",\n      title.position = \"top\",\n      barwidth = 12, barheight = 0.8\n    ))+\n  coord_sf(expand = FALSE) +\n  theme_bw(base_size = 14) +\n  theme(legend.position = \"top\")\n\n\n\nFIGURE 9.3. Bathymetry and altitude in the WIO region superimposed with isobar"
  },
  {
    "objectID": "ch6.html#glimpse-of-raster",
    "href": "ch6.html#glimpse-of-raster",
    "title": "10  Raster Manipulation",
    "section": "\n10.1 glimpse of raster",
    "text": "10.1 glimpse of raster\nin Chapter 9, we were intoduced with raster dataset. In this chapter we expand that introduction much deeper. To begin, a raster is a fundamental spatial data structure used to represent and store geographic information. It divides a region into a grid of rectangular cells, also known as pixels. Each cell within the raster can hold one or more values, allowing for the storage of various types of information, such as elevation, temperature, land cover, or satellite imagery.\nThis grid-like structure of a raster distinguishes it from vector data, which represents geographic features using points, lines, and polygons. While vector data focuses on discrete spatial entities, rasters provide a continuous representation of data across the entire extent of the region.\nThe raster data structure is commonly employed in various fields, including remote sensing, environmental modeling, terrain analysis, and geographic information systems (GIS). It offers a flexible and efficient means of storing and analyzing spatial data, facilitating tasks such as spatial interpolation, overlay analysis, and spatial statistics."
  },
  {
    "objectID": "ch6.html#sec-terra",
    "href": "ch6.html#sec-terra",
    "title": "10  Raster Manipulation",
    "section": "\n10.2 terra package",
    "text": "10.2 terra package\nThe terra package is a powerful tool in R that offers a comprehensive suite of functions for handling raster data. It provides a wide range of capabilities, including creating, reading, manipulating, and writing raster data. These functions allow users to efficiently work with raster datasets and perform various operations on them.\nOne of the notable strengths of the terra package is its collection of general raster data manipulation functions. These functions serve as building blocks and can be combined to develop more specialized functions tailored to specific analysis tasks. For instance, users can leverage functions to read a subset of raster values from a file or convert cell numbers to coordinates and vice versa, enabling them to extract relevant information and perform coordinate-based operations.\nAdditionally, the terra package implements raster algebra, enabling users to perform mathematical operations on raster data. This capability is particularly useful when working with multiple overlapping rasters or when applying mathematical transformations to derive new raster layers.\nBy encompassing such a comprehensive set of functions for raster data manipulation, the terra package empowers users to efficiently work with raster datasets, perform complex operations, and integrate spatial analysis seamlessly into their R workflow.\n\n10.2.1 Data structure in terra\nThe terra package is built upon several key classes, with SpatRaster and SpatVector being the most significant ones. These classes serve as the foundation for representing and manipulating spatial raster and vector data, respectively.\n\n10.2.2 spatRaster\nA SpatRaster is a data structure used to represent multi-layer raster data. It contains essential parameters that describe the raster, including the number of columns and rows, the spatial extent (bounding box) that defines its coverage, and the coordinate reference system (map projection) used to define its spatial coordinates.\nApart from these fundamental parameters, a SpatRaster object can also store information about the file(s) associated with the raster cell values. This allows for the possibility of storing the cell values either in memory or in external files. By supporting both options, SpatRaster provides flexibility in handling raster data, enabling efficient memory management for large datasets.\nWhen the raster cell values are stored in files, the SpatRaster object can hold metadata related to the file(s), such as file paths, data formats, compression methods, and other properties. This information facilitates efficient access and manipulation of raster data from external sources.\nIn summary, a SpatRaster represents multi-layer raster data and contains essential parameters to describe its dimensions, spatial extent, and coordinate reference system. It also allows for the storage of raster cell values either in memory or in external files, enabling efficient handling of raster data and supporting various data management scenarios.\nThese classes, SpatRaster and SpatVector, are fundamental to the terra package and serve as the primary means of representing and manipulating spatial raster and vector data, respectively. They provide a consistent and efficient framework for handling diverse spatial analysis tasks within the R environment.\n\n10.2.3 spatVector\nA SpatVector in the terra package represents vector data, which encompasses geometries such as points, lines, or polygons, along with their associated tabular attributes.\nThe SpatVector class provides a versatile structure for storing and analyzing vector data within the terra package. It can handle different types of geometries, allowing you to work with point data (e.g., locations of sampling sites), line data (e.g., roads, rivers), or polygon data (e.g., land parcels, administrative boundaries).\nIn addition to geometric information, SpatVector objects also store attribute data associated with the vector features. This tabular information could include attributes such as names, population counts, categories, or any other relevant information pertaining to the features.\nBy combining both geometric and attribute data, SpatVector objects enable you to perform various operations and analyses on vector data. You can perform spatial queries, spatial joins, attribute filtering, and calculations based on the geometries and their attributes.\n\n10.2.4 Creating spatRaster\nTo generate a SpatRaster from scratch using the rast() function in the terra package is a simple task. The default settings create a global raster data structure with a longitude/latitude coordinate reference system (CRS) and 1 by 1 degree cells. However, you have the option to modify these settings by providing additional arguments to the function or changing the parameters after creating the object. First, make sure the terra package is installed and loaded in your R environment:\n\nCoderequire(terra)\nrequire(tidyverse)\n\n\nTo create a SpatRaster from scratch with custom settings, you can specify arguments such as xmin, nrow, ncol, and crs when calling the rast() function. For example:\n\nCodemy.rast &lt;- rast(\n  xmin = -180, ymin = -90, xmax = 180, ymax = 90, \n  nrow = 180, \n  ncol = 360, \n  crs = \"+proj=longlat +datum=WGS84\")\n\n\nThis example creates a SpatRaster object covering the entire globe with a resolution of 1 by 1 degree and a WGS84 (longitude/latitude) CRS.\n\nCodemy.rast\n\nclass       : SpatRaster \ndimensions  : 180, 360, 1  (nrow, ncol, nlyr)\nresolution  : 1, 1  (x, y)\nextent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=longlat +datum=WGS84 +no_defs \n\n\nTo transform a SpatRaster object to a different CRS (projection), you can use the crs() function. It allows you to reproject the SpatRaster to a new CRS while preserving the underlying cell values. For example:\n\nCodemy.rast.proj = my.rast\ncrs(my.rast.proj) &lt;- \"+proj=utm +zone=37 +datum=WGS84\"\n\nmy.rast.proj\n\nclass       : SpatRaster \ndimensions  : 180, 360, 1  (nrow, ncol, nlyr)\nresolution  : 1, 1  (x, y)\nextent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=utm +zone=37 +datum=WGS84 +units=m +no_defs \n\n\nThis will transform the SpatRaster object r to a new CRS specified by the Universal Transverse Mercator (UTM) projection with zone 37.\nIn the example provided above, the my.rast object represents a skeleton or framework of a SpatRaster. It contains information about the number of rows and columns, as well as the geographic location of the raster. However, at this stage, there are no actual cell values associated with the my.rast object.\nlet’s first use ncell function to compute total number of cells in the my.rast raster.\n\nCodemy.rast |&gt;\n  ncell()\n\n[1] 64800\n\n\nThen generate the random values of sea surface temprature equivalent to the number of cell in the my.rast spatRaster object\n\nCodemy.values = rnorm(\n  n = my.rast |&gt; ncell(), \n  mean = 25, \n  sd = 1.8\n  )\n\n\nThen fill the sst values we just generated into the cells of the spatRaster object as highlighed in the code;\n\nCodemy.rast.sst = my.rast\nmy.rast.sst[] = my.values\n\n\nIn a nutshell, the The code my.rast.sst = my.rast; my.rast.sst[] = my.values; my.rast.sst creates a new SpatRaster object named my.rast.sst by assigning the my.rast object to it. Then, it assigns the values stored in the my.values variable to the cell values of the my.rast.sst object using the empty square brackets ([]) notation.\n\nCodemy.rast.sst\n\nclass       : SpatRaster \ndimensions  : 180, 360, 1  (nrow, ncol, nlyr)\nresolution  : 1, 1  (x, y)\nextent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=longlat +datum=WGS84 +no_defs \nsource(s)   : memory\nname        :    lyr.1 \nmin value   : 17.89994 \nmax value   : 32.50958 \n\n\nFinally, the code retrieves and returns the my.rast.sst object. This sequence of operations ensures that the my.rast.sst object has the same spatial properties as my.rast, but with the cell values replaced by the values in my.values.\n\n10.2.5 Reading spaRaster file\nWhile creating a SpatRaster object from scratch using the rast() function is useful, in many cases, you will need to import raster data from your working directory or external sources. To accomplish this, you can utilize the rast() function with appropriate arguments to read and import raster data. The function allows you to specify the file path or name of the raster file you want to import.\nBy providing the necessary information, such as the file format, location, and other relevant details, the rast() function will load the raster data into a SpatRaster object. This enables you to access and manipulate the imported raster data within your R environment for further analysis, visualization, or any other required tasks. Importing raster objects using the rast() function provides a convenient way to incorporate existing raster data into your workflow.\n\nCodebathy = rast(\"data/wioregio-7753.asc\")\n\nbathy\n\nclass       : SpatRaster \ndimensions  : 3181, 2641, 1  (nrow, ncol, nlyr)\nresolution  : 0.01666667, 0.01666667  (x, y)\nextent      : 25.99167, 70.00834, -40.00833, 13.00834  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource      : wioregio-7753.asc \nname        : wioregio-7753 \n\n\n\nCodebathy  |&gt;\n  class()\n\n[1] \"SpatRaster\"\nattr(,\"package\")\n[1] \"terra\"\n\n\n\nCodecrop_extent &lt;- terra::ext(30,65,-30,10)\n\n\n\nCodebath.wio = bathy |&gt;\n  terra::crop(crop_extent)\n\n\n\nCodebath.wio\n\nclass       : SpatRaster \ndimensions  : 2400, 2100, 1  (nrow, ncol, nlyr)\nresolution  : 0.01666667, 0.01666667  (x, y)\nextent      : 29.99167, 64.99167, -30.00833, 9.991677  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource(s)   : memory\nname        : wioregio-7753 \nmin value   :         -6756 \nmax value   :          5677 \n\n\n\n10.2.6 Visualize raster\nTo visualize a raster with terra, you can use theplot() function, which creates a map of the values of a SpatRaster or SpatVector object.\n\nCodebath.wio %&gt;% \n  terra::plot()\n\n\n\nFIGURE 10.1. Bathymetry and altitude in the WIO region plotted with terra tools\n\n\n\n\n\n10.2.7 stacked SpatRaster\nOne important issue in terra is the ability to stack multiple rasters into a single seamless raster object. This is useful when working with multiple rasters that have the same spatial extent, resolution, and coordinate reference system (CRS). We will come to learn how to stack raster objects in terra using the stack() function or c. In this section, I will compute indices using a terrain() to illustrate the concept of stacked spatRaster.\nLets, compute these indeces, which derive eight indices stacked into a single raster object.\n\nCodeocean = bath.wio\nocean[ocean &gt;= 0] = NA\n\nterrain.indices = ocean %&gt;% \n  terra::terrain(\n    c('slope', 'aspect', 'TPI', 'TRI', 'TRIriley', 'TRIrmsd', 'roughness', 'flowdir')\n    )\n\nstacked = c(ocean,terrain.indices)\nstacked\n\nclass       : SpatRaster \ndimensions  : 2400, 2100, 9  (nrow, ncol, nlyr)\nresolution  : 0.01666667, 0.01666667  (x, y)\nextent      : 29.99167, 64.99167, -30.00833, 9.991677  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource(s)   : memory\nnames       : wiore~-7753,    slope, aspect,      TPI,  TRI, TRIriley, ... \nmin values  :       -6756,  0.00000,      0, -1200.00,    0,    0.000, ... \nmax values  :          -1, 34.89659,    360,   932.25, 1200, 3773.468, ... \n\n\n\nCodestacked %&gt;% \n  terra::plot()"
  },
  {
    "objectID": "ch6.html#sec-tidyterra",
    "href": "ch6.html#sec-tidyterra",
    "title": "10  Raster Manipulation",
    "section": "\n10.3 tidyterra package",
    "text": "10.3 tidyterra package\nIn Section 10.2, we learn how to create a raster from scratch and also how to read raster files stored in either our working directories or from online with terra package. In this section we expand our understanding of manipulating raster data with a tidyterra package.\nThe tidyterra package adds common methods from the tidyverse for SpatRaster and SpatVector objects created with the terra package. It enhances the data manipulation and analysis capabilities of SpatRaster and SpatVector objects, allowing for a seamless integration of spatial data into the tidyverse workflow.\nWith tidyterra, you can leverage the power of popular tidyverse packages like dplyr, tidyr, and ggplot2 for working with SpatRaster and SpatVector objects. It provides a set of methods and functions that align with the tidyverse philosophy, allowing for consistent and intuitive data manipulation.\nFurthermore, tidyterra includes specialized functions such as as_tibble() to convert SpatRaster and SpatVector objects into tidy tibbles, enabling smooth integration with other tidyverse functions and pipelines. This integration makes it easier to perform data wrangling, aggregation, and analysis on spatial datasets.\n\nCoderequire(terra)\nrequire(tidyterra)\nrequire(tidyverse)\n\n\nYou can use dplyr functions like, rename(), filter(), mutate(), and select() to filter, transform, and select specific subsets of your spatial data. The gather() and spread() functions from tidyr can be employed for reshaping your data into a tidy format.\n\n10.3.1 rename\nOne of the functions available in tidyterra is the rename() function, which is a wrapper around the rename()function in the *dplyr* package.rename()function in tidyterra allows to easily modify layer names in a in spatRaster object. By usingrename()`, we can provide new names for specific layer, which can be helpful for improving the clarity, consistency, and readability of the raster data.\nMoreover, when working with raster data from different sources or combine more than one raster object to create a stack spatRaster object, the rename() function enables us to harmonize raster layer names. By aligning the names of corresponding layer across rasterstack, we can ensure consistency and compatibility, making it easier to combine, compare, and analyze the data.\n\nCodebath.depth = bath.wio %&gt;% \n  tidyterra::rename(depth = 1)\n\nbath.depth\n\nclass       : SpatRaster \ndimensions  : 2400, 2100, 1  (nrow, ncol, nlyr)\nresolution  : 0.01666667, 0.01666667  (x, y)\nextent      : 29.99167, 64.99167, -30.00833, 9.991677  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource(s)   : memory\nname        : depth \nmin value   : -6756 \nmax value   :  5677 \n\n\n\n10.3.2 filter\nThe filter() function in the tidyterra package is a powerful tool for raster data manipulation and subsetting. It allows us to extract specific values from a raster object on certain conditions or criteria. Using filter(), we can define logical expressions that evaluate values in raster object. Values that meet the specified criteria are retained, while those that do not meet the conditions are removed from the resulting subset.\nThe filter() function is flexible and supports a wide range of conditions, including comparisons (==, !=, &lt;, &gt;, &lt;=, &gt;=), logical operators (&, |), and other functions like is.na() or startsWith(). This versatility enables us to filter data based on various criteria, such as specific values, ranges, patterns, or combinations of multiple conditions.\n\nCodeocean = bath.depth %&gt;% \n  tidyterra::filter(depth &lt;= 0)\n\nocean\n\nclass       : SpatRaster \ndimensions  : 2400, 2100, 1  (nrow, ncol, nlyr)\nresolution  : 0.01666667, 0.01666667  (x, y)\nextent      : 29.99167, 64.99167, -30.00833, 9.991677  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource(s)   : memory\nname        : depth \nmin value   : -6756 \nmax value   :     0 \n\n\n\n10.3.3 mutate\nThe mutate() function in the tidyterra package is a powerful tool for creating new raster object or modifying existing values within a raster. It allows us to add calculated raster or transform existing raster based on specific operations or functions.\nUsing mutate(), we can define new raster by specifying a name for the new raster and providing an expression that calculates its values. This expression can involve mathematical operations, logical operations, or functions that operate on individual values or entire raster.\nmutate() facilitates the creation of derived raster. It allows us to perform calculations, transformations, or modifications on the raster, enabling us to derive new insights or prepare the raster for further analysis.\nThe mutate() function operates on the principle of non-destructive modification, meaning that it creates a new layer of raster object with the modified values while preserving the original raster. This allows us to create a stacked raster and supports a more organized raster manipulation workflow.\n\nCodeocean.depth = ocean %&gt;% \n  tidyterra::mutate(bathy_m = depth * -1)\n\nocean.depth\n\nclass       : SpatRaster \ndimensions  : 2400, 2100, 2  (nrow, ncol, nlyr)\nresolution  : 0.01666667, 0.01666667  (x, y)\nextent      : 29.99167, 64.99167, -30.00833, 9.991677  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource(s)   : memory\nnames       : depth, bathy_m \nmin values  : -6756,       0 \nmax values  :     0,    6756 \n\n\n\n10.3.4 transmute\nUnlike mutate function, which add a raster in the existing spatraster object, a transmutate create a new raster object that is not stacked in the oriign fiel instead. Let’s say artisanal fisheries operates in shallow areas below 200 and we ought to dermacate this area.\n\nCodebath_class = ocean %&gt;% \n  tidyterra::transmute(\n    bath.class = if_else(depth  &lt; -200, \"Shallow\", \"Deep\")\n    )\n\nbath_class\n\nclass       : SpatRaster \ndimensions  : 2400, 2100, 1  (nrow, ncol, nlyr)\nresolution  : 0.01666667, 0.01666667  (x, y)\nextent      : 29.99167, 64.99167, -30.00833, 9.991677  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource(s)   : memory\ncategories  : label \nname        : bath.class \nmin value   :       Deep \nmax value   :    Shallow \n\n\n\n10.3.5 select\nThe select() function in tidyterra allow us to choose and rearrange layer(s) from stacked spatRaster object. It provides a flexible way to subset and reorder raster, making it easier to work with specific subsets of data or to reorganize the structure of the raster.\nUsing select(), we can specify which raster layer to include or exclude from the spatraster object based on their names or other conditions. select() also provides the ability to rename raster layer while selecting them, enabling us to give more meaningful or concise names to raster.\nThe select() function can be used in conjunction with other tidyterra functions, such as mutate(), filter(), and arrange(), to perform data manipulation tasks on SpatRaster and SpatVector objects\n\nCodeocean.depth  %&gt;% \n  tidyterra::select(depth)\n\nclass       : SpatRaster \ndimensions  : 2400, 2100, 1  (nrow, ncol, nlyr)\nresolution  : 0.01666667, 0.01666667  (x, y)\nextent      : 29.99167, 64.99167, -30.00833, 9.991677  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource(s)   : memory\nname        : depth \nmin value   : -6756 \nmax value   :     0 \n\n\n\n10.3.6 pull\nOne of the functions available in tidyterra is the pull() function, which is a wrapper around the pull() function in the dplyr package, The pull() function in the dplyr package is a useful tool for extracting a values from a raster object as vector. It allows us to easily access and work with a specific raster object of a spatraster.\n\nCodevalues.pull = ocean.depth %&gt;% \n  tidyterra::select(bathy_m ) %&gt;% \n  tidyterra::pull()\n\nvalues.pull[1:20]\n\n [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n\n\n\n10.3.7 Visualize\nIn addition to data manipulation, tidyterra also provides geom_spat*() functions for plotting SpatRaster and SpatVector objects using ggplot2. This allows for the creation of visually appealing and informative plots, leveraging the flexibility and customization options offered by ggplot2. For example, the bathymetry and elevation map shown in Figure 10.2 is created with tidyterra geom tools highlighted in chunk below;\n\nCodeggplot()+\n  tidyterra::geom_spatraster(data = bath.wio)+\n  tidyterra::scale_fill_hypso_tint_c(\n    palette = \"gmt_globe\",\n    labels = scales::label_number(),\n    breaks = seq(-6000, 6000,3000), \n    # limits = c(-6000, 6500),\n    guide = guide_colorbar(\n      direction = \"horizontal\",\n      title = \"Bathymetry & altitude (meters)\",\n      title.position = \"top\",\n      barwidth = 12, barheight = 0.8\n    ))+\n  coord_sf(expand = FALSE) +\n  theme_bw(base_size = 14) +\n  theme(legend.position = \"top\")\n\n\n\nFIGURE 10.2. Bathymetry and altitude in the WIO region plotted with tidyterra tools"
  },
  {
    "objectID": "ch6.html#references",
    "href": "ch6.html#references",
    "title": "10  Raster Manipulation",
    "section": "\n10.4 References",
    "text": "10.4 References\n\nCodeenso = wior::enso(begin_date = \"2015-01-01\", end_date = \"2023-04-30\")"
  },
  {
    "objectID": "ch6_processing.html#reading-in-data",
    "href": "ch6_processing.html#reading-in-data",
    "title": "11  Handling Rasters",
    "section": "\n11.1 Reading in data",
    "text": "11.1 Reading in data\nTo begin, let’s discuss the central function of the terra package for reading raster files - rast(). This function is designed to handle a wide range of raster file formats, making it a versatile tool for working with raster data. When a raster file is read using rast(), the resulting object is assigned its own class called SpatRaster.\nNow, let’s dive into a practical example. Suppose we want to read in a digital elevation model (DEM) for the Western Indian Ocean Region. With the terra package, this process becomes straightforward. We can start by calling the rast() function and providing the path to our raster file as an argument.\nFirst, make sure the terra package is installed and loaded in your R environment:\n\nCoderequire(terra)\nrequire(tidyterra)\nrequire(tidyverse)\nrequire(gt)\n\n\n\nCode# Read in the digital elevation model (DEM) for the Western Indian Ocean Region\ndem &lt;- rast(\"data/bathy_ele.tif\")\n\n\nOnce the raster file is read, it is stored in the dem object as a SpatRaster.\n\nCodedem\n\nclass       : SpatRaster \ndimensions  : 720, 570, 1  (nrow, ncol, nlyr)\nresolution  : 0.01666667, 0.01666667  (x, y)\nextent      : 38.49167, 47.99167, -12.00833, -0.008325335  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : bathy_ele.tif \nname        : wioregio-7753 \nmin value   :         -5040 \nmax value   :          2264 \n\n\nThis object contains both the spatial extent of the raster and its associated values. The coord. ref. field shows lon/lat WGS 84, which is Geographic Coordinates. The extent appears to be in degree, with the latitude being a mix of positive and negative numbers. If you just want to know the CRS from a SpatRaster, you just call crs() like so:\n\nCodedem |&gt;\n  terra::crs()\n\n[1] \"GEOGCRS[\\\"WGS 84\\\",\\n    ENSEMBLE[\\\"World Geodetic System 1984 ensemble\\\",\\n        MEMBER[\\\"World Geodetic System 1984 (Transit)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G730)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G873)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1150)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1674)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1762)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G2139)\\\"],\\n        ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        ENSEMBLEACCURACY[2.0]],\\n    PRIMEM[\\\"Greenwich\\\",0,\\n        ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    CS[ellipsoidal,2],\\n        AXIS[\\\"geodetic latitude (Lat)\\\",north,\\n            ORDER[1],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        AXIS[\\\"geodetic longitude (Lon)\\\",east,\\n            ORDER[2],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    USAGE[\\n        SCOPE[\\\"Horizontal component of 3D system.\\\"],\\n        AREA[\\\"World.\\\"],\\n        BBOX[-90,-180,90,180]],\\n    ID[\\\"EPSG\\\",4326]]\"\n\n\nAlthough the information is a bit disorganized, it does mention the presence of key details such as the datum and and the used World Geodetic System 1984. For instance, we can visualize the DEM using the plot() function from the terra package:\n\nCode# Visualize the DEM\nplot(dem)"
  },
  {
    "objectID": "ch6_processing.html#glimpse-of-a-raster",
    "href": "ch6_processing.html#glimpse-of-a-raster",
    "title": "11  Handling Rasters",
    "section": "\n11.2 Glimpse of a raster",
    "text": "11.2 Glimpse of a raster\n\nCodedem %&gt;% terra::res()\n\n[1] 0.01666667 0.01666667\n\n\nWith a cell size of 0.01666667 degrees, the equivalent grid size is approximately 1.83km. This information is crucial as it provides insight into the spatial resolution of the raster, which is estimated to be around 1.8km.\n\nCodedem %&gt;% terra::ext()\n\nSpatExtent : 38.491669165, 47.991671065, -12.008327735, -0.0083253350000021 (xmin, xmax, ymin, ymax)\n\n\nSimilarly, observing the geographical extent of the dem file, we can determine that it ranges from approximately 38.49 degrees East to 47.99 degrees East in longitude, and from 12 degrees South to around 0 degrees North in latitude.\n\nCodedem %&gt;% terra::nrow();\n\n[1] 720\n\nCodedem %&gt;% terra::ncol();\n\n[1] 570\n\nCodedem %&gt;% terra::ncell()\n\n[1] 410400\n\n\nIn summary, this raster file contains digital elevation and bathymetry data of the East Africa area. It has a spatial resolution of approximately 1.8km, covering a geographical extent from approximately 38.49 degrees East to 47.99 degrees East in longitude and from 12 degrees South to around 0 degrees North in latitude. The file consists of 720 rows and 570 columns, resulting in a total of 410,400 pixels."
  },
  {
    "objectID": "ch6_processing.html#cropping",
    "href": "ch6_processing.html#cropping",
    "title": "11  Handling Rasters",
    "section": "\n11.3 Cropping",
    "text": "11.3 Cropping\nCropping a raster file to the area of interest is essential for several reasons. Firstly, it helps to reduce the size of the dataset, making it more manageable and efficient to work with, particularly when dealing with large raster files. By focusing only on the area of interest, we can eliminate unnecessary data, thereby improving processing speed and reducing memory usage.\nAdditionally, cropping allows us to extract specific regions or features that are relevant to our analysis, enabling a more targeted and precise examination of the data. This can enhance the accuracy and reliability of subsequent analyses, as well as facilitate clearer visualization of the specific area under investigation. Ultimately, cropping a raster file to the area of interest streamlines the workflow, optimizes computational resources, and facilitates a more detailed analysis of the desired geographic region.\nTo crop a raster using the terra package in R, we need first to define the extent for cropping, which determine the extent of the area you want to crop. terra package has a ext function that define the cropping extent by specifying the minimum and maximum values for the x and y coordinates.\nFor example, we want to crop the data for the Pemba Channel and hence we can define the geographical cropping extent of the area as;\n\nCodepemba.channel = terra::ext(39.0, 40.0, -5.8, -4.2)\npemba.channel\n\nSpatExtent : 39, 40, -5.8, -4.2 (xmin, xmax, ymin, ymax)\n\n\n\n\n\n\n\n\nImportant\n\n\n\nKeep your eye on the the ext() function, as it need to follow the order xmin, xmax, ymin, ymax as opposed to xmin, ymin, xmax, ymax as you do for st_crop().\n\n\nThen we can crop the file using crop() function.\n\nCodepemba.dem = dem %&gt;% \n  terra::crop(pemba.channel) \n\npemba.dem\n\nclass       : SpatRaster \ndimensions  : 96, 60, 1  (nrow, ncol, nlyr)\nresolution  : 0.01666667, 0.01666667  (x, y)\nextent      : 38.99167, 39.99167, -5.808326, -4.208326  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource(s)   : memory\nname        : wioregio-7753 \nmin value   :         -1286 \nmax value   :           413 \n\n\nNotice that the cropped raster for the Pemba Channel has a spatial resolution of approximately 1.8km, which is similar to the origin file but covering a geographical extent from approximately 38.99 degrees East to 39.99 degrees East in longitude and from 5.8 degrees South to around 4.2 degrees South in latitude. The file consists of 96 rows and 60 columns, resulting in a total of 5,760 pixels. The values of raster ranges from 413 to around -1286 meters.\n\nCodepemba.dem %&gt;% \n  terra::plot()"
  },
  {
    "objectID": "ch6_processing.html#classifying-rasters",
    "href": "ch6_processing.html#classifying-rasters",
    "title": "11  Handling Rasters",
    "section": "\n11.4 Classifying rasters",
    "text": "11.4 Classifying rasters\nIn the terra package, the classify() function allows to assign divide continuous values into discrete or categories of different groups of raster cells based on specified criteria.\nTo perform raster classification using terra, you typically need a raster object and a classification scheme. The classification scheme defines the rules or conditions to determine how the values in the raster will be categorized. This can be achieved by providing a custom function or a set of thresholds.\nWe will divide our ocean bathymetry into two groups, all cells with less than or equal to 200 meters are shallow and those cells above 200 as deep\n\nCodecut.points  = c(-1300, -200,0)\n\n\n\nCode depth.class = pemba.dem %&gt;% \n  terra::classify(cut.points)\n\ndepth.class\n\nclass       : SpatRaster \ndimensions  : 96, 60, 1  (nrow, ncol, nlyr)\nresolution  : 0.01666667, 0.01666667  (x, y)\nextent      : 38.99167, 39.99167, -5.808326, -4.208326  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource(s)   : memory\ncategories  : wioregio-7753 \nname        : wioregio-7753 \nmin value   :  (-1300–-200] \nmax value   :      (-200–0] \n\n\n\nCodedepth.class %&gt;% \n  terra::plot()"
  },
  {
    "objectID": "ch6_processing.html#aggregating",
    "href": "ch6_processing.html#aggregating",
    "title": "11  Handling Rasters",
    "section": "\n11.5 Aggregating",
    "text": "11.5 Aggregating\nAdjusting the grid size of raster data, whether by increasing or decreasing it, serves several purposes. Increasing the grid size, or decreasing the resolution, is useful when fine details are not crucial or when working with large datasets, as it reduces computational burden and memory usage. On the other hand, decreasing the grid size, or increasing the resolution, allows for a more precise representation of spatial phenomena, which is advantageous for accurate analyses such as environmental modeling.\nGrid size adjustments also aid in data compatibility, harmonizing multiple datasets with different resolutions, and can enhance visualization by simplifying large-scale patterns. Additionally, modifying the grid size can optimize processing efficiency, trading off between detail and computational performance.\n\nCodepemba.dem.low = pemba.dem %&gt;% \n  terra::aggregate(fact = 5, fun = mean)\n\n\nThe aggregate() function reduces the resolution of the raster by aggregating cells into larger groups. In this case, the fact parameter is set to 5, indicating that each new cell in the aggregated raster will represent a 5x5 grid of the original cells. The fun parameter is set to “mean”, meaning that the values of the cells within each group will be averaged to produce the value of the aggregated cell. This obviously results in some data loss, but that can be acceptable, depending on the purpose of your analysis. Note that you can pass just about any function to fun =, like min(), max() or even your own function.\n\nCodepemba.dem.low\n\nclass       : SpatRaster \ndimensions  : 20, 12, 1  (nrow, ncol, nlyr)\nresolution  : 0.08333335, 0.08333335  (x, y)\nextent      : 38.99167, 39.99167, -5.874993, -4.208326  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource(s)   : memory\nname        : wioregio-7753 \nmin value   :      -1202.76 \nmax value   :        258.20 \n\n\nThe resulting raster, assigned to the object pemba.dem.low, has a lower resolution compared to the original pemba.dem raster, as cells are combined and averaged."
  },
  {
    "objectID": "ch6_processing.html#resampling",
    "href": "ch6_processing.html#resampling",
    "title": "11  Handling Rasters",
    "section": "\n11.6 Resampling",
    "text": "11.6 Resampling\nWhile the aggregate() function in the terra package is used to increase the grid size and decrease the resolution of a raster, the resample() function is used to modify the grid size and change the resolution of the expected output. By specifying the target resolution or grid size, you can increase or decrease the resolution of the output raster.\nTo understand the concept of resampling, I am going to demonstrate using the MUR SST product, which has a spatial resolution of approximately 1 kilometer (km). It offers high-resolution global coverage, providing detailed information about sea surface temperature patterns at a fine scale.\nLet’s read the file into session\n\nCodesst = terra::rast(\"data/pemba_mur.nc\")\nsst\n\nclass       : SpatRaster \ndimensions  : 201, 301, 1  (nrow, ncol, nlyr)\nresolution  : 0.01, 0.01  (x, y)\nextent      : 37.995, 41.005, -6.005, -3.995  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource      : pemba_mur.nc \nvarname     : analysed_sst (Analysed Sea Surface Temperature) \nname        : analysed_sst \nunit        :     degree_C \ntime        : 2023-06-14 09:00:00 UTC \n\n\nUpon observation, it becomes apparent that the sea surface temperature (SST) dataset has a resolution of 0.01 degree, which translates to approximately 1.1 kilometers. This resolution is notably higher compared to the 1.8-kilometer resolution of the digital elevation model (DEM).\n\nCodepemba.dem.high = pemba.dem %&gt;% \n  terra::resample(sst)\n\npemba.dem.high\n\nclass       : SpatRaster \ndimensions  : 201, 301, 1  (nrow, ncol, nlyr)\nresolution  : 0.01, 0.01  (x, y)\nextent      : 37.995, 41.005, -6.005, -3.995  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource(s)   : memory\nname        : wioregio-7753 \nmin value   :    -1284.7975 \nmax value   :      389.0256 \n\n\nThe resampled pemba.dem.high now has a resolution of 0.01 degree, which translates to approximately 1.1 kilometers, the same value to that of sea surface temperature. We can confirm the resolutions using a res function.\n\nCodepemba.dem.low %&gt;% terra::res()*110;\n\n[1] 9.166669 9.166669\n\nCodepemba.dem %&gt;% terra::res()*110;\n\n[1] 1.833334 1.833334\n\nCodepemba.dem.high %&gt;% terra::res()*110\n\n[1] 1.1 1.1\n\nCodesst %&gt;% terra::res()*110\n\n[1] 1.1 1.1\n\n\nSimilarly, using the resample() function, it is possible to modify the grid size and adjust the resolution of a raster dataset. In this context, we want to decrease the SST data resolution to align it with the DEM.\n\nCodesst.low = sst %&gt;% \n  resample(pemba.dem)\n\nsst.low\n\nclass       : SpatRaster \ndimensions  : 96, 60, 1  (nrow, ncol, nlyr)\nresolution  : 0.01666667, 0.01666667  (x, y)\nextent      : 38.99167, 39.99167, -5.808326, -4.208326  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource(s)   : memory\nname        : analysed_sst \nmin value   :     -7.76800 \nmax value   :     27.97364 \ntime        : 2023-06-14 09:00:00 UTC \n\n\n\nThis process ensures that both datasets share a consistent grid size, enabling accurate analysis and comparison between the SST and DEM datasets at a matching resolution.\n\nlet’s check whether the extents is the same for temperature and dem\n\nCodepemba.dem %&gt;% terra::ext();\n\nSpatExtent : 38.991669265, 39.991669465, -5.808326495, -4.208326175 (xmin, xmax, ymin, ymax)\n\nCodesst.low %&gt;% terra::ext()\n\nSpatExtent : 38.991669265, 39.991669465, -5.808326495, -4.208326175 (xmin, xmax, ymin, ymax)\n\n\nWe can generate visual representations of the temperature, elevation, and bathymetry variations in the Pemba Channel by plotting the corresponding raster datasets.\n\nCodepar(mfrow = c(1,2))\n\npemba.dem %&gt;% \n  terra::plot()\n\nsst.low%&gt;% \n  terra::plot()"
  },
  {
    "objectID": "ch6_processing.html#stacking-raster",
    "href": "ch6_processing.html#stacking-raster",
    "title": "11  Handling Rasters",
    "section": "\n11.7 Stacking raster",
    "text": "11.7 Stacking raster\nIn terra, you can stack multiple rasters into a single seamless raster object using the c() function, which combines SpatRaster objects as long as they have the same pixel size, extent, and coordinate reference system (CRS) . This is useful when working with multiple rasters that represent different variables or time steps, as it simplifies the reading of raster data by using a single object to represent multiple rasters.\nOnce we are sure that our raster have similar geographical extents and spatial resolution, we can stack them together using c function in R. This function combines several raster objects into a single stack or brick, enabling simultaneous analysis and manipulation of the stacked layers.\nRaster stacking allows for convenient access and processing of multiple layers within a single object, facilitating operations such as extracting values, conducting calculations, or visualizing the combined information. The stacked raster is a useful tool for analyzing and exploring the relationships and variations among different raster datasets in a unified manner.\n\nCodepemba.rast = c(sst.low, pemba.dem)\npemba.rast\n\nclass       : SpatRaster \ndimensions  : 96, 60, 2  (nrow, ncol, nlyr)\nresolution  : 0.01666667, 0.01666667  (x, y)\nextent      : 38.99167, 39.99167, -5.808326, -4.208326  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource(s)   : memory\nnames       : analysed_sst, wioregio-7753 \nmin values  :     -7.76800,         -1286 \nmax values  :     27.97364,           413"
  },
  {
    "objectID": "ch6_processing.html#basic-plotting",
    "href": "ch6_processing.html#basic-plotting",
    "title": "11  Handling Rasters",
    "section": "\n11.8 Basic Plotting",
    "text": "11.8 Basic Plotting\nWith the dataset size reduced dataset and stacked the file, we can now explore the visualization capabilities offered by the base plotting function. This function, built into R, provides a wide range of options for creating plots and graphs from raster dataset.\n\nCodepemba.rast %&gt;% \n  plot(\n    main = c(\"Sea Surface Temperature\", \"Elevation & Bathymetry\"),\n    xlab = \"X-axis\",         # Label for the x-axis\n    ylab = \"Y-axis\",         # Label for the y-axis\n     )"
  },
  {
    "objectID": "ch6_processing.html#raster-manipulation",
    "href": "ch6_processing.html#raster-manipulation",
    "title": "11  Handling Rasters",
    "section": "\n11.9 Raster Manipulation",
    "text": "11.9 Raster Manipulation\nThe packages “terra” allow for easy manipulation of raster data in R. With these packages, rasters can be treated as variables in mathematical equations, allowing for exploration of data loss by calculating the difference between original and disaggregated DEMS.\n\nCodeggplot() +\n  geom_spatraster(data = sst.low)\n\n\n\nFIGURE 11.1. Sea surface temperature\n\n\n\n\nHere we stumble! Why? because this area is close to the equator and hardly find sea surface temperature below 25 degree. But Figure 11.1 indicate a range of temperature from -7 to 28 oC. The color ramp clearly show as the data has the land with darkblue and the ocean with light blue colors, instead of the gradient colors. We should not be fooled by the colors but lets dig the numbers. Lets first check the summary of the raster.\n\nCodesst.low %&gt;% summary()\n\n  analysed_sst   \n Min.   :-7.768  \n 1st Qu.:27.155  \n Median :27.561  \n Mean   :20.190  \n 3rd Qu.:27.711  \n Max.   :27.974  \n\n\nWe notice that from the first quantile through the maximum value are less than one degree celcius, but the minimum values is far low. Then we spot this as outlier. Let’s us make a histogram of the sst from the raster\n\nCodesst.low %&gt;% \n  terra::as.data.frame() %&gt;% \n  ggplot(aes(x = analysed_sst)) +\n  geom_histogram()\n\n\n\n\n\nCodesst.low = sst.low %&gt;% \n  tidyterra::rename(sst = 1) %&gt;% \n  tidyterra::filter(sst &gt; 0)\n\nsst.low\n\nclass       : SpatRaster \ndimensions  : 96, 60, 1  (nrow, ncol, nlyr)\nresolution  : 0.01666667, 0.01666667  (x, y)\nextent      : 38.99167, 39.99167, -5.808326, -4.208326  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource(s)   : memory\nname        :      sst \nmin value   : 26.69426 \nmax value   : 27.97364 \n\n\nNow the raster file has correct temperature range between 26 and 28 degree celcius. we can make a plot to see the spatial distribution of this temperature in the channel\n\nCodepemba.rast = c(sst.low, pemba.dem)\n\n\n\nCodepemba.rast %&gt;% \n  plot(\n    main = c(\"Sea Surface Temperature\", \"Elevation & Bathymetry\"),\n    xlab = \"X-axis\",         # Label for the x-axis\n    ylab = \"Y-axis\",         # Label for the y-axis\n    col = hcl.colors(palette = \"Spectral\", n = 20)\n     )\n\n\n\n\nWe can use mutate together with if_else function to classify raster based on a threshold value. For instance, we know from literature that artisanal fisheries operated within shallow water of less than 200 meters. Hence, we need to demarcate this area for fisheries management plans. Instead of using mutate, which add raster to existing raster to form stacked raster object, transmute create a raster with only computed values.\n\nCodepemba.depth.class = pemba.rast %&gt;% \n  tidyterra::rename(bathy = 2) %&gt;% \n  tidyterra::transmute(\n    bath.class = if_else(bathy &gt; -200, \"Shallow\", \"Deep\")\n    )\npemba.depth.class\n\nclass       : SpatRaster \ndimensions  : 96, 60, 1  (nrow, ncol, nlyr)\nresolution  : 0.01666667, 0.01666667  (x, y)\nextent      : 38.99167, 39.99167, -5.808326, -4.208326  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource(s)   : memory\ncategories  : label \nname        : bath.class \nmin value   :       Deep \nmax value   :    Shallow \n\n\n\nCodepemba.depth.class %&gt;% \n  plot(\n    # main = c(\"Sea Surface Temperature\", \"Elevation & Bathymetry\"),\n    xlab = \"X-axis\",         # Label for the x-axis\n    ylab = \"Y-axis\",         # Label for the y-axis\n    col = c(\"red\", \"green\")\n     )\n\n\n\n\n\nCodepemba.depth.class %&gt;% \n  terra::expanse()\n\n  layer        area\n1     1 19620711224\n\n\nTo visualize data in R, you can use the ggplot2 package, which provides a flexible and powerful system for creating graphics[1][2]. The ggplot2 package uses a grammar of graphics that allows you to build complex plots by combining simple components[1]. However, if you prefer to use the base plotting function in R, you can do so by using the plot() function in terra[3]. The plot() function creates a map of the values of a SpatRaster or SpatVector object, which can be customized using the available arguments[3]. Overall, both ggplot2 and plot() are useful tools for visualizing data in R, and the choice of which one to use depends on personal preference and the specific needs of the analysis.\n\nCodeggplot() +\n  # ggspatial::annotation_map_tile(type = \"osm\", zoom = 8, progress = \"none\")+\n  geom_spatraster(data = sst.low, alpha = 1) +\n  ggspatial::annotation_scale()"
  },
  {
    "objectID": "ch6_processing.html#point-extraction",
    "href": "ch6_processing.html#point-extraction",
    "title": "11  Handling Rasters",
    "section": "\n11.10 Point Extraction",
    "text": "11.10 Point Extraction\n\nCodechl = list.files(\"data/satellite data/Chl-a/\", pattern = \".nc\", full.names = TRUE)\nsst = list.files(\"data/satellite data/SST/\", pattern = \".nc\", full.names = TRUE)\n\n\nfor (i in 1:12){\n\nlabel = paste0(\"data/satellite data/processed/\",\"chl_\",month.abb[i] %&gt;% str_to_lower(), \".tif\")\nlabel2 = paste0(\"data/satellite data/processed/\",\"sst_\",month.abb[i] %&gt;% str_to_lower(), \".tif\")\n\nchl[i] %&gt;% \n  terra::rast() %&gt;% \n  tidyterra::select(chl = 2) %&gt;% \n  # tidyterra::filter(chl &lt; 10) %&gt;%\n  # terra::crop(pemba.dem)  %&gt;%\n  terra::writeRaster(filename = label)\n\nlabe2 = paste0(\"data/satellite data/processed/\",\"sst_\",month.abb[i] %&gt;% str_to_lower(), \".tif\")\n\nsst[i] %&gt;% \n  terra::rast() %&gt;% \n  tidyterra::select(sst = 1) %&gt;% \n  # tidyterra::filter(chl &lt; 10) %&gt;%\n  # terra::crop(pemba.dem)  %&gt;%\n  terra::writeRaster(filename = label2)\n\n  \n}\n\n\n\nCodesst.list = list.files(path = \"data/satellite data/processed/\", pattern = \"sst\", include.dirs = TRUE, full.names = TRUE) \n\nsst.stack = sst.list %&gt;% \n  terra::rast() \n\nsst.stack = sst.stack %&gt;% \n  tidyterra::select(\n    jan = 5,  feb = 4, mar = 8, apr = 1, may = 9, jun = 7, jul = 6, aug = 2, sep = 12, oct = 11, nov = 10, dec = 3\n  )\n\n\n\nCodepemba.sst.stack = sst.stack %&gt;% \n  terra::crop(pemba.dem)\n\n\n\nCodepemba.sst.stack %&gt;% \n  terra::plot(\n    col = hcl.colors(palette = \"RdBu\", n = 12, rev = TRUE)\n    )\n\n\n\n\n\nCodechl.list = list.files(path = \"data/satellite data/processed/\", pattern = \"chl\", include.dirs = TRUE, full.names = TRUE) \n\nchl.stack = chl.list %&gt;% \n  terra::rast() \n\nchl.stack = chl.stack %&gt;% \n  tidyterra::select(\n    jan = 5,  feb = 4, mar = 8, apr = 1, may = 9, jun = 7, jul = 6, aug = 2, sep = 12, oct = 11, nov = 10, dec = 3\n  )\n\n\n\nCodepemba.chl.stack = chl.stack %&gt;% \n  terra::crop(pemba.dem)\n\n\n\nCodepemba.chl.stack %&gt;% \n  terra::hist()\n\n\n\n\n\nCodepemba.chl.stack %&gt;% \n  terra::plot(\n    col = hcl.colors(palette = \"RdBu\", n = 12, rev = TRUE)\n    )\n\n\n\n\n\nCodevikatio = pemba.chl.stack %&gt;% \n  terra::values() %&gt;% \n  quantile( c(.05,.95), na.rm = TRUE) %&gt;% \n  as.vector()\n\n\n\nCodeggplot() +\n  geom_spatraster(data = pemba.chl.stack)+\n  facet_wrap(~lyr, nrow = 2)+\n  scale_fill_gradientn(colours = hcl.colors(n = 12, palette = \"RdBu\", rev = TRUE), na.value = NA, limits = vikatio)\n\n\n\n\n\nCodeset.seed(1254)\n\nrandom.locations = pemba.dem %&gt;% \n  terra::as.polygons() %&gt;% \n  sf::st_as_sf() %&gt;% \n  sf::st_sample(size = 50) %&gt;% \n  sf::st_as_sf() %&gt;% \n  dplyr::mutate(id = 1:n())\n\nrandom.locations.coords = random.locations %&gt;% \n  st_coordinates() %&gt;% \n  as_tibble() %&gt;% \n  select(lon = 1, lat = 2)\n\n\n\nCodeggplot() +\n  ggspatial::annotation_map_tile(type = \"osm\", zoom = 8) +\n  ggspatial::layer_spatial(data = random.locations)\n\n\n\n\n\n11.10.1 Temperature monthly\n\nCodepemba.sst.locations = pemba.sst.stack %&gt;% \n  terra::extract(random.locations)\n\n\n\nCoderandom.locations.coords %&gt;% \n  bind_cols(\n    pemba.sst.locations\n  ) %&gt;% \n  gt::gt()%&gt;% \n  gt::opt_interactive(\n    use_search = TRUE,\n    # use_filters = TRUE,\n    use_resizers = TRUE,\n    use_highlight = TRUE,\n    use_compact_mode = TRUE,\n    use_text_wrapping = FALSE,\n    use_page_size_select = TRUE\n  ) %&gt;%\n  gt::fmt_number(decimals = 3) |&gt;\n  gt::fmt_integer(ID) |&gt;\n  gt::cols_label_with(\n    fn = ~ janitor::make_clean_names(., case = \"all_caps\")\n  ) |&gt;\n  gt::data_color(\n    columns = jan:mar,\n    palette = \"Reds\"\n  ) |&gt;\n  gt::data_color(\n    columns = may:sep,\n    palette = \"Blues\"\n  ) |&gt;\n  gt::data_color(\n    columns = nov:dec,\n    palette = \"Reds\"\n  )|&gt;\n  gt::data_color(\n    columns = c(apr, oct),\n    palette = \"Greens\"\n  )|&gt;\n  gt::tab_style(\n    style = cell_fill(color = \"gray95\"),\n    locations = cells_body(columns = c(lon, lat))\n  ) |&gt;\n  gt::tab_style(\n    locations = cells_body(columns = ID),\n    style = cell_text(weight = \"bold\")\n  ) \n\n\n?(caption)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodesst.extracts = random.locations.coords %&gt;% \n  bind_cols(pemba.sst.locations)\n\n\nsst.extracts %&gt;% \n  pivot_longer(cols = jan:dec, names_to = \"months\", values_to = \"data\") %&gt;% \n  mutate(months = str_to_title(months), months = forcats::fct_inorder(months)) %&gt;% \n  ggplot(aes(x = months, y = data))+\n  geom_boxplot()\n\n\n\n\n\n11.10.2 Chlorophyll monthly\n\nCodepemba.chl.locations = pemba.chl.stack %&gt;% \n  terra::extract(random.locations)\n\n\n\nCoderandom.locations.coords %&gt;% \n  bind_cols(\n    pemba.chl.locations\n  ) %&gt;% \n  gt::gt()%&gt;% \n  gt::opt_interactive(\n    use_search = TRUE,\n    # use_filters = TRUE,\n    use_resizers = TRUE,\n    use_highlight = TRUE,\n    use_compact_mode = TRUE,\n    use_text_wrapping = FALSE,\n    use_page_size_select = TRUE\n  ) %&gt;%\n  gt::fmt_number(decimals = 3) |&gt;\n  gt::fmt_integer(ID) |&gt;\n  gt::cols_label_with(\n    fn = ~ janitor::make_clean_names(., case = \"all_caps\")\n  ) |&gt;\n  gt::data_color(\n    columns = jan:mar,\n    palette = \"Reds\"\n  ) |&gt;\n  gt::data_color(\n    columns = may:sep,\n    palette = \"Blues\"\n  ) |&gt;\n  gt::data_color(\n    columns = nov:dec,\n    palette = \"Reds\"\n  )|&gt;\n  gt::data_color(\n    columns = c(apr, oct),\n    palette = \"Greens\"\n  )|&gt;\n  gt::tab_style(\n    style = cell_fill(color = \"gray95\"),\n    locations = cells_body(columns = c(lon, lat))\n  ) |&gt;\n  gt::tab_style(\n    locations = cells_body(columns = ID),\n    style = cell_text(weight = \"bold\")\n  ) \n\n\n?(caption)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodechl.extracts = random.locations.coords %&gt;% \n  bind_cols(pemba.chl.locations)\n\n\nchl.extracts %&gt;% \n  pivot_longer(cols = jan:dec, names_to = \"months\", values_to = \"data\") %&gt;% \n  mutate(months = str_to_title(months), months = forcats::fct_inorder(months)) %&gt;% \n  dplyr::filter(data &lt; 1) %&gt;% \n  ggplot(aes(x = months, y = data))+\n  geom_boxplot()+\n  scale_y_continuous(trans = scales::log10_trans())"
  },
  {
    "objectID": "ch6_processing.html#raster-conversion",
    "href": "ch6_processing.html#raster-conversion",
    "title": "11  Handling Rasters",
    "section": "\n11.11 Raster conversion",
    "text": "11.11 Raster conversion\nyou can derive contour lines as spatVector from bathymetry or elevation raster with as.contour() function\n\nCodedepth.contour = pemba.dem %&gt;% \n  terra::as.contour(level = seq(-1200,0,200))\n\ndepth.contour\n\n class       : SpatVector \n geometry    : lines \n dimensions  : 7, 1  (geometries, attributes)\n extent      : 39, 39.98334, -5.799993, -4.21666  (xmin, xmax, ymin, ymax)\n coord. ref. : lon/lat WGS 84 (EPSG:4326) \n names       : level\n type        : &lt;num&gt;\n values      : -1200\n               -1000\n                -800\n\n\n\nCodedepth.contour %&gt;% \n  terra::plot()\n\n\n\n\nWe can also\n\nCodedepth.classes = pemba.dem %&gt;% \n  terra::classify(c(-1300,-800,-500,-200,0)) %&gt;% \n  terra::as.polygons()\n\n\n\nCodedepth.classes %&gt;% \n  terra::plot()"
  },
  {
    "objectID": "ch7.html#aims",
    "href": "ch7.html#aims",
    "title": "12  Vector layers",
    "section": "12.1 Aims",
    "text": "12.1 Aims\nOur aims in this chapter are:\n\nBecome familiar with data structures for vector layers: points, lines and polygons\nExamine spatial and non-spatial properties of vector layers\nCreate subsets of vector layers based on their attributes\nLearn to transform a layer from one Coordinate Reference System (CRS) to another\n\nWe will use the following R packages:\n\nsf\nmapview\nstars"
  },
  {
    "objectID": "ch7.html#vector-layers",
    "href": "ch7.html#vector-layers",
    "title": "12  Vector layers",
    "section": "12.2 Vector layers",
    "text": "12.2 Vector layers\n\n12.2.1 What is a vector layer?\nVector layers are essentially sets of geometries associated with non-spatial attributes (Figure 7.1). The geometries are sequences of one or more point coordinates, possibly connected to form lines or polygons. The non-spatial attributes come in the form of a table.\nFigure 7.1: Geometry (left) and non-spatial attributes (right) of vector layers (https://www.neonscience.org/dc-shapefile-attributes-r)\n\n\n12.2.2 Vector file formats\nCommonly used vector layer file formats (Table 7.1) include binary formats (such as the Shapefile) and plain text formats (such as GeoJSON). Vector layers are also frequently kept in a spatial database, such as PostgreSQL/PostGIS.\n\n\n\n\n\n\n\n\n\n\nTable 7.1: Common vector layer file formats\n\n\nType\nFormat\nFile extension\n\n\nBinary\nESRI Shapefile\n.shp, .shx, .dbf, .prj, …\n\n\n\nGeoPackage (GPKG)\n.gpkg\n\n\nPlain Text\nGeoJSON\n.json or .geojson\n\n\n\nGPS Exchange Format (GPX)\n.gpx\n\n\n\nKeyhole Markup Language (KML)\n.kml\n\n\nSpatial Databases\nPostGIS / PostgreSQL\n\n\n\n\n\n\n12.2.3 The sp package\nThe first R package to establish a uniform vector layer class system was sp, released in 2005. Together with rgdal (2003) and rgeos (2011), the sp package dominated the landscape of spatial analysis in R for many years.\nThe sp package defines 6 main classes for vector layers (Table 7.2):\n\nOne for each geometry type (points, lines, polygons)\nOne for geometry only and one for geometry with attributes\n\n\n\n\n\n\n\n\n\n\n\nTable 7.2: Spatial data structures in package sp\n\n\nClass\nGeometry type\nAttributes\n\n\nSpatialPoints\nPoints\n-\n\n\nSpatialPointsDataFrame\nPoints\ndata.frame\n\n\nSpatialLines\nLines\n-\n\n\nSpatialLinesDataFrame\nLines\ndata.frame\n\n\nSpatialPolygons\nPolygons\n-\n\n\nSpatialPolygonsDataFrame\nPolygons\ndata.frame\n\n\n\nWe are not going to use sp in this book, but the newer sf package (Section 7.1.4). However, if you are going to work with spatial data in R it is very likely you will encounter sp in forums, books, or other packages, so it is important to be aware of it.\n\n\n12.2.4 The sf package\nThe sf package (Figure 7.2), released in 2016, is a newer package for working with vector layers in R, which we are going to use in this book. In recent years, sf has become the standard package for working with vector data in R, practically replacing sp, rgdal, and rgeos.\nFigure 7.2: Pebesma, 2018, The R Journal (https://journal.r-project.org/archive/2018-1/)\nOne of the important innovations in sf is a complete implementation of the Simple Features standard. Since 2003, Simple Features been widely implemented in spatial databases (such as PostGIS), commercial GIS (e.g., ESRI ArcGIS) and forms the vector data basis for libraries such as GDAL. The Simple Features standard defines several types of geometries, of which seven are most commonly used in the world of GIS and spatial data analysis (Figure 7.6). When working with spatial databases, Simple Features are commonly specified as Well Known Text (WKT). A subset of simple features forms the GeoJSON standard.\nThe sf package depends on several external software components (installed along with the R package27), most importantly GDAL, GEOS and PROJ (Figure 7.3). These well-tested and popular open-source components are common to numerous open-source and commercial software for spatial analysis, such as QGIS and PostGIS.\nFigure 7.3: sf package dependencies (https://github.com/edzer/rstudio_conf)\nPackage sf defines a hierarchical class system with three classes (Table 7.3):\n\nClass sfg—a single geometry\nClass sfc—a geometry column, which is a set of sfg geometries + CRS information\nClass sf—a layer, which is an sfc geometry column inside a data.frame with non-spatial attributes\n\n\n\n\n\n\n\n\n\n\n\nTable 7.3: Spatial data structures in package sf\n\n\nClass\nHierarchy\nInformation\n\n\nsfg\nGeometry\ntype, coordinates\n\n\nsfc\nGeometry column\nset of sfg + CRS\n\n\nsf\nLayer\nsfc + attributes\n\n\n\nThe sf class represents a vector layer by extending the data.frame class, supplementing it with a geometry column. This is similar to the way that spatial databases are structured. For example, the sample dataset shown in Figure 7.4 represents a polygonal layer with three features and six non-spatial attributes. The attributes refer to demographic and epidemiological attributes of US counties, such as the number of births in 1974 (BIR74), the number of sudden infant death cases in 1974 (SID74), and so on. The seventh column is the geometry column, containing the polygon geometries.\nFigure 7.4: Structure of an sf object (https://cran.r-project.org/web/packages/sf/vignettes/sf1.html)\nFigure 7.5 shows what the layer in Figure 7.4 would look like when mapped. We can see the outline of the three polygons, as well as the values of all six non-spatial attributes (in separate panels).\nFigure 7.5: Visualization of the sf object shown in Figure 7.4"
  },
  {
    "objectID": "ch7.html#vector-layers-from-scratch",
    "href": "ch7.html#vector-layers-from-scratch",
    "title": "12  Vector layers",
    "section": "12.3 Vector layers from scratch",
    "text": "12.3 Vector layers from scratch\n\n12.3.1 Overview\nAs mentioned above (Table 7.3), the sf package defines a hierarchical system of data structures, composed of three classes, from simple to complex: sfg, sfc and sf. In this section, we are going to create an object of each of those thress classes, to learn more about them.\n\n\n12.3.2 Geometry (sfg)\nObjects of class sfg, i.e., a single geometry, can be created using the appropriate function for each geometry type:\n\nst_point\nst_multipoint\nst_linestring\nst_multilinestring\nst_polygon\nst_multipolygon\nst_geometrycollection\n\nfrom coordinates passed as:\n\nnumeric vectors—POINT\nmatrix objects—MULTIPOINT or LINESTRING\nlist objects—All other geometries\n\nThe seven most commonly used Simple Feature geometry types are displayed in Figure 7.6.\nFigure 7.6: Simple feature geometry (sfg) types in package sf\nOf those seven types, the GEOMETRYCOLLECTION is more rarely used and more difficult to work with. For example, the Shapefile format does not support geometries of type GEOMETRYCOLLECTION. You may wonder why does it even exist. One of the reasons is that some spatial operations may produce a mixture of geometry types. For example, the intersection (Section 8.3.4.6) between two polygons may be composed of points, lines, and polygons (Figure 7.7).\nFigure 7.7: Intersection between two polygons may yield a GEOMETRYCOLLECTION\nLet's create some sfg geometries to see the principles in action. For example, we can create a point geometry object named pnt1, representing a POINT geometry, using the st_point function as follows:\nlibrary(sf) pnt1 = st_point(c(34.798443, 31.243288))\nPrinting an sfg object in the console gives its WKT representation:\npnt1 ## POINT (34.79844 31.24329)\nNote that the class definition of an sfg (geometry) object:\nclass(pnt1) ## [1] \"XY\"    \"POINT\" \"sfg\"\nis actually composed of three parts:\n\n\"XY\"—The dimensions type (one of: \"XY\", \"XYZ\", \"XYM\" or \"XYZM\"). In this book, as in most cases of spatial analysis in general, we will be working only with two-dimensional \"XY\" geometries.\n\"POINT\"—The geometry type (one of the geometry types: \"POINT\", \"MULTIPOINT\", etc.)\n\"sfg\"—The general class (sfg = Simple Feature Geometry)\n\nHere is another example of creating an sfg object. This time, we are creating a POLYGON geometry named a, using function st_polygon. (Don't worry if the expression is unclear: we learn about using list in Chapter 11).\na = st_polygon(list(cbind(c(0,0,7.5,7.5,0),c(0,-1,-1,0,0))))\nAgain, printing the object shows its WKT representation:\na ## POLYGON ((0 0, 0 -1, 7.5 -1, 7.5 0, 0 0))\nwhile class reports the dimensionality, geometry type, and general class, in that order:\nclass(a) ## [1] \"XY\"      \"POLYGON\" \"sfg\"\nThe polygon is displayed in Figure 7.8:\nplot(a, border = \"blue\", col = \"#0000FF33\", lwd = 2)\nFigure 7.8: An sfg object of type POLYGON\nLet's create another POLYGON, named b:\nb = st_polygon(list(cbind(c(0,1,2,3,4,5,6,7,7,0),c(1,0,0.5,0,0,0.5,-0.5,-0.5,1,1))))\nb ## POLYGON ((0 1, 1 0, 2 0.5, 3 0, 4 0, 5 0.5, 6 -0.5, 7 -0.5, 7 1, 0 1))\nclass(b) ## [1] \"XY\"      \"POLYGON\" \"sfg\"\nThe second polygon is shown in Figure 7.9.\nplot(b, border = \"red\", col = \"#FF000033\", lwd = 2)\nFigure 7.9: Another sfg object of type POLYGON\nThe c function, when given sfg geometries, combines those geometries into one. For example, combining two POLYGON geometries results in a single MULTIPOLYGON geometry:\nab = c(a, b)\nab ## MULTIPOLYGON (((0 0, 0 -1, 7.5 -1, 7.5 0, 0 0)), ((0 1, 1 0, 2 0.5, 3 0, 4 0, 5 0.5, 6 -0.5, 7 -0.5, 7 1, 0 1)))\nclass(ab) ## [1] \"XY\"           \"MULTIPOLYGON\" \"sfg\"\n\nWhat type of geometry do you think c(a, b, pnt1) is?\n\nKeep in mind that c always returns a single geometry, composed of all the shapes in its input. This is different from collecting the geometries into a geometry column, where the geometries are kept separate, which is done using function st_sfc as shown below (Section 7.2.3).\nThe multipolygon we created is shown in Figure 7.10:\nplot(ab, border = \"darkgreen\", col = \"#00FF0033\", lwd = 2)\nFigure 7.10: An sfg object of type MULTIPOLYGON\nA new geometry can be calculated applying various functions on an existing one(s). For example, the following example calculates the intersection of a and b, which is a new geometry hereby named i. We are going to learn about st_intersection, and other geometry-generating functions, in Chapter 8.\ni = st_intersection(a, b)\nThe result happens to be a GEOMETRYCOLLECTION, as demonstrated in Figure 7.7:\ni ## GEOMETRYCOLLECTION (POINT (1 0), LINESTRING (4 0, 3 0), POLYGON ((5.5 0, 7 0, 7 -0.5, 6 -0.5, 5.5 0)))\nclass(i) ## [1] \"XY\"                 \"GEOMETRYCOLLECTION\" \"sfg\"\nFigure 7.11 displays the GEOMETRYCOLLECTION named i:\nplot(i, border = \"black\", col = \"darkgrey\", lwd = 2)\nFigure 7.11: An sfg object of type GEOMETRYCOLLECTION\n\n\n12.3.3 Geometry column (sfc)\nLet's create two more point geometries (Section 7.2.2) named pnt2 and pnt3, representing two more points:\npnt2 = st_point(c(34.812831, 31.260284)) pnt3 = st_point(c(35.011635, 31.068616))\nGeometry objects (sfg) can be collected into a geometry column (sfc) object. This is done with function st_sfc.\nIn addition to the geometries, a geometry column object also contains a Coordinate Reference System (CRS) (more information on CRS is given below, in Section 7.9) definition, specified with the crs parameter of function st_sfc. Four types of CRS definitions are accepted:\n\nAn EPSG code (e.g., 4326)\nA PROJ4 string (e.g., \"+proj=longlat +datum=WGS84 +no_defs\")\nA WKT string\nA crs object of another layer, as returned by st_crs\n\nLet's combine the three POINT geometries—pnt1, pnt2 and pnt3—into a geometry column (sfc) object named geom. We will specify that the coordinates are lon/lat (WGS84), using the simplest of the three methods—an EPSG code (4326). More information on types of CRS, as well as where we can find the EPSG code of a particular CRS, is given in Section 7.9.2.\ngeom = st_sfc(pnt1, pnt2, pnt3, crs = 4326)\nHere is a summary of the resulting geometry column:\ngeom ## Geometry set for 3 features  ## Geometry type: POINT ## Dimension:     XY ## Bounding box:  xmin: 34.79844 ymin: 31.06862 xmax: 35.01164 ymax: 31.26028 ## Geodetic CRS:  WGS 84 ## POINT (34.79844 31.24329) ## POINT (34.81283 31.26028) ## POINT (35.01163 31.06862)\nThe printout demostrates that other than the geometries themselves, and the derived properties of type, dimensionality, and bounding box, the geometry column contains the additional piece of information on the CRS.\n\n\n12.3.4 Layer (sf)\nA geometry column (sfc) can be combined with non-spatial columns, also known as attributes, resulting in a layer (sf) object. In our case, the three points in the sfc geometry column geom (Section 7.2.3) represent the location of the three railway stations in Beer-Sheva and Dimona. Let's create a data.frame with several non-spatial properties of the stations, which we already worked with in Section 4.1.2, using function data.frame (Section 4.1.2):\nname = c(\"Beer-Sheva Center\", \"Beer-Sheva University\", \"Dimona\") city = c(\"Beer-Sheva\", \"Beer-Sheva\", \"Dimona\") lines = c(4, 5, 1) piano = c(FALSE, TRUE, FALSE) dat = data.frame(name, city, lines, piano) dat ##                    name       city lines piano ## 1     Beer-Sheva Center Beer-Sheva     4 FALSE ## 2 Beer-Sheva University Beer-Sheva     5  TRUE ## 3                Dimona     Dimona     1 FALSE\nNote that the order of rows in the attribute table must match the order of the geometries!\ndat ##                    name       city lines piano ## 1     Beer-Sheva Center Beer-Sheva     4 FALSE ## 2 Beer-Sheva University Beer-Sheva     5  TRUE ## 3                Dimona     Dimona     1 FALSE\nNow, we can combine the attribute table dat (data.frame) and the geometry column geom (sfc). This is done using function st_sf, resulting in a layer (sf):\nlayer = st_sf(dat, geom)\nlayer ## Simple feature collection with 3 features and 4 fields ## Geometry type: POINT ## Dimension:     XY ## Bounding box:  xmin: 34.79844 ymin: 31.06862 xmax: 35.01164 ymax: 31.26028 ## Geodetic CRS:  WGS 84 ##                    name       city lines piano                      geom ## 1     Beer-Sheva Center Beer-Sheva     4 FALSE POINT (34.79844 31.24329) ## 2 Beer-Sheva University Beer-Sheva     5  TRUE POINT (34.81283 31.26028) ## 3                Dimona     Dimona     1 FALSE POINT (35.01163 31.06862)\n\n\n12.3.5 Interactive mapping with mapview\nFunction mapview—which we are familiar with from Section 5.3.7.2—is useful for inspecting vector layers too. For example:\nlibrary(mapview) mapview(layer)\n+−\nlayer\n5 km\n3 mi\nlayer\nLeaflet | © OpenStreetMap contributors © CARTO"
  },
  {
    "objectID": "ch7.html#extracting-layer-components",
    "href": "ch7.html#extracting-layer-components",
    "title": "12  Vector layers",
    "section": "12.4 Extracting layer components",
    "text": "12.4 Extracting layer components\nIn Section 7.2 we:\n\nStarted from raw coordinates\nConvered them to geometry objects (sfg) using a function such as st_point, st_polygon, etc. (Section 7.2.2)\nCombined the geometries to a geometry column (sfc) using function st_sfc (Section 7.2.3)\nAdded attributes to the geometry column to get a layer (sf) using function st_sf (Section 7.2.4)\n\nwhich can be summarized as: coordinates → sfg → sfc → sf.\nSometimes we are interested in the opposite \"direction.\" In other words, we sometimes need to extract the simpler components (geometry, attributes, coordinates) from an existing layer:\n\nsf → geometry column (sfc)\nsf → attribute table (data.frame)\nsf → coordinates (matrix)\n\nThe geometry column (sfc) component can be extracted from an sf layer object using function st_geometry:\nst_geometry(layer) ## Geometry set for 3 features  ## Geometry type: POINT ## Dimension:     XY ## Bounding box:  xmin: 34.79844 ymin: 31.06862 xmax: 35.01164 ymax: 31.26028 ## Geodetic CRS:  WGS 84 ## POINT (34.79844 31.24329) ## POINT (34.81283 31.26028) ## POINT (35.01163 31.06862)\nThe non-spatial columns of an sf layer, i.e., the attribute table, can be extracted from an sf object using function st_drop_geometry:\nst_drop_geometry(layer) ##                    name       city lines piano ## 1     Beer-Sheva Center Beer-Sheva     4 FALSE ## 2 Beer-Sheva University Beer-Sheva     5  TRUE ## 3                Dimona     Dimona     1 FALSE\nThe latter is analogous to opening an attribute table of a vector layer in GIS software, such as ArcGIS (Figure 7.12).\nFigure 7.12: Attribute table in ArcGIS\nThe coordinates of sf, sfc or sfg objects can be obtained with function st_coordinates. The coordinates are returned as a matrix:\nst_coordinates(layer) ##          X        Y ## 1 34.79844 31.24329 ## 2 34.81283 31.26028 ## 3 35.01163 31.06862\nIn the case of a two-dimensional POINT layer, which is the most common use case of st_coordinates, the returned matrix has two columns X and Y as shown above. (With other types of geometries, the matrix has additional columns containing the information on grouping of point coordinates into shapes.)"
  },
  {
    "objectID": "ch7.html#creating-point-layer-from-table",
    "href": "ch7.html#creating-point-layer-from-table",
    "title": "12  Vector layers",
    "section": "12.5 Creating point layer from table",
    "text": "12.5 Creating point layer from table\nA common way of creating a point layer is to transform a table which has X and Y coordinate columns. Function st_as_sf can transform a table (data.frame) into a point layer (sf). In st_as_sf we specify:\n\nx—The data.frame to be converted\ncoords—Columns names with the coordinates (X, Y)\ncrs—The CRS (NA if left unspecified)\n\nLet's take the rainfall.csv table as an example. This table contains UTM 36N (EPSG: 32636) coordinates in the columns named x_utm and y_utm:\nrainfall = read.csv(\"rainfall.csv\")\nhead(rainfall) ##      num altitude sep oct nov dec jan feb mar apr may              name ## 1 110050       30 1.2  33  90 117 135 102  61  20 6.7 Kfar Rosh Hanikra ## 2 110351       35 2.3  34  86 121 144 106  62  23 4.5              Saar ## 3 110502       20 2.7  29  89 131 158 109  62  24 3.8             Evron ## 4 111001       10 2.9  32  91 137 152 113  61  21 4.8       Kfar Masrik ## 5 111650       25 1.0  27  78 128 136 108  59  21 4.7     Kfar Hamakabi ## 6 120202        5 1.5  27  80 127 136  95  49  19 2.7        Haifa Port ##      x_utm   y_utm ## 1 696533.1 3660837 ## 2 697119.1 3656748 ## 3 696509.3 3652434 ## 4 696541.7 3641332 ## 5 697875.3 3630156 ## 6 687006.2 3633330\nThe table can be converted to an sf layer using st_as_sf as follows:\nrainfall = st_as_sf(rainfall, coords = c(\"x_utm\", \"y_utm\"), crs = 32636)\nNote:\n\nThe order of coords column names corresponds to X-Y!\n32636 is the EPSG code of the UTM 36N projection (Table 7.4)\n\nThe analogous operation in ArcGIS is the Display XY Data menu (Figures 7.13–7.15).\nFigure 7.13: Displaying XY data from CSV in ArcGIS (Step 1)\nFigure 7.14: Displaying XY data from CSV in ArcGIS (Step 2)\nFigure 7.15: Displaying XY data from CSV in ArcGIS (Step 3)\nHere is the resulting sf layer:\nrainfall ## Simple feature collection with 169 features and 12 fields ## Geometry type: POINT ## Dimension:     XY ## Bounding box:  xmin: 629301.4 ymin: 3270290 xmax: 761589.2 ymax: 3681163 ## Projected CRS: WGS 84 / UTM zone 36N ## First 10 features: ##       num altitude sep oct nov dec jan feb mar apr may              name ## 1  110050       30 1.2  33  90 117 135 102  61  20 6.7 Kfar Rosh Hanikra ## 2  110351       35 2.3  34  86 121 144 106  62  23 4.5              Saar ## 3  110502       20 2.7  29  89 131 158 109  62  24 3.8             Evron ## 4  111001       10 2.9  32  91 137 152 113  61  21 4.8       Kfar Masrik ## 5  111650       25 1.0  27  78 128 136 108  59  21 4.7     Kfar Hamakabi ## 6  120202        5 1.5  27  80 127 136  95  49  19 2.7        Haifa Port ## 7  120630      450 1.9  36  93 161 166 128  71  21 4.9  Haifa University ## 8  120750       30 1.6  31  91 163 170 146  76  22 4.9             Yagur ## 9  120870      210 1.1  32  93 147 147 109  61  16 4.3        Nir Etzyon ## 10 121051       20 1.8  32  85 147 142 102  56  13 4.5         En Carmel ##                    geometry ## 1  POINT (696533.1 3660837) ## 2  POINT (697119.1 3656748) ## 3  POINT (696509.3 3652434) ## 4  POINT (696541.7 3641332) ## 5  POINT (697875.3 3630156) ## 6  POINT (687006.2 3633330) ## 7  POINT (689553.7 3626282) ## 8  POINT (694694.5 3624388) ## 9  POINT (686489.5 3619716) ## 10 POINT (683148.4 3616846)\nAn interactive map, showing the spatial locations of the rainfall stations, can be created using mapview (Section 7.2.5). Here, we are using the additional zcol parameter to choose which attribute will be used for the color scale:\nmapview(rainfall, zcol = \"jan\")\n+−\nrainfall - jan\n50100150200\n100 km\n50 mi\nrainfall - jan\nLeaflet | © OpenStreetMap contributors © CARTO"
  },
  {
    "objectID": "ch7.html#sf-layer-properties",
    "href": "ch7.html#sf-layer-properties",
    "title": "12  Vector layers",
    "section": "12.6 sf layer properties",
    "text": "12.6 sf layer properties\n\n12.6.1 Dimensions\nAn sf layer is basically a special type of data.frame, where one of the columns is a geometry column. Therefore, many of the functions we learned when working with data.frame tables (Chapter 4) also work on sf layers.\nFor example, we can get the number of rows, or features, with nrow:\nnrow(rainfall) ## [1] 169\nand the number of columns (including the geometry column) with ncol:\nncol(rainfall) ## [1] 13\nor both with dim:\ndim(rainfall) ## [1] 169  13\n\nWhat is the result of st_geometry(rainfall)? st_drop_geometry(rainfall)?\n\n\n\n12.6.2 Spatial properties\nThe st_bbox function returns the bounding box coordinates, just like for stars objects (Section 5.3.8.3):\nst_bbox(rainfall) ##      xmin      ymin      xmax      ymax  ##  629301.4 3270290.2  761589.2 3681162.7\nThe st_crs function returns the Coordinate Reference System (CRS), also the same way as for stars objects (Section 5.3.8.3):\nst_crs(rainfall) ## Coordinate Reference System: ##   User input: EPSG:32636  ##   wkt: ## PROJCRS[\"WGS 84 / UTM zone 36N\", ##     BASEGEOGCRS[\"WGS 84\", ##         DATUM[\"World Geodetic System 1984\", ##             ELLIPSOID[\"WGS 84\",6378137,298.257223563, ##                 LENGTHUNIT[\"metre\",1]]], ##         PRIMEM[\"Greenwich\",0, ##             ANGLEUNIT[\"degree\",0.0174532925199433]], ##         ID[\"EPSG\",4326]], ##     CONVERSION[\"UTM zone 36N\", ##         METHOD[\"Transverse Mercator\", ##             ID[\"EPSG\",9807]], ##         PARAMETER[\"Latitude of natural origin\",0, ##             ANGLEUNIT[\"degree\",0.0174532925199433], ##             ID[\"EPSG\",8801]], ##         PARAMETER[\"Longitude of natural origin\",33, ##             ANGLEUNIT[\"degree\",0.0174532925199433], ##             ID[\"EPSG\",8802]], ##         PARAMETER[\"Scale factor at natural origin\",0.9996, ##             SCALEUNIT[\"unity\",1], ##             ID[\"EPSG\",8805]], ##         PARAMETER[\"False easting\",500000, ##             LENGTHUNIT[\"metre\",1], ##             ID[\"EPSG\",8806]], ##         PARAMETER[\"False northing\",0, ##             LENGTHUNIT[\"metre\",1], ##             ID[\"EPSG\",8807]]], ##     CS[Cartesian,2], ##         AXIS[\"(E)\",east, ##             ORDER[1], ##             LENGTHUNIT[\"metre\",1]], ##         AXIS[\"(N)\",north, ##             ORDER[2], ##             LENGTHUNIT[\"metre\",1]], ##     USAGE[ ##         SCOPE[\"unknown\"], ##         AREA[\"World - N hemisphere - 30°E to 36°E - by country\"], ##         BBOX[0,30,84,36]], ##     ID[\"EPSG\",32636]]\n\nQuestion: what is the difference between the two plots in Figure 7.16, created using the following expressions?\n\nplot(st_geometry(rainfall)) plot(st_coordinates(rainfall)[, 1], st_coordinates(rainfall)[, 2])\nFigure 7.16: Two plots"
  },
  {
    "objectID": "ch7.html#subsetting-based-on-attributes",
    "href": "ch7.html#subsetting-based-on-attributes",
    "title": "12  Vector layers",
    "section": "12.7 Subsetting based on attributes",
    "text": "12.7 Subsetting based on attributes\nSubsetting of features in an sf vector layer is exactly the same as filtering rows in a data.frame (Section 4.1.5). Remember: an sf layer is a data.frame. For example, the following expressions subset the rainfall layer:\nrainfall[1:10, ] rainfall[rainfall$jan &gt; 100, ]\n\nWhich meteorological stations are being selected in each of these two expressions?\n\nFigure 7.17 shows the resulting subsets:\nplot(st_geometry(rainfall[1:10, ]), main = \"rainfall[1:10, ]\") plot(st_geometry(rainfall[rainfall$jan &gt; 100, ]), main = \"rainfall[rainfall$jan &gt; 100, ]\")\nFigure 7.17: Subsets of the rainfall layer\nSubsetting columns in sf is also similar to subsetting columns in a data.frame, with one exception: the geometry column. The geometry column \"sticks\" to the subset, by default, even if we do not explicitly select it, so that the resulting subset remains an sf object:\nrainfall[1:5, c(\"jan\", \"feb\")] ## Simple feature collection with 5 features and 2 fields ## Geometry type: POINT ## Dimension:     XY ## Bounding box:  xmin: 696509.3 ymin: 3630156 xmax: 697875.3 ymax: 3660837 ## Projected CRS: WGS 84 / UTM zone 36N ##   jan feb                 geometry ## 1 135 102 POINT (696533.1 3660837) ## 2 144 106 POINT (697119.1 3656748) ## 3 158 109 POINT (696509.3 3652434) ## 4 152 113 POINT (696541.7 3641332) ## 5 136 108 POINT (697875.3 3630156)\nIn case we do need to omit the geometry column and get a data.frame, we can apply st_drop_geometry (Section 7.3) on the subset:\nst_drop_geometry(rainfall[1:5, c(\"jan\", \"feb\")]) ##   jan feb ## 1 135 102 ## 2 144 106 ## 3 158 109 ## 4 152 113 ## 5 136 108"
  },
  {
    "objectID": "ch7.html#reading-vector-layers",
    "href": "ch7.html#reading-vector-layers",
    "title": "12  Vector layers",
    "section": "12.8 Reading vector layers",
    "text": "12.8 Reading vector layers\nIn addition to creating from raw coordinates (Section 7.2) and transforming a data.frame to point layer (Section 7.4), we often create vector layers by reading from a file or from a spatial database (Section 7.1.2)28. Reading a vector layer from a file or a database is done using the st_read function.\nFor example, the following expression reads the Shapefile of US county boundaries, named USA_2_GADM_fips.shp, from the course materials. In case the Shapefile is located in the working directory, we need to specify the name of just the .shp file, even though the Shapefile contains several other files (Table 7.1):\ncounty = st_read(\"USA_2_GADM_fips.shp\")\ncounty ## Simple feature collection with 3103 features and 4 fields ## Geometry type: MULTIPOLYGON ## Dimension:     XY ## Bounding box:  xmin: -124.7628 ymin: 24.52042 xmax: -66.94889 ymax: 49.3833 ## Geodetic CRS:  WGS 84 ## First 10 features: ##         NAME_1     NAME_2 TYPE_2  FIPS                       geometry ## 1  Connecticut Litchfield County 09005 MULTIPOLYGON (((-73.05166 4... ## 2  Connecticut   Hartford County 09003 MULTIPOLYGON (((-72.50941 4... ## 3  Connecticut    Tolland County 09013 MULTIPOLYGON (((-72.13582 4... ## 4  Connecticut    Windham County 09015 MULTIPOLYGON (((-71.79794 4... ## 5   California   Siskiyou County 06093 MULTIPOLYGON (((-122.2873 4... ## 6   California  Del Norte County 06015 MULTIPOLYGON (((-124.2492 4... ## 7   California      Modoc County 06049 MULTIPOLYGON (((-120.0003 4... ## 8  Connecticut New London County 09011 MULTIPOLYGON (((-72.225 41.... ## 9  Connecticut  Fairfield County 09001 MULTIPOLYGON (((-73.65778 4... ## 10 Connecticut  Middlesex County 09007 MULTIPOLYGON (((-72.47361 4...\nLet's also read a GeoJSON file with the location of three particular airports in New Mexico:\nairports = st_read(\"airports.geojson\")\nairports ## Simple feature collection with 3 features and 1 field ## Geometry type: POINT ## Dimension:     XY ## Bounding box:  xmin: -106.7947 ymin: 35.04807 xmax: -106.0892 ymax: 35.61621 ## Geodetic CRS:  WGS 84 ##                        name                   geometry ## 1 Albuquerque International POINT (-106.6169 35.04807) ## 2           Double Eagle II POINT (-106.7947 35.15559) ## 3        Santa Fe Municipal POINT (-106.0892 35.61621)"
  },
  {
    "objectID": "ch7.html#basic-plotting",
    "href": "ch7.html#basic-plotting",
    "title": "12  Vector layers",
    "section": "12.9 Basic plotting",
    "text": "12.9 Basic plotting\nWhen plotting an sf object with plot, we get multiple small maps—one map for each attribute. This can be useful to quickly examine the types of spatial variation in our data. For example (Figure 7.18):\nplot(county)\nFigure 7.18: Plot of sf object\nPlotting a single attribute adds a legend (Figure 7.19). The key.width and key.pos let us control the amount of space the legend takes and its placement, respectively:\nplot(county[, \"TYPE_2\"], key.width = lcm(5), key.pos = 4)\nFigure 7.19: Plot of sf object, single attribute with legend\nPlotting an sfc or an sfg object shows just the geometry (Figure 7.20):\nplot(st_geometry(county))\nFigure 7.20: Plot of sfc object\nWe can use graphical parameters to control the appearance of plotted geometries, such as:\n\ncol—Fill color\nborder—Outline color\npch—Point shape\ncex—Point size\n\nFor example, the following expression draws county borders in grey (Figure 7.21):\nplot(st_geometry(county), border = \"grey\")\nFigure 7.21: Basic plot of sfc object\nAdditional vector layers can be drawn in an existing graphical window using add=TRUE, similarly to the concept of layers in GIS software. For example, the following expressions draw both county and airports geometries (Figure 7.22). Note how the second expression uses add=TRUE:\nplot(st_geometry(county), border = \"grey\") plot(st_geometry(airports), col = \"red\", add = TRUE)\nFigure 7.22: Using add=TRUE in plot\nWe can also use add=TRUE to combine sfg or sfc geometries with rasters in the same plot. For example, let's plot the rainfall layer on top of the rainfall.tif raster, which is an interpolated rainfall surface we met in Chapter 1. First we will read the raster file:\nlibrary(stars) r = read_stars(\"rainfall.tif\") names(r) = \"rainfall (mm)\"\nand then plot both layers (Figure 7.23):\nplot(r, breaks = \"equal\", col = hcl.colors(11, \"Spectral\"), reset = FALSE) plot(st_geometry(rainfall), add = TRUE)\nFigure 7.23: sfc layer on top of a raster\nNote that we need to use the additional argument reset=FALSE whenever we are adding more layers to a stars raster plot."
  },
  {
    "objectID": "ch7.html#coordinate-reference-systems-crs",
    "href": "ch7.html#coordinate-reference-systems-crs",
    "title": "12  Vector layers",
    "section": "12.10 Coordinate Reference Systems (CRS)",
    "text": "12.10 Coordinate Reference Systems (CRS)\n\n12.10.1 What are CRS?\nA Coordinate Reference System (CRS) defines how the coordinates in our geometries relate to the surface of the Earth. There are two main types of CRS:\n\nGeographic—longitude and latitude, in degrees\nProjected—implying flat surface, usually in units of true distance (e.g., meters)\n\nFor example, Figure 7.24 shows the same polygonal layer (U.S. counties) in two different projection. On the left, the county layer is in the WGS84 geographic projection. Indeed, we can see that the axes are given in degrees of longitude and latitude. For example, we can see how the nothern border of U.S. follows the 49° latitude line. On the right, the same layer is displayed in the US National Atlas projection, where units are arbitrary but reflect true distance (meters). For example, the distance between every two consecutive grid lines is 1,000,000 meters or 1,000 kilometers.\nFigure 7.24: US counties in WGS84 and US Atlas projections\n\n\n12.10.2 Vector layer reprojection\nReprojection is the transformation of geometry coordinates, from one CRS to another. It is an important part of spatial analysis workflow, since we often need to:\n\nTransform several layers into the same projection, so that they can be displayed one on top of the other (e.g., Figure 7.22) or so that they can be subject to a spatial operator (e.g., Figure 7.7)\nSwitch between geographic and projected CRS\n\nA vector layer can be reprojected with st_transform. The st_transform function has two important parameters:\n\nx—The layer to be reprojected\ncrs—The target CRS\n\n\nWhy don't we need to specify the origin CRS in st_transform?\n\nAs mentioned above, the CRS can be specified in one of four ways, using an EPSG code, a PROJ4 string, a WKT string, or a crs object (Section 7.2.3). Where can we find EPSG codes or WKT definitions of different projections?\n\nWe can use CRS databases on the internet, such as http://spatialreference.org or http://epsg.io/, to look up CRS definitions for a given country, of CRS name, using the search box.\nWe can use the make_EPSG function from the rgdal package in R, which returns a data.frame of CRS definitions. The data.frame can be filtered to locate the CRS of interest. For example, the following expressions return the information on CRS where the description contains the word \"Israel.\"\n\nlibrary(rgdal) dat = make_EPSG() dat[grepl(\"Israel\", dat$note), ] ##       code                             note ## 86    4141                      Israel 1993 ## 1158  2039    Israel 1993 / Israeli TM Grid ## 2527 28193 Palestine 1923 / Israeli CS Grid ## 5458  6984                  Israeli Grid 05 ## 5459  6991               Israeli Grid 05/12 ##                                                                                                                                                       prj4 ## 86                                                                                                           +proj=longlat +ellps=GRS80 +no_defs +type=crs ## 1158      +proj=tmerc +lat_0=31.7343936111111 +lon_0=35.2045169444444 +k=1.0000067 +x_0=219529.584 +y_0=626907.39 +ellps=GRS80 +units=m +no_defs +type=crs ## 2527 +proj=cass +lat_0=31.7340969444444 +lon_0=35.2120805555556 +x_0=170251.555 +y_0=1126867.909 +a=6378300.789 +b=6356566.435 +units=m +no_defs +type=crs ## 5458      +proj=tmerc +lat_0=31.7343936111111 +lon_0=35.2045169444444 +k=1.0000067 +x_0=219529.584 +y_0=626907.39 +ellps=GRS80 +units=m +no_defs +type=crs ## 5459      +proj=tmerc +lat_0=31.7343936111111 +lon_0=35.2045169444444 +k=1.0000067 +x_0=219529.584 +y_0=626907.39 +ellps=GRS80 +units=m +no_defs +type=crs ##               prj_method ## 86                (null) ## 1158 Transverse Mercator ## 2527     Cassini-Soldner ## 5458 Transverse Mercator ## 5459 Transverse Mercator\nIn this book, we are going to encounter five different projections: WGS84, Sinusoidal, UTM 36N, ITM and US National Atlas (Table 7.4). We are going to specify them (e.g., in st_transform) using their EPSG codes, which is the easiest method.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 7.4: Projections used in this book\n\n\nName\nType\nArea\nUnits\nEPSG code\n\n\nWGS84\nGeographic\nWorld\ndegrees\n4326\n\n\nSinusoidal\nProjected\nWorld\nmeters\n-\n\n\nUTM 36N\nProjected\nIsrael\nmeters\n32636\n\n\nITM\nProjected\nIsrael\nmeters\n2039\n\n\nUS National Atlas\nProjected\nUSA\nmeters\n2163\n\n\n\nNow, let's see how we can use the st_transform function to repriject layers, in practice. The county and airports are currently in WGS84 (EPSG:4326) (How can we check?). Suppose that we would like to reproject those layers to the the US National Atlas Equal Area projection (EPSG:2163). The following expressions implement the reprojection:\ncounty = st_transform(county, 2163) airports = st_transform(airports, 2163)\nThe modified layers are shown in Figure 7.25. We can clearly see that the layer orientation has changes (also see Figure 7.24).\nplot(st_geometry(county), border = \"grey\") plot(st_geometry(airports), col = \"red\", add = TRUE)\nFigure 7.25: The county and airports layers in the US National Atlas Projection (EPSG=2163)\nExamining the layer coordinates also shows that, indeed, the coordinates have changed with reprojecton:\nst_coordinates(st_transform(airports, 4326))  # WGS84 coordinates ##           X        Y ## 1 -106.6169 35.04807 ## 2 -106.7947 35.15559 ## 3 -106.0892 35.61621\nst_coordinates(st_transform(airports, 2163))  # US National Atlas coordinates ##           X        Y ## 1 -603868.2 -1081605 ## 2 -619184.5 -1068431 ## 3 -551689.8 -1022366\n\nCreate a subset of county with the counties of New-Mexico, Arizona and Texas only, and plot the result (Figure 7.26).\n\nFigure 7.26: Subset of three states from the county layer"
  },
  {
    "objectID": "ch7.html#writing-vector-layers",
    "href": "ch7.html#writing-vector-layers",
    "title": "12  Vector layers",
    "section": "12.11 Writing vector layers",
    "text": "12.11 Writing vector layers\nWriting an sf object to a file can be done with st_write. Before writing the rainfall layer to disk, let's calculate a new column called annual with the annual rainfall (Section 4.5):\nm = c(\"sep\", \"oct\", \"nov\", \"dec\", \"jan\", \"feb\", \"mar\", \"apr\", \"may\") rainfall$annual = apply(st_drop_geometry(rainfall[, m]), 1, sum)\n\nIn the last expression, why did we use st_drop_geometry(rainfall[, m]) as the first argument in apply, instead of rainfall[, m]?\n\nThe sf object can be written to a Shapefile with st_write, as follows:\nst_write(rainfall, \"rainfall_pnt.shp\")\nThe format is automatically determined based on the .shp file extension. To overwrite an existing file, use delete_dsn=TRUE."
  },
  {
    "objectID": "ch8.html#aims",
    "href": "ch8.html#aims",
    "title": "13  Geometric operations with vector layers",
    "section": "13.1 Aims",
    "text": "13.1 Aims\nOur aims in this chapter are:\n\nLearn how to join data to a vector layer, by attributes or by location\nLearn to make geometric calculations with vector layers\n\nWe will use the following R packages:\n\nsf\nunits"
  },
  {
    "objectID": "ch8.html#join-by-location",
    "href": "ch8.html#join-by-location",
    "title": "13  Geometric operations with vector layers",
    "section": "13.2 Join by location",
    "text": "13.2 Join by location\nFor most of the examples in this Chapter, we are going to use the US counties and three airports layers (Section 7.7):\nlibrary(sf) airports = st_read(\"airports.geojson\") county = st_read(\"USA_2_GADM_fips.shp\")\ntransformed to the US National Atlas projection (Section 7.9.2):\nairports = st_transform(airports, 2163) county = st_transform(county, 2163)\nJoin by spatial location, or spatial join, is one of the most common operations in spatial analysis. In a spatial join we are \"attaching\" attributes from one layer to another, based on their spatial relations. In ArcGIS this is done using \"Join data from another layer based on spatial location\" Figures 8.1–8.2.\nFigure 8.1: Join by location in ArcGIS (Step 1)\nFigure 8.2: Join by location in ArcGIS (Step 2)\nFor simplicity, we will create a subset of the counties of New Mexico only, since all three airports are located in that state:\nnm = county[county$NAME_1 == \"New Mexico\", ]\nFigure 8.3 shows the geometry of the nm and airports layers. Note that the pch parameter determines point shape (see ?points), while cex determines point size (Section 7.8):\nplot(st_geometry(nm)) plot(st_geometry(airports), col = \"red\", pch = 16, cex = 2, add = TRUE)\nFigure 8.3: The nm and airports layers\nGiven two layers x and y, a function call of the form st_join(x, y) returns the x layer along with matching attributes from y. The default join type is by intersection, meaning that two geometries are considered matching if they intersect. Other join types can be specified using the join parameter (see ?st_join).\nFor example, the following expression joins the airports layer with the matching nm attributes. The resulting layer contains all three airports with their original geometry and name attribute, plus the NAME_1, NAME_2, TYPE_2 and FIPS of the nm feature that each airport intersects with:\nst_join(airports, nm) ## Simple feature collection with 3 features and 5 fields ## Geometry type: POINT ## Dimension:     XY ## Bounding box:  xmin: -619184.5 ymin: -1081605 xmax: -551689.8 ymax: -1022366 ## Projected CRS: US National Atlas Equal Area ##                        name     NAME_1     NAME_2 TYPE_2  FIPS ## 1 Albuquerque International New Mexico Bernalillo County 35001 ## 2           Double Eagle II New Mexico Bernalillo County 35001 ## 3        Santa Fe Municipal New Mexico   Santa Fe County 35049 ##                     geometry ## 1 POINT (-603868.2 -1081605) ## 2 POINT (-619184.5 -1068431) ## 3 POINT (-551689.8 -1022366)\n\nWhat do you think will happen if we reverse the order of the arguments, as in st_join(nm, airports)? How many features is the result going to have, and why?"
  },
  {
    "objectID": "ch8.html#subsetting-by-location",
    "href": "ch8.html#subsetting-by-location",
    "title": "13  Geometric operations with vector layers",
    "section": "13.3 Subsetting by location",
    "text": "13.3 Subsetting by location\nIn Section 7.6 we learned how to subset sf vector layers based on their attributes, just like an ordinary data.frame (Section 4.1.5). We can also subset an sf layer based on its intersection with another layer. Subsetting by location uses the same [ operator, only using a layer as the \"index.\" Namely, an expression of the form x[y, ]—where x and y are vector layers—returns a subset of x features that intersect with y.\nFor example, the following expression returns only those nm counties that intersect with the airports layer:\nnm1 = nm[airports, ]\nThe subset is shown in light blue in Figure 8.4:\nplot(st_geometry(nm)) plot(st_geometry(nm1), col = \"lightblue\", add = TRUE) plot(st_geometry(airports), col = \"red\", pch = 16, cex = 2, add = TRUE)\nFigure 8.4: Subset of the nm layer based on intersection with airports\n\nWhat will be the result of airports[nm, ]?\n\n\nWhat will be the result of nm[nm[20, ], ]?"
  },
  {
    "objectID": "ch8.html#geometric-calculations",
    "href": "ch8.html#geometric-calculations",
    "title": "13  Geometric operations with vector layers",
    "section": "13.4 Geometric calculations",
    "text": "13.4 Geometric calculations\n\n13.4.1 Introduction\nGeometric operations on vector layers can conceptually be divided into three groups, according to their output:\n\nNumeric values (Section 8.3.2)—Functions that summarize geometrical properties of:\n\nA single layer—e.g., area, length (Section 8.3.2.2)\nA pair of layers—e.g., distance (Section 8.3.2.3)\n\nLogical values (Section 8.3.3)—Functions that evaluate whether a certain condition holds true, regarding:\n\nA single layer—e.g., geometry is valid\nA pair of layers—e.g., feature A intersects feature B (Section 8.3.3)\n\nSpatial layers (Section 8.3.4)—Functions that create a new layer based on:\n\nA single layer—e.g., centroid, buffer (Sections 8.3.4.2, 8.3.4.4, 8.3.4.5, 8.3.4.7)\nA pair of layers—e.g., intersection area (Section 8.3.4.6)\n\n\nThe following three sections elaborate and give examples of numeric (Section 8.3.2), logical (Section 8.3.3) and spatial (Section 8.3.4) geometric calculations on vector layers.\n\n\n13.4.2 Numeric geometric calculations\n\n13.4.2.1 Introduction\nThere are several functions to calculate numeric geometric properties of vector layers in package sf:\n\nst_bbox—Section 7.5.2\nst_area—Section 8.3.2.2\nst_distance—Section 8.3.2.3\nst_length—Section 11.4.6\nst_dimension\n\nFunctions st_bbox, st_area, st_length and st_dimension are operation applicable for individual geometries. The st_distance function is applicable to pairs of geometries. The next two sections give an example from each category: st_area for individual geometries (Section 8.3.2.2) and st_distance for pairs of geometries (Section 8.3.2.3).\n\n\n13.4.2.2 Area\nThe st_area function returns the area of each feature or geometry. For example, we can use st_area to calculate the area of each feature in the county layer (i.e., each county):\ncounty$area = st_area(county) county$area[1:6] ## Units: [m^2] ## [1]  2451875694  1941109814  1077788608  1350475635 16416571545  2626706989\nThe result is an object of class units, which is basically a numeric vector associated with the units of measurement:\nclass(county$area) ## [1] \"units\"\nCRS units (e.g., meters) are used by default in length, area and distance calculations. For example, the units of measurement in the US National Atlas CRS are meters, which is why the area values are returned in m2�2 units.\nWe can convert units objects to different units, with set_units from package units29. Note that the units package needs to be loaded to do that. For example, here is how we can convert the area column from m2�2 to km2��2:\nlibrary(units) county$area = set_units(county$area, \"km^2\") county$area[1:6] ## Units: [km^2] ## [1]  2451.876  1941.110  1077.789  1350.476 16416.572  2626.707\nDealing with units, rather than numeric values has several advantages:\n\nWe don't need to remember the coefficients for conversion between one unit to another, avoiding conversion mistakes.\nWe don't need to remember, or somehow encode, the units of measurement (e.g., rename the area column to area_m2) since they are already binded with the values.\nWe can keep track of the units when making numeric calculations. For example, if we divide a numeric value by km2��2, the result is going to be associated with units of density, km−2��−2 (Section 8.5).\n\nNevertheless, if necessary, we can always convert units to an ordinary numeric vector with as.numeric:\nas.numeric(county$area[1:6]) ## [1]  2451.876  1941.110  1077.789  1350.476 16416.572  2626.707\nThe calculated county area attribute values are visualized in Figure 8.5:\nplot(county[, \"area\"])\nFigure 8.5: Calculated area attribute\n\n\n13.4.2.3 Distance\nAn example of a numeric operator on a pair of geometries is geographical distance. Distances can be calculated using function st_distance. For example, the following expression calculates the distance between each feature in airports and each feature in nm:\nd = st_distance(airports, nm)\nThe result is a matrix of units values:\nd[, 1:6] ## Units: [m] ##          [,1]     [,2]      [,3]      [,4]      [,5]      [,6] ## [1,] 266778.9 140470.5 103328.22 140913.62 175353.34 121934.94 ## [2,] 275925.0 120879.9  93753.25 141511.80 177624.40 125327.45 ## [3,] 197540.4 145580.9  34956.53  62231.01  96549.17  43630.68\nIn the distance matrix resulting from st_distance(x,y):\n\nRows refer to features of x, e.g., airports\nColumns refer to features of y, e.g., nm\n\nTherefore, the matrix dimensions are equal to c(nrow(x), nrow(y)):\nc(nrow(airports), nrow(nm)) ## [1]  3 33\ndim(d) ## [1]  3 33\nJust like areas (Section 8.3.2.2), distances can be converted to different units with set_units. For example, the following expression converts the distance matrix d from m� to km��:\nd = set_units(d, \"km\") d[, 1:6] ## Units: [km] ##          [,1]     [,2]      [,3]      [,4]      [,5]      [,6] ## [1,] 266.7789 140.4705 103.32822 140.91362 175.35334 121.93494 ## [2,] 275.9250 120.8799  93.75325 141.51180 177.62440 125.32745 ## [3,] 197.5404 145.5809  34.95653  62.23101  96.54917  43.63068\nTo work with the distance matrix, it can be convenient to set row and column names. We can use any of the unique attributes of x and y, such as airport names and county names:\nrownames(d) = airports$name colnames(d) = nm$NAME_2 d[, 1:6] ## Units: [km] ##                              Union San Juan Rio Arriba      Taos    Colfax ## Albuquerque International 266.7789 140.4705  103.32822 140.91362 175.35334 ## Double Eagle II           275.9250 120.8799   93.75325 141.51180 177.62440 ## Santa Fe Municipal        197.5404 145.5809   34.95653  62.23101  96.54917 ##                                Mora ## Albuquerque International 121.93494 ## Double Eagle II           125.32745 ## Santa Fe Municipal         43.63068\nOnce row and column names are set, it is more convenient to find out the distance between a specific airport and a specific county:\nd[\"Santa Fe Municipal\", \"Santa Fe\", drop = FALSE] ## Units: [km] ##                    Santa Fe ## Santa Fe Municipal        0\n\n\n\n13.4.3 Logical geometric calculations\nGiven two sets of geometries, or layers, x and y, the following logical geometric functions check whether each of the geometries in x maintains the specified relation with each of the geometries in y:\n\nst_contains_properly\nst_contains\nst_covered_by\nst_covers\nst_crosses\nst_disjoint\nst_equals_exact\nst_equals\nst_intersects\nst_is_within_distance\nst_overlaps\nst_touches\nst_within\n\nIn this book we mostly use the first function in the above list, st_intersects. The st_intersects function detects intersection, which is the simplest and most useful type of relation. Two geometries intersect when they share at least one point in common. However, it is important to be aware of the many other relations that can be evaluated using the other 12 functions.\nBy default, the above functions return a list, which takes less memory but is less convenient to work with. When specifying sparse=FALSE the functions return a logical matrix which is more convenient to work with. Each element i,j in the matrix is TRUE when f(x[i], y[j]) is TRUE. For example, the following expression creates a matrix of intersection relations between counties in nm:\nint = st_intersects(nm, nm, sparse = FALSE) int[1:4, 1:4] ##       [,1]  [,2]  [,3]  [,4] ## [1,]  TRUE FALSE FALSE FALSE ## [2,] FALSE  TRUE  TRUE FALSE ## [3,] FALSE  TRUE  TRUE  TRUE ## [4,] FALSE FALSE  TRUE  TRUE\nFigure 8.6 shows two matrices of spatial relations between counties in New Mexico: st_intersects (left) and st_touches (right).\nFigure 8.6: Spatial relations between counties in New Mexico, st_intersects (left) and st_touches (right)\n\nHow can we calculate airports count per county in nm, using st_intersects (Figure 8.7)?\n\nFigure 8.7: Airport count per county in New Mexico\n\n\n13.4.4 Spatial geometric calculations\n\n13.4.4.1 Introduction\nThe sf package provides common geometry-generating functions applicable to individual geometries, such as:\n\nst_centroid—Section 8.3.4.2\nst_buffer—Section 8.3.4.5\nst_convex_hull—Section 8.3.4.7\nst_voronoi\nst_sample\n\nWhat each function does is illustrated in Figure 8.8.\nFigure 8.8: Geometry-generating operations on individual layers. The input geometries are shown in red, the output geometries are shown in black.\nAdditionally, the st_cast function is used to transform geometries from one type to another (Figure 7.6), which can also be considred as geometry generation (Section 8.3.4.4). The st_union and st_combine functions are used to combine multiple geometries into one, possibly dissolving their borders (Section 8.3.4.3).\nFinally, geometry-generating functions which operate on pairs of geometries (Section 8.3.4.6) include:\n\nst_intersection\nst_difference\nst_sym_difference\nst_union\n\nWe elaborate on these functions in the following Sections 8.3.4.2–8.3.4.7.\n\n\n13.4.4.2 Centroids\nA centroid is the geometric center of a geometry, i.e., its center of mass, represented as a point. For example, the following expression uses st_centroid to create the centroid of each polygon in county:\nctr = st_centroid(nm)\nThe nm layer along with the calculated centroids is shown in Figure 8.9:\nplot(st_geometry(nm), border = \"grey\") plot(st_geometry(ctr), col = \"red\", pch = 3, add = TRUE)\nFigure 8.9: Centroids of New Mexico counties\n\n\n13.4.4.3 Combining and dissolving\nThe st_combine and st_union functions can be used to combine multiple geometries into a single one. The difference is that st_combine only combines the geometries into one, just like the c function combines sfg objects into one sfg (Section 7.2.2), while st_union also dissolves internal borders.\nTo understand the difference between st_combine and st_union, let's examine the result of applying each of them on nm:\nst_combine(nm) ## Geometry set for 1 feature  ## Geometry type: MULTIPOLYGON ## Dimension:     XY ## Bounding box:  xmin: -863763.9 ymin: -1478601 xmax: -267074.1 ymax: -845634.3 ## Projected CRS: US National Atlas Equal Area ## MULTIPOLYGON (((-267074.1 -884175.1, -267309.2 ...\nst_union(nm) ## Geometry set for 1 feature  ## Geometry type: POLYGON ## Dimension:     XY ## Bounding box:  xmin: -863763.9 ymin: -1478601 xmax: -267074.1 ymax: -845634.3 ## Projected CRS: US National Atlas Equal Area ## POLYGON ((-852411.9 -1350402, -850603.1 -133127...\nThe results are plotted in Figure 8.10:\nplot(st_combine(nm), main = \"st_combine(nm)\") plot(st_union(nm), main = \"st_union(nm)\")\nFigure 8.10: Combining (st_combine) and dissolving (st_union) polygons\n\nWhat is the class of the resulting objects? Why? What happened to the original nm attributes?\n\n\nWhy did st_combine return a MULTIPOLYGON, while st_union returned a POLYGON?\n\n\nWhy do you think there are internal \"patches\" in the result of st_union(nm) (Figure 8.10)30?\n\nLet's solve a geometric question to practice some of the functions we learned so far. What is the distance between the centroids of California and New Jersey, in kilometers? Using [, st_union, st_centroid and st_distance, we can calculate the distance as follows:\n# Subsetting ca = county[county$NAME_1 == \"California\", ] nj = county[county$NAME_1 == \"New Jersey\", ]  # Union ca = st_union(ca) nj = st_union(nj)  # Calculating centroids ca_ctr = st_centroid(ca) nj_ctr = st_centroid(nj)  # Calculating distance d = st_distance(ca_ctr, nj_ctr)  # Converting to km set_units(d, \"km\") ## Units: [km] ##         [,1] ## [1,] 3846.64\nNote that we are using st_union to combine the county polygons into a single geometry, so that we can calculate the centroid of the state polygon, as a whole.\nTo plot the centroids more conveniently, we can use the c function to combine two sfc (geometry columns) into one, long, geometry column. Recall that this is unlike what the c function does when given sfg (geometries), in which case the geometries are combined to one (multi- or collection) geometry (Section 7.2.2).\np = c(ca_ctr, nj_ctr) p ## Geometry set for 2 features  ## Geometry type: POINT ## Dimension:     XY ## Bounding box:  xmin: -1707694 ymin: -667480.7 xmax: 2111204 ymax: -206332.8 ## Projected CRS: US National Atlas Equal Area ## POINT (-1707694 -667480.7) ## POINT (2111204 -206332.8)\nThe centroids we calculated can be displayed as follows (Figure 8.11):\nplot(st_geometry(county), border = \"grey\") plot(p, col = \"red\", pch = 16, cex = 2, add = TRUE)\nFigure 8.11: California and New Jersey centroids\nHow can we draw a line line between the centroids, corresponding to the distance we just calculated (Figure 8.12)?\nFirst, we can use st_combine to transform the points into a single MULTIPOINT geometry. Remember, the st_combine function is similar to st_union, but only combines and does not dissolve.\np = st_combine(p) p ## Geometry set for 1 feature  ## Geometry type: MULTIPOINT ## Dimension:     XY ## Bounding box:  xmin: -1707694 ymin: -667480.7 xmax: 2111204 ymax: -206332.8 ## Projected CRS: US National Atlas Equal Area ## MULTIPOINT ((-1707694 -667480.7), (2111204 -206...\n\nDo you think the results of st_union(p) and st_combine(p) will be different, and if so in what way?\n\nWhat is left to be done is to transform the MULTIPOINT geometry to a LINESTRING geometry, which is what we learn next (Section 8.3.4.4).\n\n\n13.4.4.4 Geometry casting\nSometimes we need to convert a given geometry to a different type, to express different aspects of the geometries in our analysis. For example, we would have to convert the county MULTIPOLYGON geometries to MULTILINESTRING geometries to calculate the perimeter of each county, using st_length (Section 8.3.2).\nThe st_cast function can be used to convert between different geometry types. The st_cast function accepts:\n\nThe input sfg, sfc or sf\nThe target geometry type\n\nFor example, we can use st_cast to convert our MULTIPOINT geometry p (Figure 8.11), which we calculated in Section 8.3.4.3, to a LINESTRING geometry l:\nl = st_cast(p, \"LINESTRING\") l ## Geometry set for 1 feature  ## Geometry type: LINESTRING ## Dimension:     XY ## Bounding box:  xmin: -1707694 ymin: -667480.7 xmax: 2111204 ymax: -206332.8 ## Projected CRS: US National Atlas Equal Area ## LINESTRING (-1707694 -667480.7, 2111204 -206332.8)\nEach MULTIPOINT geometry is replaced with a LINESTRING geometry, comprising a line that goes through all points from first to last. In this case, p contains just one MULTIPOINT geometry that contains two points. Therefore, the resulting geometry column l contains one LINESTRING geometry with a straight line.\nThe line we calculated is shown in Figure 8.12:\nplot(st_geometry(county), border = \"grey\") plot(st_geometry(ca_ctr), col = \"red\", pch = 16, cex = 2, add = TRUE) plot(st_geometry(nj_ctr), col = \"red\", pch = 16, cex = 2, add = TRUE) plot(l, col = \"red\", add = TRUE)\nFigure 8.12: California and New Jersey centroids, with a line\nIt is worth mentioning that not every conversion operation between geometry types is possible. Meaningless st_cast operations will fail with an error message. For example, a LINESTRING composed of just two points cannot be converted to a POLYGON:\nst_cast(l, \"POLYGON\") ## Error in FUN(X[[i]], ...): polygons require at least 4 points\n\n\n13.4.4.5 Buffers\nAnother example of a function that generates new geometries based on individual input geometries is st_buffer. The st_buffer function calculates buffers, polygons encompassing all point up to the specified distance from a given geometry.\nFor example, here is how we can calculate 100 km�� buffers around the airports. Note that the st_buffer requires buffer distance (dist), which can be passed as numeric, in which case CRS units are assumed, or as units, in which case we can use any distance unit we wish. The following two expressions are equivalent, in both cases resulting in a layer of 100 km�� buffers named airports_100:\n# Buffer distance as 'numeric' airports_100 = st_buffer(airports, dist = 100 * 10^3)\n# Buffer distance as 'units' airports_100 = st_buffer(airports, dist = set_units(100, \"km\"))\nThe airports_100 layer with the airport buffers is shown in Figure 8.13:\nplot(st_geometry(nm)) plot(st_geometry(airports_100), add = TRUE)\nFigure 8.13: airports buffered by 100 km\n\n\n13.4.4.6 Geometry generation from pairs\nIn addition to geometry-generating functions that operate on individual geometries, such as st_centroid (Section 8.3.4.2) and st_buffer (Section 8.3.4.5), there are four geometry-generating functions that operate on pairs of input geometries. These are:\n\nst_intersection\nst_difference\nst_sym_difference\nst_union\n\nFigure 8.14 shows what each of these four functions does. Note that st_intersection, st_sym_difference, st_union are symmetrical meaning that the order of the two inputs x and y does not matter. The st_difference function is not symmetrical: it returns the area of x, minus the intersection of x and y.\nFigure 8.14: Geometry-generating operations on pairs of layers\nFor example, suppose that we need to calculate the area that is within 100 km�� range of all three airports at the same time? We can find the area of intersection of the three airports, using st_intersection. Since st_intersection accepts two geometries at a time, this can be done in two steps:\ninter1 = st_intersection(airports_100[1, ], airports_100[2, ]) inter2 = st_intersection(inter1, airports_100[3, ])\nThe resulting polygon inter2 is shown, in blue, in Figure 8.15:\nplot(st_geometry(nm)) plot(st_geometry(airports_100), add = TRUE) plot(inter2, col = \"lightblue\", add = TRUE)\nFigure 8.15: The intersection of three airports buffers\nHow can we calculate the area that is within 100 km�� range of at least one of the three airports? We can use exactly the same technique, only replacing st_intersection with st_union, as follows:\nunion1 = st_union(airports_100[1, ], airports_100[2, ]) union2 = st_union(union1, airports_100[3, ])\nAs shown in Section 8.3.4.3, st_union can be applied on an individual layer, in which case it returns the (dissolved) union of all geometries. Therefore we can get to the same result with just one expression, as follows:\nunion2 = st_union(airports_100)\nThe resulting polygon union2 is shown, in blue, in Figure 8.16:\nplot(st_geometry(nm)) plot(union2, col = \"lightblue\", add = TRUE)\nFigure 8.16: The union of three airports buffers\n\n\n13.4.4.7 Convex hull\nThe Convex Hull of a set X� of points is the smallest convex polygon that contains X�. The convex hull may be visualized as the shape resulting when stretching a rubber band around X�, then releasing it until it shrinks to minimal size (Figure 8.17).\nFigure 8.17: Convex Hull: elastic-band analogy (https://en.wikipedia.org/wiki/Convex_hull)\nThe st_convex_hull function is used to calculate the convex hull of the given geometry or geometries. For example, the following expression calculates the convex hull (per feature) in nm1:\nh = st_convex_hull(nm1)\nThe resulting layer is shown in red in Figure 8.18:\nplot(st_geometry(nm1)) plot(st_geometry(h), add = TRUE, border = \"red\")\nFigure 8.18: Convex hull polygons for two counties in New Mexico\n\nHow can we calculate the convex hull of all polygons in nm1 (Figure 8.19)?\n\nFigure 8.19: Convex Hull of multiple polygons\nHere is another question to practice the geometric operations we learned in this chapter. Suppose we build a tunnel, 10 km�� wide, between the centroids of \"Harding\" and \"Sierra\" counties in New Mexico. Which counties does the tunnel go through? The following code gives the result. Take a moment to go over it and make sure you understand what happens in each step:\nw = nm[nm$NAME_2 %in% c(\"Harding\", \"Sierra\"), ] w_ctr = st_centroid(w) w_ctr_buf = st_buffer(w_ctr, dist = 5000) w_ctr_buf_u = st_combine(w_ctr_buf) w_ctr_buf_u_ch = st_convex_hull(w_ctr_buf_u) nm_w = nm[w_ctr_buf_u_ch, ] nm_w$NAME_2 ## [1] \"Harding\"    \"San Miguel\" \"Guadalupe\"  \"Torrance\"   \"Socorro\"    ## [6] \"Lincoln\"    \"Sierra\"\nFigure 8.20 shows the calculated tunnel geometry and highlights the counties that the tunnel goes through. The 4th expression uses the text function to draw text labels. The text function accepts the matrix of coordinates, where labels should be drawn (in this case, county centroid coordinates), and the corresponding character vector of lables. In this case, the labels are county names (nm_w$NAME_2). (Note that in case the labels are numeric, such as in Figure 8.7, then we must convert them to character using as.character for them to be displayed using the text function.)\nplot(st_geometry(nm_w), border = NA, col = \"lightblue\") plot(st_geometry(nm), add = TRUE) plot(st_geometry(w_ctr_buf_u_ch), add = TRUE) text(st_coordinates(st_centroid(nm_w)), nm_w$NAME_2)\nFigure 8.20: Tunnel between \"Sierra\" and \"Harding\" county centroids\nHow is the area of the \"tunnel\" divided between the various counties that it crosses? We can use st_intersection (Section 8.3.4.6) to calculate all intersections between the tunnel polygon w_ctr_buf_u_ch, and the county polygons nm_w. Since we already know that the tunnel (w_ctr_buf_u_ch) intersects with each of the seven polygons in nm_w, the result of the intersection is of the same length, and in the same order, as the corresponding geometries in nm_w:\nint = st_intersection(w_ctr_buf_u_ch, nm_w) int ## Geometry set for 7 features  ## Geometry type: POLYGON ## Dimension:     XY ## Bounding box:  xmin: -677075.1 ymin: -1293982 xmax: -340138.8 ymax: -1002634 ## Projected CRS: US National Atlas Equal Area ## First 5 geometries: ## POLYGON ((-359510.4 -1013406, -348484.4 -100391... ## POLYGON ((-430575.6 -1074562, -359510.4 -101340... ## POLYGON ((-484643 -1121091, -430575.6 -1074562,... ## POLYGON ((-545452.3 -1173421, -484643 -1121091,... ## POLYGON ((-638440.3 -1253443, -546193.4 -117405...\nTherefore, we can divide the area (Section 8.3.2.2) of each county intersection, by the total area of the tunnel, to get the proportion of tunnel area in each county:\narea = st_area(int) area ## Units: [m^2] ## [1]  199064581  872145769  813577715  702562404 1119738156  108077571  575538789\nprop = area / sum(area) prop ## Units: [1] ## [1] 0.04533773 0.19863456 0.18529546 0.16001130 0.25502469 0.02461508 0.13108118\nThe proportions can be displayed as text labels, using the text function (Figure 8.21):\nplot(st_geometry(nm_w), border = \"darkgrey\") plot(int, col = hcl.colors(7, \"Set3\"), border = \"grey\", add = TRUE) text(st_coordinates(st_centroid(int)), paste0(round(prop, 2)*100, \"%\"))\nFigure 8.21: Proportion of tunnel area within each county"
  },
  {
    "objectID": "ch8.html#vector-layer-aggregation",
    "href": "ch8.html#vector-layer-aggregation",
    "title": "13  Geometric operations with vector layers",
    "section": "13.5 Vector layer aggregation",
    "text": "13.5 Vector layer aggregation\nWe don't always want to dissolve all features into a single geometry (Section 8.3.4.3). Sometimes, we need to dissolve each group of geometries, whereas groups are specified by attribute or by location. To demonstrate dissolving by attribute, also known as aggregation, let's take a subset of county with the counties of just Arizona and Utah states:\ns = county[county$NAME_1 %in% c(\"Arizona\", \"Utah\"), ]\nThe selected counties are shown in Figure 8.22:\nplot(s[, \"NAME_1\"], key.width = lcm(3))\nFigure 8.22: Subset of two states from county\nAs shown previously (Section 8.3.4.3), we can dissolve all features into a single geometry with st_union:\ns1 = st_union(s) s1 ## Geometry set for 1 feature  ## Geometry type: POLYGON ## Dimension:     XY ## Bounding box:  xmin: -1389076 ymin: -1470784 xmax: -757960.3 ymax: -235145.7 ## Projected CRS: US National Atlas Equal Area ## POLYGON ((-852411.9 -1350402, -852611 -1352512,...\nThe dissolved geometry is shown in Figure 8.23:\nplot(s1)\nFigure 8.23: Union of all counties in s\nAggregating/dissolving by attributes can be done with aggregate31, which accepts the following arguments:\n\nx—The sf layer being dissolved\nby—A corresponding data.frame determining the groups\nFUN—The function to be applied on each attribute in x\n\nFor example, the following expression dissolves the layer s, summing the area attribute, by state:\ns2 = aggregate(s[, \"area\"], st_drop_geometry(s[, \"NAME_1\"]), sum) s2 ## Simple feature collection with 2 features and 2 fields ## Attribute-geometry relationship: 0 constant, 1 aggregate, 1 identity ## Geometry type: POLYGON ## Dimension:     XY ## Bounding box:  xmin: -1389076 ymin: -1470784 xmax: -757960.3 ymax: -235145.7 ## Projected CRS: US National Atlas Equal Area ##    NAME_1            area                       geometry ## 1 Arizona 295423.1 [km^2] POLYGON ((-852411.9 -135040... ## 2    Utah 219634.4 [km^2] POLYGON ((-1011317 -819521....\nThe result is shown in Figure 8.24:\nplot(s2[, \"area\"])\nFigure 8.24: Union by state name of s"
  },
  {
    "objectID": "ch8.html#join-by-attributes",
    "href": "ch8.html#join-by-attributes",
    "title": "13  Geometric operations with vector layers",
    "section": "13.6 Join by attributes",
    "text": "13.6 Join by attributes\nAttaching new attributes to a vector layer from a table, based on common attribute(s), is known as a non-spatial join, or join by attributes. Joining by attributes works exactly the same way as a join between two tables, e.g., using the merge function (Section 4.6). The difference is just that the \"left\" table, x, is an sf layer, instead of a data.frame. Using merge to join an sf layer with a data.frame is analogous to the \"Join attributes from a table\" workflow in ArcGIS (Figure 8.25).\nFigure 8.25: Join by attributes in ArcGIS\nIn the next example we will join county-level demographic data, from CO-EST2012-Alldata.csv, with the county layer. The join will be based on the common Federal Information Processing Standards (FIPS) code of each county.\nFirst, let's read the CO-EST2012-Alldata.csv file into a data.frame named dat. This file contains numerous demographic variables, on the county level, in the US:\ndat = read.csv(\"CO-EST2012-Alldata.csv\")\nNext, we subset the columns we are interested in, so that it is easier to display the table:\n\nSTATE—State code\nCOUNTY—County code\nCENSUS2010POP—Population size in 2010\n\nThe first two columns, STATE and COUNTY, contain the state and county components of the FIPS code, which we will use to join the data. The third column, CENSUS2010POP, contains our variable of interest, which we want to attach to the county layer.\ndat = dat[, c(\"STATE\", \"COUNTY\", \"CENSUS2010POP\")] head(dat) ##   STATE COUNTY CENSUS2010POP ## 1     1      0       4779736 ## 2     1      1         54571 ## 3     1      3        182265 ## 4     1      5         27457 ## 5     1      7         22915 ## 6     1      9         57322\nRecords where COUNTY code is 0 are state sums, i.e. sub-totals. We need to remove the state sums and keep just the county-level records:\ndat = dat[dat$COUNTY != 0, ] head(dat) ##   STATE COUNTY CENSUS2010POP ## 2     1      1         54571 ## 3     1      3        182265 ## 4     1      5         27457 ## 5     1      7         22915 ## 6     1      9         57322 ## 7     1     11         10914\nThe FIPS code in the county layer is a character value with five digits, where the first two digits are the state code and the last three digits are the county code. When necessary, leading zeros are added. For example, state code 9 and county code 5 is encoded as \"09005\".\ncounty$FIPS[1:6] ## [1] \"09005\" \"09003\" \"09013\" \"09015\" \"06093\" \"06015\"\nwhile the dat table contains separate state and county codes as numeric values:\ndat$STATE[1:10] ##  [1] 1 1 1 1 1 1 1 1 1 1\ndat$COUNTY[1:10] ##  [1]  1  3  5  7  9 11 13 15 17 19\nTo get the corresponding county FIPS codes in dat, we need to:\n\nStandardize the state code to a two-digit code\nStandardize the county code to three-digit code\nPaste the state code and the county code, to obtain the five-digit FIPS code\n\nThe formatC function can be used to format numeric values into different formats, using different \"scenarios.\" The \"add leading zeros\" scenario is specified using width=n, where n is the required number of digits, and flag=\"0\". Therefore:\ndat$STATE = formatC(dat$STATE, width = 2, flag = \"0\") dat$COUNTY = formatC(dat$COUNTY, width = 3, flag = \"0\") dat$FIPS = paste0(dat$STATE, dat$COUNTY)\nNow we have a column named FIPS, with FIPS codes in exactly the same format as in the county layer:\nhead(dat) ##   STATE COUNTY CENSUS2010POP  FIPS ## 2    01    001         54571 01001 ## 3    01    003        182265 01003 ## 4    01    005         27457 01005 ## 5    01    007         22915 01007 ## 6    01    009         57322 01009 ## 7    01    011         10914 01011\nThis means we can join the county layer with the dat table, based on the common column named FIPS:\ncounty = merge(county, dat[, c(\"FIPS\", \"CENSUS2010POP\")], by = \"FIPS\", all.x = TRUE) county ## Simple feature collection with 3103 features and 6 fields ## Geometry type: MULTIPOLYGON ## Dimension:     XY ## Bounding box:  xmin: -2031903 ymin: -2116922 xmax: 2516534 ymax: 732304.6 ## Projected CRS: US National Atlas Equal Area ## First 10 features: ##     FIPS  NAME_1   NAME_2 TYPE_2            area CENSUS2010POP ## 1  01001 Alabama  Autauga County 1562.805 [km^2]         54571 ## 2  01003 Alabama  Baldwin County 4247.589 [km^2]        182265 ## 3  01005 Alabama  Barbour County 2330.135 [km^2]         27457 ## 4  01007 Alabama     Bibb County 1621.356 [km^2]         22915 ## 5  01009 Alabama   Blount County 1692.125 [km^2]         57322 ## 6  01011 Alabama  Bullock County 1623.924 [km^2]         10914 ## 7  01013 Alabama   Butler County 2013.066 [km^2]         20947 ## 8  01015 Alabama  Calhoun County 1591.091 [km^2]        118572 ## 9  01017 Alabama Chambers County 1557.054 [km^2]         34215 ## 10 01019 Alabama Cherokee County 1549.770 [km^2]         25989 ##                          geometry ## 1  MULTIPOLYGON (((1225972 -12... ## 2  MULTIPOLYGON (((1200268 -15... ## 3  MULTIPOLYGON (((1408158 -13... ## 4  MULTIPOLYGON (((1207082 -12... ## 5  MULTIPOLYGON (((1260171 -11... ## 6  MULTIPOLYGON (((1370531 -12... ## 7  MULTIPOLYGON (((1280245 -13... ## 8  MULTIPOLYGON (((1313695 -11... ## 9  MULTIPOLYGON (((1374433 -12... ## 10 MULTIPOLYGON (((1333380 -10...\nNote that we are using a left join, therefore the county layer retains all of its records, whether or not there is a match in the dat table (Section 4.6.2). Features in county that do not have a matching FIPS value in dat get NA in the new CENSUS2010POP column.\nA map of the new CENSUS2010POP attribute, using the default equal breaks, is shown in Figure 8.26:\nplot(county[, \"CENSUS2010POP\"])\nFigure 8.26: Population size per county in the US (equal breaks)\nAlmost all counties fall into the first category, therefore the map is uniformely colored and not very informative. A map using quantile breaks (Figure 8.27) reveals the spatial pattern of population sizes more clearly:\nplot(county[, \"CENSUS2010POP\"], breaks = \"quantile\")\nFigure 8.27: Population size per county in the US (quantile breaks)\nAnother issue with the map is that county population size also depends on county area, which can make the pattern misleading in case when county area sizes are variable (which they are). To safely compare the counties we need to standardize population size by county area. In other words, we need to calculate population density:\ncounty$density = county$CENSUS2010POP / county$area\nNote how the measurement units (km−2��−2, i.e., number of people per km2��2) for the new column are automatically determined based on the inputs:\nhead(county$density) ## Units: [1/km^2] ## [1] 34.91863 42.91023 11.78344 14.13323 33.87575  6.72076\nThe map of population densities per county is shown in Figure 8.28:\nplot(county[, \"density\"], breaks = \"quantile\")\nFigure 8.28: Population density in the US, quantile breaks\n\nHow many features in county did not have a matching FIPS code in dat? What are their names?\n\n\nHow many features in dat do not have a matching FIPS code in county?\n\nWe can also calculate the average population density in the US, by dividing the total population by the total area:\nsum(county$CENSUS2010POP, na.rm = TRUE) / sum(county$area) ## 39.2295 [1/km^2]\nThis is close to the population density in 2010 according to Wikipedia, which is 40.015.\nSimply averaging the density column gives an overestimation, because all counties get equal weight while in reality the smaller counties are more dense:\nmean(county$density, na.rm = TRUE) ## 93.87977 [1/km^2]"
  },
  {
    "objectID": "ch8.html#recap-main-data-structures",
    "href": "ch8.html#recap-main-data-structures",
    "title": "13  Geometric operations with vector layers",
    "section": "13.7 Recap: main data structures",
    "text": "13.7 Recap: main data structures\nTable 8.1 summarizes the main data structures we learn in this book.\n\n\n\n\n\nTable 8.1: Main data structures in the book\n\n\nCategory\nClass\nChapter\n\n\nVector\nnumeric, character, logical\n2\n\n\nDate\nDate\n3\n\n\nTable\ndata.frame\n4\n\n\nMatrix\nmatrix\n5\n\n\nArray\narray\n5\n\n\nRaster\nstars\n5\n\n\nVector layer\nsfg, sfc, sf\n7\n\n\nUnits\nunits\n8\n\n\nList\nlist\n11\n\n\nGeostatistical model\ngstat\n12\n\n\nVariogram model\nvariogramModel\n12"
  },
  {
    "objectID": "ch9.html#aims",
    "href": "ch9.html#aims",
    "title": "14  Geometric operations with rasters",
    "section": "14.1 Aims",
    "text": "14.1 Aims\nOur aims in this chapter are:\n\nMake changes in the geometric component of rasters:\n\nMosaicking\nResampling\nReprojecting\n\nApply focal filters on a raster\n\nWe will use the following R packages:\n\nstars\nunits"
  },
  {
    "objectID": "ch9.html#mosaicking-rasters",
    "href": "ch9.html#mosaicking-rasters",
    "title": "14  Geometric operations with rasters",
    "section": "14.2 Mosaicking rasters",
    "text": "14.2 Mosaicking rasters\nIn the next few examples, we will prepare a Digital Elevation Model (DEM) raster of Haifa, by mosaicking, subsetting and reprojecting (Section 9.3).\nWe start with two 5°×5°5°×5° tiles of elevation data from the Shuttle Radar Topography Mission (SRTM) dataset. The tiles are included as two .tif files in the sample files (Appendix A):\nlibrary(stars) dem1 = read_stars(\"srtm_43_06.tif\") dem2 = read_stars(\"srtm_44_06.tif\")\nAs shown in Figure 9.1, the tiles cover the area of northern Israel, including Haifa:\nplot(dem1, breaks = \"equal\", col = terrain.colors(10), axes = TRUE) plot(dem2, breaks = \"equal\", col = terrain.colors(10), axes = TRUE)\nFigure 9.1: Two elevation tiles from the SRTM dataset\nUsing the st_bbox and dim functions (Section 5.3.8.3), we can see that the two rasters indeed comprise two tiles of the same \"large\" dataset. First, we can see that the dimensions (number of rows and columns) of the two rasters are identical:\ndim(dem1) ##    x    y  ## 6000 6000 dim(dem2) ##    x    y  ## 6000 6000\nSecond, we can see that their extents of the rasters are aligned. The y-axis (i.e., latitude) extent of the rasters is the same (30-35). The x-axis (i.e., longitude) extents of the rasters are side-by-side (30-35 and 35-40):\nst_bbox(dem1) ## xmin ymin xmax ymax  ##   30   30   35   35 st_bbox(dem2) ## xmin ymin xmax ymax  ##   35   30   40   35\nRasters can be mosaicked using the st_mosaic function. The st_mosaic function accepts two or more stars objects—such as dem1 and dem2—and returns a combined raster:\ndem = st_mosaic(dem1, dem2) names(dem) = \"elevation\"\nNote that the st_moisaic function can be used to combine rasters even if they are not aligned, such as in this example. In case the rasters are not aligned, resampling (Section 9.2) take place.\nThe mosaicked DEM is shown in Figure 9.2:\nplot(dem, axes = TRUE, breaks = \"equal\", col = terrain.colors(10))\nFigure 9.2: Mosaicked raster\nThe extent of the resulting raster covers the extents of both inputs:\nst_bbox(dem) ## xmin ymin xmax ymax  ##   30   30   40   35\nFor the next examples, we \"crop\" the dem raster according to an extent of 0.25°×0.25°0.25°×0.25° around Haifa (Figure 9.3).\nFigure 9.3: An 0.25°×0.25°0.25°×0.25° rectangular extent\nThe [ operator can be used to crop the raster (Section 6.2):\ndem = dem[, 5687:6287, 2348:2948]\nThe result is shown in Figure 9.4:\nplot(dem, axes = TRUE, breaks = \"equal\", col = terrain.colors(10))\nFigure 9.4: Cropped raster\nNote that in this example, we needed to know in advance the row and column indices of the extent we are interested in cropping. Later on (Section 10.1), we are going to learn how to crop a raster based on an existing vector layer, such as the bounding box of a buffer of 0.125°0.125° around a point layer representing the city of Haifa, which is a much more practical approach."
  },
  {
    "objectID": "ch9.html#raster-resampling",
    "href": "ch9.html#raster-resampling",
    "title": "14  Geometric operations with rasters",
    "section": "14.3 Raster resampling",
    "text": "14.3 Raster resampling\n\n14.3.1 The st_warp function\nRaster resampling is the process of transferring raster values from the original grid to a different grid (Figure 9.5). Resampling is onften required for:\n\nAligning several input rasters that come from different sources to the same grid, so that they can be subject to spatial operators such as raster algebra (Sections 6.4 and 6.6.1)\nReducing the resolution of very detailed rasters, so that they are more convenient to work with in terms of processing time and memory use\n\nFigure 9.5: Raster resampling (https://www.safe.com/transformers/raster-resampler/)\nTo demonstrate resampling, we will create a custom stars grid, using the same extent as the dem raster (st_bbox(dem)), but a coarser resolution. The new resolution is 0.002°0.002°, which is ~2.4 times the original resolution of 0.00083°0.00083°. The grid is created using st_as_stars:\ngrid = st_as_stars(st_bbox(dem), dx = 0.002, dy = 0.002) grid ## stars object with 2 dimensions and 1 attribute ## attribute(s): ##     values   ##  Min.   :0   ##  1st Qu.:0   ##  Median :0   ##  Mean   :0   ##  3rd Qu.:0   ##  Max.   :0   ## dimension(s): ##   from  to  offset  delta refsys point values x/y ## x    1 251 34.7383  0.002 WGS 84    NA   NULL [x] ## y    1 251 33.0442 -0.002 WGS 84    NA   NULL [y]\nRecall that we already used the st_as_stars function to convert a matrix to a stars raster (Section 6.3.3). What we do here is another mode of operation of the st_as_stars function, creating an empty raster given a bbox and the x- and y-axis resolutions dx and dy.\nWe can resample a raster using the st_warp function. The first two parameters of st_warp are:\n\nThe raster to be resampled, where the values come from\nThe raster defining the new grid\n\nHere is the expression to resample the values of the dem raster into the new raster grid:\ndem1 = st_warp(dem, grid)\n\nWhich one of the rasters in Figure 9.5 represents the role of dem and grid, in our case?\n\n\nTry resampling dem to a different grid with an even coarser resulution, such as 0.02, and plot the result\n\nThe original DEM is shown along with the resampled one in Figure 9.6.\nplot(dem, breaks = \"equal\", col = terrain.colors(10), main = \"input (delta=0.0008)\") plot(dem1, breaks = \"equal\", col = terrain.colors(10), main = \"output (delta=0.002)\")\nFigure 9.6: DEM resampled by a factor of 2.4, from 0.0008 (left) to 0.002 (right) resolution\nWhen resampling, the computer needs to decide which pixel value(s) to transfer to each of the \"new\" pixels, defined by the new grid. There are several possible options, known as resampling algorithms or methods. In the next three sections, we are going to demonstrate three common options:\n\nNearest neighbor resampling (the default) (Section 9.2.2)\nBilinear resampling (Section 9.2.3)\nAverage resampling (Section 9.2.4)\n\n\n\n14.3.2 Nearest neighbor resampling\nThe st_warp expression from Section 9.2.1 actually used the nearest neighbor resampling method (method=\"near\"), which is the default:\ndem1 = st_warp(dem, grid, method = \"near\")\nTo understand what actually happens in nearest neighbor resampling, let's take a look at a small part of the raster \"before\" and \"after\" images (Figure 9.7). If you look closely, you can see that the values of the original raster are passed to the resampled raster. What happens when there is more than one pixel of the original raster coinciding with a single pixel in the new grid? How can we decide which value is going to be passed? In nearest neighbor resampling, the new raster pixels get the value from the nearest pixel of the original raster. Note that some of the values may be \"lost\" this way, since they were not passed on to the new raster32.\nFigure 9.7: Nearest neighbor resampling\n\n\n14.3.3 Bilinear resampling\nBilinear resampling is another resampling method. In bilinear resampling, each new raster cell gets a weighted average of four nearest cells from the input, rather than just one. Bilinear resampling is specified with method=\"bilinear\" in st_warp. Note that use_gdal=TRUE needs to be specified when using any method other than method=\"near\", otherwise the method argument is ignored:\ndem1 = st_warp(dem, grid, method = \"bilinear\", use_gdal = TRUE)\nWith bilinear resampling, the output raster is \"smoothed,\" containing new values which are averages of (some of) the values in the original raster (Figure 9.8).\nFigure 9.8: Bilinear resampling\n\n\n14.3.4 Average resampling\nAnother useful method is the average resampling method, where each new cell gets the weighted average of all overlapping input cells:\ndem1 = st_warp(dem, grid, method = \"average\", use_gdal = TRUE)\nThe result of average resampling is shown in Figure 9.9:\nFigure 9.9: Average resampling\nIn addition to \"near\", \"bilinear\", and \"average\", the st_warp function supports other resampling methods, including: \"cubic\", \"cubicspline\", \"lanczos\", \"mode\", \"max\", \"min\", \"med\", \"q1\" and \"q3\".\nBilinear resampling may be preferred when the result is primarily used for visualization, because the result appears smoother. Nearest neighbor resampling, however, is preferable when we are using the result for further analysis, because the original values are preserved. When the input raster is categorical, such as a raster with land cover classes 1, 2, 3, etc., nearest neighbor resampling is the only valid resampling option, because averaging category IDs makes no sense.\n\nIn what situations do you think the \"average\" resampling method is mostly appropriate, while \"near\" and \"bilinear\" are not?"
  },
  {
    "objectID": "ch9.html#raster-reprojection",
    "href": "ch9.html#raster-reprojection",
    "title": "14  Geometric operations with rasters",
    "section": "14.4 Raster reprojection",
    "text": "14.4 Raster reprojection\nRaster reprojection is more complex than vector layer reprojection (Section 7.9.2). In addition to transforming (pixel) coordinates, like in vector layer reprojection, raster reprojection requires a resampling step in order to \"arrange\" the transformed values back into a regular grid (Figure 9.14).\nIn terms of code, the st_warp function, which we used for resampling (Section 9.2), is used for raster reprojection too. The only difference is that, in raster reprojection, the \"destination\" grid is specified in a different CRS.\nFor example, the following expression reprojects the DEM of Haifa from WGS84 (4326) to UTM (32636), using the nearest neighbor resampling method. Note that, in this example, we are not passing a stars object with the destination grid. Instead, we are letting the function to automatically generate the new grid, only specifying the destination CRS (crs=32636) and resolution (cellsize=90):\ndem = st_warp(dem, crs = 32636, cellsize = 90)\nThe original raster (in WGS84) and the reprojected one (in UTM) are shown in Figure 9.10.\nFigure 9.10: Original (left, in WGS84) and reprojected (right, in UTM) dem raster\nNote that the coordinate units in the reprojected raster (Figure 9.10) are no longer degrees, but meters. Also, the area contaning non-missing values is slightly rotated compared to the input, because the WGS84 and UTM systems are not parallel, at least in this particular location.\nA zoomed-in view (Figure 9.11) of the original raster and the new grid demonstrates that the two are not parallel. Again, note that this time the new grid is in a different CRS, which is why the two grids are not parallel to each other (Figure 9.7–9.9).\nFigure 9.11: The reprojected raster grid (UTM, black) and the original raster grid (WGS84, in red), displayed in UTM\nAs another example, let's reproject the MOD13A3_2000_2019.tif, from its sinusoidal projection to a projection more suitable for the specific region, such as ITM (Table 7.4). First, let's import the raster from the GeoTIFF file:\nr = read_stars(\"MOD13A3_2000_2019.tif\") names(r) = \"NDVI\"\nThen, we can reproject the raster using st_warp. This time we specify just the destination CRS (crs=2039), letting the function automatically determine the resolution:\nr_itm = st_warp(r, crs = 2039)\nThe original and reprojected rasters are shown in Figure 9.12:\nplot(r[,,,1,drop=TRUE], key.pos = 4, axes = TRUE, col = hcl.colors(11, \"Spectral\"), main = \"input (Sinusoidal)\") plot(r_itm[,,,1,drop=TRUE], key.pos = 4, axes = TRUE, col = hcl.colors(11, \"Spectral\"), main = \"output (ITM)\")\nFigure 9.12: Reprojection of the MODIS NDVI raster from Sinusoidal (left) to ITM (right)\nTo see the process of reprojection more clearly, let's examine a small subset of the NDVI raster:\nu = r[, 100:105, 200:207, 2, drop = TRUE] u_itm = st_warp(u, crs = 2039)\nThe original and reprojected raster subsets are shown in Figure 9.13.\nFigure 9.13: Reprojection of a small subset of the MODIS NDVI raster (left) to ITM (right)\nWhat happens in the reprojection can be thought of as a two-step process (Figure 9.14). In the first step, the pixel outlines are reprojected as if they were polygons (Section 7.9.2), which results in an irregular grid. The grid is then resampled (Section 9.2) to form a regular grid, so that it can be represented by a raster once again.\nFigure 9.14: Reprojection process: the original raster (left), the reprojected raster cells as polygons (middle) and the resampled reprojected raster (right)"
  },
  {
    "objectID": "ch9.html#focal-filters",
    "href": "ch9.html#focal-filters",
    "title": "14  Geometric operations with rasters",
    "section": "14.5 Focal filters",
    "text": "14.5 Focal filters\n\n14.5.1 Introduction\nSo far, we only dealt with arithmetic operations that address the values of each per pixel in isolation from neighboring pixels, such as in raster algebra (Sections 6.4 and 6.6.1). Another class of raster operations is where the calculation of each pixels depends on values of neighboring cells.\nThe most prominent example of a raster calculation based on neighboring cells is moving window calculations, also known as applying a focal filter. With a moving window calculation, raster values are transformed based on the values from a neighborhood surrounding each pixel. The functions applied on the neighborhood are varied, from simple functions such as mean for a low-pass filter (Section 9.4.2) to more complex functions, such as those used to calculate topographic slope and aspect (Section 9.4.4–9.4.5).\nFor example, a 3×33×3 mean filter applied on a raster results in a new raster, where the values are averages of 3×33×3 neighborhoods centered on that pixel. In Figure 9.15, the highlghted value in the output on the right (18.7) is the average of the highlighted 3×33×3 neighborhood of the input on the left:\nmean(c(27, 8, 22, 16, 21, 16, 6, 44, 8)) ## [1] 18.66667\nFigure 9.15: Focal filter (http://courses.washington.edu/gis250/lessons/raster_analysis1/index.html)\n\n\n14.5.2 Low pass filter\n\n14.5.2.1 What is a low pass filter?\nThe purpose of the mean, or low-pass, filter (Figure 9.15) is to produce a smoothed image, where extreme values (possibly noise) are cacelled out. For example, the result of applying a 3×33×3 mean filter on a uniform image with one extreme value is shown in Figure 9.16.\nFigure 9.16: Low pass filter excample (http://desktop.arcgis.com/en/arcmap/10.3/tools/spatial-analyst-toolbox/how-filter-works.htm)\n\nHow was the value of 5.4 obtained, in the result shown in Figure 9.16?\n\nThe stars package does not, at the moment, contain a function for focal filtering33. As an exercise, we are going to create our own function for focal filtering. For simplicity, we will restrict ourselves to the case where the focal \"window\" is 3×33×3 pixels—which is the most common case.\nOur working plan is as follows:\n\nWrite a function named get_neighbors that accepts a position of the focal cell (row & column) in a matrix, and returns the 9 values of its 3×33×3 pixel neighborhood\nWrite another function named focal2 that accepts a two-dimensional stars object and a function fun, iterates over the cells and applies fun on all 3×33×3 pixel neighborhoods extracted using get_heighbors\n\n\n\n14.5.2.2 The get_neighbors function\nWe start with a function that accepts a position (row & column) in a matrix and returns a numeric vector with the 3×33×3 neighborhood, hereby named get_neighbors. The function accepts a matrix m and a position pos. The pos argument is a vector of length two, of the form c(row,column). The function extracts the 9 values in the respective 3×33×3 neighborhood and returns them as a vector of length 9:\nget_neighbors = function(m, pos) {   i = (pos[1]-1):(pos[1]+1)   j = (pos[2]-1):(pos[2]+1)   as.vector(t(m[i, j])) }\nFor example, suppose we have a 5×55×5 matrix m:\nm = matrix(1:25, ncol = 5, nrow = 5) m ##      [,1] [,2] [,3] [,4] [,5] ## [1,]    1    6   11   16   21 ## [2,]    2    7   12   17   22 ## [3,]    3    8   13   18   23 ## [4,]    4    9   14   19   24 ## [5,]    5   10   15   20   25\nUsing get_neighbors, we can get the values of just about any 3×33×3 neighborhood except for the outermost rows and columns (see below). For example, the following expression returns the values of the neighborhood centered on row 3, column 4:\nget_neighbors(m, c(3, 4)) ## [1] 12 17 22 13 18 23 14 19 24\nHow does the get_neighbors function work? The function first calculates the required range of rows and columns:\npos = c(3, 4)\ni = (pos[1]-1):(pos[1]+1)  # Rows i ## [1] 2 3 4\nj = (pos[2]-1):(pos[2]+1)  # Columns j ## [1] 3 4 5\nThen, the function extracts the corresponding matrix subset:\nm[i, j] ##      [,1] [,2] [,3] ## [1,]   12   17   22 ## [2,]   13   18   23 ## [3,]   14   19   24\ntransposes it:\nt(m[i, j]) ##      [,1] [,2] [,3] ## [1,]   12   13   14 ## [2,]   17   18   19 ## [3,]   22   23   24\nand converts to a vector:\nas.vector(t(m[i, j])) ## [1] 12 17 22 13 18 23 14 19 24\nTransposing is necessary so that the matrix values are returned by row, rather than the default by column (Section 5.1.4.1).\nNote that out function is not designed to operate on the matrix edges, where the 3×33×3 neighborhood is incomplete. For example, the following expression produces an error:\nget_neighbors(m, c(1, 5)) ## Error in m[i, j]: subscript out of bounds\n\n\n14.5.2.3 The focal2 function\nNow, let's see how we can use the get_neighbors function to apply a focal filter on a raster. We will use the dem.tif small DEM for demonstration:\nx = read_stars(\"dem.tif\")\nFirst, we create a copy of the raster, named template. The template raster will be used as a \"template\" when converting the filtered matrix back to a stars object:\ntemplate = x\nNext, we extract the raster values as a matrix:\ninput = t(template[[1]])\nNote that the function relies on the fact that the stars object has just two dimensions (x and y), in which case input is going to be a matrix. The matrix is transposed, using t, to maintain the right orientation of the values matrix (Figure 6.11). This is important when using functions that distinguish between the north-south and east-west directions, topographic aspect (Section 9.4.5).\nNext, we create another matrix to hold the output values. The values are initially set to NA:\noutput = matrix(NA, nrow = nrow(input), ncol = ncol(input))\nNow comes the actual computation. We are using two for loops to go over all raster cells, excluding the first and last rows and columns. For each cell, we:\n\nextract the 3×33×3 neighborhood [i,j],\napply a function—such as mean, in this case—on the vector of extracted values, and\nplace the result into the corresponding cell [i,j] in the output.\n\nThe complete code of the for loops is as follows:\nfor(i in 2:(nrow(input) - 1)) {   for(j in 2:(ncol(input) - 1)) {     v = get_neighbors(input, c(i, j))     output[i, j] = mean(v, na.rm = TRUE)   } }\nNote that the function starts at row i=2 and ends at row i=nrow(input)-1. Similarly, it starts at column j=2 and ends at j=ncol(input)-1.\nIn the end, when both for loops have been completed, the output matrix contains the new, filtered, raster values. What is left to be done is to put the matrix of new values into the template, to get back a stars object. We are using t once again, to transform the matrix back into the \"stars\" arrangement (Figure 6.11):\ntemplate[[1]] = t(output)\nThe original image (x) and the filtered image (template) are shown in Figure 9.17. The figure highlights one 3×33×3 neighborhood in the input, and the corresponding average of that neighborhood in the output34.\ncol = terrain.colors(5) plot(x, text_values = TRUE, col = col, key.pos = 4, reset = FALSE, main = \"input\") plot(st_geometry(st_as_sf(x, na.rm = FALSE)), border = \"grey\", add = TRUE) plot(st_geometry(st_as_sf(x[,4:6,6:8])), lwd = 2, add = TRUE) plot(round(template, 1), text_values = TRUE, col = col, key.pos = 4, reset = FALSE, main = \"output\") plot(st_geometry(st_as_sf(template, na.rm = FALSE)), border = \"grey\", add = TRUE) plot(st_geometry(st_as_sf(template[,5,7])), lwd = 2, add = TRUE)\nFigure 9.17: Focal filter with the mean function (left: input, right: result)\n\nWhy do the outermost rows and columns in Figure 9.17 appear empty? Which value do these pixels contain, and where did it come from?\n\nLet's manually check the calculation of the 3×33×3 neighborhood highlighted in Figure 9.17:\nt(x[[1]][4:6,6:8]) ##      [,1] [,2] [,3] ## [1,]  146    6    6 ## [2,]  340  163   13 ## [3,]  383  448  152\nmean(t(x[[1]][4:6,6:8])) ## [1] 184.1111\n\nWhy do you think we got NaN values in some of the cells? Can you simulate the situation to see how an NaN values is produced? What can we do to get NA, instead, in those pixes that get NaN?\n\nWrapping up out code in a function, called focal2, can be done as follows. The input raster and the function are replaced with parameters named r and fun, respectively:\nfocal2 = function(r, fun) {   template = r   input = t(template[[1]])   output = matrix(NA, nrow = nrow(input), ncol = ncol(input))   for(i in 2:(nrow(input) - 1)) {     for(j in 2:(ncol(input) - 1)) {       v = get_neighbors(input, c(i, j))       output[i, j] = fun(v)     }   }   template[[1]] = t(output)   return(template) }\nHow can we pass additional parameters to the function we use, such as na.rm=TRUE for mean? The special dots ... argument is used for that. Now, any additional argument(s) passed to focal2 (such as na.rm=TRUE) will be passed on as additional argument(s) to fun:\nfocal2 = function(r, fun, ...) {   template = r   input = t(template[[1]])   output = matrix(NA, nrow = nrow(input), ncol = ncol(input))   for(i in 2:(nrow(input) - 1)) {     for(j in 2:(ncol(input) - 1)) {       v = get_neighbors(input, c(i, j))       output[i, j] = fun(v, ...)     }   }   template[[1]] = t(output)   return(template) }\nNow that we have a custom focal filter function focal2, let's try to apply a different filter, such as a maximum filter:\nx_max = focal2(x, max, na.rm = TRUE) ## Warning in fun(v, ...): no non-missing arguments to max; returning -Inf  ## Warning in fun(v, ...): no non-missing arguments to max; returning -Inf\nThe reason for the warnings produced by the above expression is that max applied on an empty vector gives -Inf.\nt(x_max[[1]]) ##       [,1] [,2] [,3] [,4] [,5] [,6] [,7] ##  [1,]   NA   NA   NA   NA   NA   NA   NA ##  [2,]   NA -Inf -Inf    6    9    9   NA ##  [3,]   NA   61   61    9   10   16   NA ##  [4,]   NA  132  132  132   11   27   NA ##  [5,]   NA  254  254  254  146   27   NA ##  [6,]   NA  254  340  340  340  163   NA ##  [7,]   NA  254  383  448  448  448   NA ##  [8,]   NA  253  383  448  448  448   NA ##  [9,]   NA  253  383  448  448  448   NA ## [10,]   NA   NA   NA   NA   NA   NA   NA\nTo get NA instead, we can use a slightly more complex function that first checks if particular neighborhood contains any non-NA values, any only then applies max:\nf = function(x) if(all(is.na(x))) NA else max(x, na.rm = TRUE) x_max = focal2(x, f)\nRecall that we used the same principle when applying the min and max functions with st_apply (Section 6.6.1.4).\nThe resulting raster x_max is shown in Figure 9.18. Indeed, every pixel value in the output raster is the maximal value among the values in its 3×33×3 neighborhood:\ncol = terrain.colors(10) plot(x, text_values = TRUE, breaks = \"equal\", col = col, key.pos = 4, reset = FALSE, main = \"input\") plot(st_geometry(st_as_sf(x, na.rm = FALSE)), border = \"grey\", add = TRUE) plot(st_geometry(st_as_sf(x[,4:6,6:8])), lwd = 2, add = TRUE) plot(x_max, breaks = \"equal\", text_values = TRUE, col = col, key.pos = 4, reset = FALSE, main = \"output\") plot(st_geometry(st_as_sf(x_max, na.rm = FALSE)), border = \"grey\", add = TRUE) plot(st_geometry(st_as_sf(x_max[,5,7])), lwd = 2, add = TRUE)\nFigure 9.18: Focal filter with the max function (left: input, right: result)\nLet's try the focal2 function on another, bigger raster. For example, we can apply a low pass (i.e., mean) filter on the first layer of the MODIS NDVI raster, as follows:\nr_itm1 = r_itm[,,,1,drop=TRUE] r_itm1_mean = focal2(r_itm1, mean, na.rm = TRUE)\nThe original image and the filtered result are shown in Figure 9.19:\nplot(r_itm1, col = hcl.colors(11, \"Spectral\"), main = \"input\") plot(r_itm1_mean, col = hcl.colors(11, \"Spectral\"), main = \"output\")\nFigure 9.19: Low pass filter result\n\nWhy are there NA areas in the raster, even though we used na.rm=TRUE?\n\n\n\n\n14.5.3 Maximum filter\nFor another example, let's reconstruct the l_rec raster (Section 6.5):\nl = read_stars(\"landsat_04_10_2000.tif\") red = l[,,,3, drop = TRUE] nir = l[,,,4, drop = TRUE] ndvi = (nir - red) / (nir + red) names(ndvi) = \"NDVI\" l_rec = ndvi l_rec[l_rec &lt; 0.2] = 0 l_rec[l_rec &gt;= 0.2] = 1\nGiven a raster with 0 and 1 values, such as l_rec, we may want to convert all 0 cells neighboring to a 1 cell to become 1. That way, for instance, the areas of the planted forests we see in the center of the image will be come more continuous, which will make it easier to transform those areas into polygons (Section 10.3.2). This can be achieved with a focal filter and the max function:\nl_rec_focal = focal2(l_rec, max)\nThe original raster and the filtered result are shown in Figure 9.20:\nplot(l_rec, col = c(\"grey90\", \"darkgreen\"), main = \"input\") plot(l_rec_focal, col = c(\"grey90\", \"darkgreen\"), main = \"output\")\nFigure 9.20: \"Buffering\" 1 values in a raster with 0s and 1s, using a focal filter with max\n\n\n14.5.4 Topographic slope\nSo far we saw examples where the focal function is a simple built-in one, such as mean or max. In some cases, it is required to use a more complex function. For example, topographic indices such as slope and aspect employ complex functions where direction matters: each of the nine pixels in the 3×33×3 neignborhood is treated differently. For example, to calculate topographic slope based on elevation values in a 3×33×3 neignborhood, the following function can be used. Note that the slope calulation also depends on raster resolution, which is passed as an additional parameter named res:\nslope = function(x, res) {   dzdx = ((x[3] + 2*x[6] + x[9]) - (x[1] + 2*x[4] + x[7])) / (8 * res)   dzdy = ((x[7] + 2*x[8] + x[9]) - (x[1] + 2*x[2] + x[3])) / (8 * res)   atan(sqrt(dzdx^2 + dzdy^2)) * (180 / pi) }\nWe will not go into details on how the function works. You may refer to the How slope works article in the ArcGIS documentation for an explanation.\nFor example:\nx = c(50, 45, 50, 30, 30, 30, 8, 10, 10) res = 5 slope(x, res)  # 75.25762 ## [1] 75.25766\nThe slope function can be passed to focal2 to apply the slope calculation on the entire dem raster:\ndem_slope = focal2(dem, slope, res = st_dimensions(dem)$x$delta) names(dem_slope) = \"slope\"\nIt is also convenient to set raster units with set_units (Section 8.3.2.2). The units of slope are decimal degrees (°):\nlibrary(units) dem_slope[[1]] = set_units(dem_slope[[1]], \"degree\")\nThe resulting topographic slope raster is shown in the left panel in Figure 9.21.\n\n\n14.5.5 Topographic aspect\nAnother function, as shown below, can be used to calculate topographic aspect:\naspect = function(x, res) {   dzdx = ((x[3] + 2*x[6] + x[9]) - (x[1] + 2*x[4] + x[7])) / 8   dzdy = ((x[7] + 2*x[8] + x[9]) - (x[1] + 2*x[2] + x[3])) / 8   a = (180 / pi) * atan2(dzdy, -dzdx)   if(is.na(a)) return(NA)   if (a &lt; 0) a = 90 - a else      if (a &gt; 90) a = 360 - a + 90 else        a = 90 - a   return(a) }\nFor details on how the function works, see the How aspect works article in the ArcGIS documentation.\nFor example:\nx = c(101, 92, 85, 101, 92, 85, 101, 91, 84) aspect(x)  # 92.64 ## [1] 92.64255\nAgain, the aspect function can be passed to focal2 to apply it on the entire raster:\ndem_aspect = focal2(dem, aspect) names(dem_aspect) = \"aspect\" dem_aspect[[1]] = set_units(dem_aspect[[1]], \"degree\")\nThe resulting topographic aspect raster is shown in the right panel in Figure 9.21:\nplot(dem_slope, breaks = \"equal\", col = rev(hcl.colors(11, \"Spectral\"))) plot(dem_aspect, breaks = \"equal\", col = hcl.colors(11, \"Spectral\"))\nFigure 9.21: Topographic slope (left) and topographic aspect (right)\nNote that our custom focal2+get_neighbors functions are quite minimal, and can be improved in several ways:\n\nBeing able to set neighbor sizes other than 3×33×3\nDealing with the first/last rows and and columns (see Section G)\nDealing with rasters that have more than two dimensions (separate filter per dimension? a three-dimensional filter?)\nMaking the calculation more efficient (e.g., using C/C++ code inside R or using parallel computation)"
  },
  {
    "objectID": "ch10.html#aims",
    "href": "ch10.html#aims",
    "title": "15  Combining rasters and vector layers",
    "section": "15.1 Aims",
    "text": "15.1 Aims\nOur aims in this chapter are to learn how to:\n\nCrop and mask a raster according to a vector layer\nSwitch from vector to raster representation, and vice versa\nCalculate a raster of distances to nearest point\nExtract raster values from locations defined by a vector layer\n\nWe will use the following R packages:\n\nsf\nstars\nunits"
  },
  {
    "objectID": "ch10.html#masking-and-cropping-rasters",
    "href": "ch10.html#masking-and-cropping-rasters",
    "title": "15  Combining rasters and vector layers",
    "section": "15.2 Masking and cropping rasters",
    "text": "15.2 Masking and cropping rasters\n\n15.2.1 Introduction\nMasking a raster means turning pixels values outside of a an area of interest—defined using a polygonal layer—into NA (Figure 10.1). Cropping a raster means deleting whole rows and/or columns, so that raster extent is reduced to a new (smaller) rectangular shape, also according to the extent of a vector layer. The [ operator can be used for masking or masking and cropping (the default).\nFigure 10.1: Masking a raster (http://rpubs.com/etiennebr/visualraster)\n\n\n15.2.2 Masking and cropping\nFor an example of masking and cropping, we will prepare a raster of average NDVI over the period 2000-2019, based on MOD13A3_2000_2019.tif. The following code section combines what we learned in Sections 6.3.2, 6.6.1.3 and 9.3:\nlibrary(stars) r = read_stars(\"MOD13A3_2000_2019.tif\") names(r) = \"NDVI\" r = st_warp(r, crs = 32636) r_avg = st_apply(r, 1:2, mean, na.rm = TRUE) names(r_avg) = \"NDVI\" dates = read.csv(\"MOD13A3_2000_2019_dates2.csv\") dates$date = as.Date(dates$date) r = st_set_dimensions(r, \"band\", values = dates$date, names = \"time\")\nWe will also read a Shapefile with a polygon of Israel borders, named israel_borders.shp:\nborders = st_read(\"israel_borders.shp\")\nFigure 10.2 shows the average NDVI raster and the borders polygon:\nplot(r_avg, breaks = \"equal\", col = hcl.colors(11, \"Spectral\"), reset = FALSE) plot(st_geometry(borders), add = TRUE)\nFigure 10.2: Raster and crop/mask geometry\nMasking and cropping a raster, based on an sf layer, can be done with the [ operator, as follows:\nr_avg = r_avg[borders]\nAs we can see in Figure 10.3, all pixels that were outside of the polygon are now NA (this is called masking). It's difficult to see, but the raster extent is also slightly reduced according to the extent of borders (this is called cropping):\nplot(r_avg, breaks = \"equal\", col = hcl.colors(11, \"Spectral\"), reset = FALSE) plot(st_geometry(borders), add = TRUE)\nFigure 10.3: Raster after cropping and masking\nZooming in on a small portion of the masked raster demonstrates the masking algorithm: pixels whose centroid does not intersect with the polygon are converted to NA (Figure 10.4).\nFigure 10.4: A small portion of the raster r_avg, brefore (left) after (right) cropping and masking\n\n\n15.2.3 Masking-only\nIn case we need the output to keep the same extent as the input, we can mask a raster without cropping. Masking-only is done with the same operator ([), using the (non-default) argument crop=FALSE. To demonstrate the difference between masking+cropping (which we just did) and masking only, consider the following example where r_avg is masked and cropped (r_avg1), or just masked (r_avg2), according to the \"Negev\" administrative area. First we prepare the polygon used for masking and cropping, obtained from a Shapefile named nafot.shp:\nnafot = st_read(\"nafot.shp\") nafot = st_transform(nafot, st_crs(r)) pol = nafot[nafot$name_eng == \"Be'er Sheva\", ]\nThen we produce the masked+cropped (r_avg1) and masked (r_avg2) rasters:\nr_avg1 = r_avg[pol] r_avg2 = r_avg[pol, crop = FALSE]\nAs we can see in Figure 10.5, masking transforms pixel values to NA (left panel) while cropping reduces raster extent by deleting whole rows and columns (right panel).\nplot(r_avg1, key.pos = NULL, reset = FALSE, breaks = \"equal\", col = hcl.colors(11, \"Spectral\"), axes = TRUE, main = \"mask+crop (crop=TRUE, default)\") plot(st_geometry(pol), add = TRUE) plot(st_as_sfc(st_bbox(r_avg1)), border = \"grey\", add = TRUE) plot(r_avg2, key.pos = NULL, reset = FALSE, breaks = \"equal\", col = hcl.colors(11, \"Spectral\"), axes = TRUE, main = \"mask (crop=FALSE)\") plot(st_geometry(pol), add = TRUE) plot(st_as_sfc(st_bbox(r_avg2)), border = \"grey\", add = TRUE)\nFigure 10.5: Cropping and masking (left) vs. masking (right), raster extent is shown in grey\nNote that for plotting the raster extents (grey boxes in Figure 10.5), the above code section uses a combination of st_bbox and st_as_sfc. The combination returns the bounding box of a stars (or sf) layer as an sfc (geometry) object. For example, the following expression returns a geometry column with a single polygon, which is the bounding box of r_avg1:\nst_as_sfc(st_bbox(r_avg1)) ## Geometry set for 1 feature  ## Geometry type: POLYGON ## Dimension:     XY ## Bounding box:  xmin: 620563.9 ymin: 3262755 xmax: 735425.7 ymax: 3487847 ## Projected CRS: WGS 84 / UTM zone 36N ## POLYGON ((620563.9 3262755, 735425.7 3262755, 7..."
  },
  {
    "objectID": "ch10.html#vector-layer-raster",
    "href": "ch10.html#vector-layer-raster",
    "title": "15  Combining rasters and vector layers",
    "section": "15.3 Vector layer → raster",
    "text": "15.3 Vector layer → raster\n\n15.3.1 10.2.1 The st_rasterize function\nThe st_rasterize function converts a vector layer to a raster, given two arguments:\n\nsf—The vector layer to convert\ntemplate—A raster \"template\" (if missing, it can be generated automatically)\n\nThe resulting raster retains the original values, from the template, in pixels that do not overlap with the vector layer (Figure 10.6). For pixels that do overlap with the vector layer, the value of the (first) vector layer attribute is \"burned\" into the pixels. The default meaning of the term \"overlap\" in the context of point, line, or polygon geometries, is:\n\nPixels that a point intersects with\nPixels chosen using Bresenham's line algorithm\nPixels whose centroid intersects with the polygon\n\nThe additional parameter options=\"ALL_TOUCHED=TRUE\" can be passed to st_rasterize so that all pixels intersecting with the geometry are considered \"overlapping\" (Figure 10.6).\nFigure 10.6: Rasterizing points, lines and polygons to raster with st_rasterize, using the default algorithms (top) and options=\"ALL_TOUCHED=TRUE\" (bottom).\nBy default, the values are burned sequentially, according to the order of features. Therefore, when there are more than one vector features coinciding with the same pixel, the attribute value of the last feature \"wins.\" Alternatively, we can add up all values of coinciding geometries, using the argument options=\"MERGE_ALG=ADD\" (Section 10.2.3).\n\n\n15.3.2 Rasterizing polygon attributes\nFor an example of rasterizing a polygon layer, let's convert the nafot vector layer to a raster, using r_avg as the template. The attribute which will be \"burned\" into the raster is the Id of the administrative area:\ns = st_rasterize(nafot[, \"Id\"], r_avg)\nThe resulting raster s has the same dimensions of r_avg but different values, as shown in Figure 10.7. We can see that those areas not covered by the nafot layer retained their original value (average NDVI). All pixels that were covered by nafot got the Id attribute value.\nplot(nafot[, \"Id\"], pal = function(n) rep(NA, n), main = 'nafot[, \"Id\"]', border = NA, reset = FALSE, key.pos = NULL, axes = TRUE) plot(nafot[, \"Id\"], pal = function(n) hcl.colors(n, \"Spectral\"), key.pos = NULL, add = TRUE) plot(nafot[, \"Id\"], pal = function(n) rep(NA, n), main = NA, border = NA, reset = FALSE, key.pos = NULL, axes = TRUE) plot(r_avg, breaks = \"equal\", col = hcl.colors(15, \"Spectral\"), main = \"r_avg\", add = TRUE) plot(nafot[, \"Id\"], pal = function(n) rep(NA, n), main = NA, border = NA, reset = FALSE, key.pos = NULL, axes = TRUE) plot(s, breaks = \"equal\", col = hcl.colors(15, \"Spectral\"), main = 'st_rasterize(nafot[, \"Id\"], r_avg)', add = TRUE)\nFigure 10.7: Rasterizing the Id of the nafot administrative areas layer into the average NDVI raster\nInstead of retaining the original values, typically we want to start with an empty template, so that the only values in the resulting raster are those coming from the vector layer. For example, we can use a copy of the r_avg raster, where all of the pixel values are replaced with NA, as the template:\ntemplate = r_avg template[[1]][] = NA s = st_rasterize(nafot[, \"Id\"], template)\nThe result is shown in Figure 10.8:\nplot(nafot[, \"Id\"], pal = function(n) hcl.colors(n, \"Spectral\"), main = 'nafot[, \"Id\"]', key.pos = NULL, axes = TRUE) plot(nafot[, \"Id\"], pal = function(n) rep(NA, n), main = \"template\", border = NA, reset = FALSE, key.pos = NULL, axes = TRUE) plot(nafot[, \"Id\"], pal = function(n) rep(NA, n), main = NA, border = NA, reset = FALSE, key.pos = NULL, axes = TRUE) plot(s, breaks = \"equal\", col = hcl.colors(15, \"Spectral\"), main = 'st_rasterize(nafot[, \"Id\"], template)', add = TRUE)\nFigure 10.8: Rasterizing into an empty template\n\n\n15.3.3 Rasterizing point counts\nSometimes we want to add up attribute values from overlapping features coinciding with the same pixel, rather that take the last value. This is useful, for instance, when calculating a point density raster. For example, let's read the plants.shp and reserve.shp Shapefiles. These Shapefiles contain a point layer of rare plants observations and a polygon with the borders of Negev Mountains Nature Reserve, the largest nature reserve in Israel, respectively:\nplants = st_read(\"plants.shp\") reserve = st_read(\"reserve.shp\")\nWe will also reproject both layers to UTM, so that all pixels correspond to equal area size, which is more appropriate for density calculations:\nplants = st_transform(plants, 32636) reserve = st_transform(reserve, 32636)\nFigure 10.9 shows both layers—the polygonal layer reserve and the point layer plants:\nplot(st_geometry(reserve)) plot(st_geometry(plants), add = TRUE, col = \"red\")\nFigure 10.9: Nature reserve and rare plants observations\nHow can we calculate a raster expressing the density of plants points across the nature reserve? First, we set up a empty template raster, where all pixels within the area of interest—the nature reserve—are given an initial value of zero. Note that we are hereby using r_avg as template, but, if necessary, we can prepare a new template with resolution of our choice, using st_as_stars (Section 9.2.1).\ntemplate = r_avg template[[1]][] = 0 template = template[reserve]\nFigure 10.10 shows the template:\nplot(template, text_values = TRUE, col = \"lightgrey\", reset = FALSE) ## Warning in plot.stars(template, text_values = TRUE, col = \"lightgrey\", reset ## = FALSE): breaks=\"quantile\" leads to a single class; maybe try breaks=\"equal\" ## instead? plot(st_geometry(reserve), add = TRUE)\nFigure 10.10: Template for calculating point density\nThen, we create a new attrubute named count, with a value of 1 for each plant observation:\nplants$count = 1\nFinally, we rasterize the \"count\" attribute into the template raster, with an additional option options=\"MERGE_ALG=ADD\" (Section 10.2.1). The \"MERGE_ALG=ADD\" option acts as a flag instructing st_rasterize to add up any overlapping attributes \"burned\" into the same pixel:\ns = st_rasterize(plants[, \"count\"], template, options = \"MERGE_ALG=ADD\")\nThe resulting raster values reflect the sum of the count attribute from all overlapping points per pixel, i.e., the number of rare plant observations per pixel (Figure 10.11):\nplot(s, text_values = TRUE, breaks = \"equal\", col = grey.colors(10, start = 0.6, rev = TRUE), reset = FALSE) plot(st_geometry(reserve), add = TRUE)\nFigure 10.11: Density (observations per pixel) of rare plants in the nature reserve\nThe raster coloring is almost uniform, because the distribution is highly skewed. Namely, there are a lot of pixels with zero count, and few pixels with high count (try running hist(s[[1]]) to see that). We can use a logarithmic scale to visualize the density pattern more clearly (Figure 10.12). Now we can see which parts of the reserve were characterized by higher occurence of rare plants.\nb = c(0, 10^(seq(0, 2.5, 0.5))) plot(s, breaks = b, reset = FALSE, col = hcl.colors(length(b)-1, \"Reds\", rev = TRUE)) plot(st_geometry(reserve), add = TRUE)\nFigure 10.12: Density (observations per pixel) of rare plants in the nature reserve, with a logarithmic scale\n\n\n15.3.4 Standardizing density units\nIn the observation density raster s (Section 10.2.3), the units of measurement are plants per pixel. Since a pixel happens to have an area of 0.86 km2��2 (see below), raster values are, in fact, counts per 0.86 km2��2.\nHow do we know that the area of a pixel is 0.86 km2��2? We can apply the st_area function to calculate pixel area sizes. Recall that, when applied on a vector layer, st_area returns a vectot of units values, with the corresponding areas of all geometries (Section 8.3.2.2). When st_area is applied on a raster, it returns a raster of area values per pixel:\na = st_area(s) a ## stars object with 2 dimensions and 1 attribute ## attribute(s): ##    area [m^2]     ##  Min.   :858040   ##  1st Qu.:858040   ##  Median :858040   ##  Mean   :858040   ##  3rd Qu.:858040   ##  Max.   :858040   ## dimension(s): ##   from to  offset    delta                refsys point values x/y ## x    1 44  645574  926.305 WGS 84 / UTM zone 36N FALSE   NULL [x] ## y    1 54 3395217 -926.305 WGS 84 / UTM zone 36N FALSE   NULL [y]\nAs usual, the area values are given in CRS units (in this case, m2�2). We can transform the matrix with the entire raster values (a[[1]]) from m2�2 to km2��2 units, using set_units (Section 8.3.2.2):\nlibrary(units) a[[1]] = set_units(a[[1]], \"km^2\") a ## stars object with 2 dimensions and 1 attribute ## attribute(s): ##   area [km^2]    ##  Min.   :0.858   ##  1st Qu.:0.858   ##  Median :0.858   ##  Mean   :0.858   ##  3rd Qu.:0.858   ##  Max.   :0.858   ## dimension(s): ##   from to  offset    delta                refsys point values x/y ## x    1 44  645574  926.305 WGS 84 / UTM zone 36N FALSE   NULL [x] ## y    1 54 3395217 -926.305 WGS 84 / UTM zone 36N FALSE   NULL [y]\nFinally, we can examine one of the raster values. It doesn't matter which one, since all pixels have exactly the same area. For example, the following expression gives the top-left corner value from the stars values matrix:\na[[1]][1,1] ## 0.8580405 [km^2]\nLet's return to the density raster s. It is more convenient to work with counts per standard area unit, such as plants per 1 km2��2. To do that, we can divide the plants count raster s, by pixel area raster a:\ns = s / a names(s) = \"density\"\nThe result is a raster with counts per unit area, in this case plant observations per km2:\ns ## stars object with 2 dimensions and 1 attribute ## attribute(s): ##  density [1/km^2]   ##  Min.   :  0.0000   ##  1st Qu.:  0.0000   ##  Median :  0.0000   ##  Mean   :  0.8035   ##  3rd Qu.:  0.0000   ##  Max.   :269.2181   ##  NA's   :1191       ## dimension(s): ##   from to  offset    delta                refsys point values x/y ## x    1 44  645574  926.305 WGS 84 / UTM zone 36N FALSE   NULL [x] ## y    1 54 3395217 -926.305 WGS 84 / UTM zone 36N FALSE   NULL [y]\nThe result is shown in Figure 10.13:\nb = c(0, 10^(seq(0, 2.5, 0.5))) plot(s, breaks = b, reset = FALSE, col = hcl.colors(length(b)-1, \"Reds\", rev = TRUE)) plot(st_geometry(reserve), add = TRUE)\nFigure 10.13: Density (observations per km2��2) of rare plants in the nature reserve, with a logarithmic scale\nWe now move on to the opposite operation, converting a raster to a vector layer. As we will see, there are three variants of this conversion:\n\nRaster to polygons (Section 10.3)\nRaster to points (Section 10.4)\nRaster to contour (Section 10.6)"
  },
  {
    "objectID": "ch10.html#raster-polygons",
    "href": "ch10.html#raster-polygons",
    "title": "15  Combining rasters and vector layers",
    "section": "15.4 Raster → Polygons",
    "text": "15.4 Raster → Polygons\n\n15.4.1 Raster to polygons conversion\nThe st_as_sf function makes the raster to polygons conversion, when using the (default) as_points=FALSE argument. The st_as_sf function ignores pixels that have a NA values in all layers35. A useful option merge=TRUE dissolves all adjacent polygons that have the same raster value (in the first layer) into a single feature. The dissolve algorithm can be specified with the connect8 parameter: 4d, which is the default (connect8=FALSE) or 8d (connect8=TRUE). The attribute table of the polygon layer contains the raster values—with a separate attribute for each layer.\nFor example, let's take a small subset of the raster r:\nu = r[, 200:202, 100:102, 1:2]\nWe will replace some of the pixel values with NA:\nu[[1]][2,3,] = NA u[[1]][3,2,1] = NA\nand round the values to one digit:\nu = round(u, 1)\nThe resulting small raster u is visualized in Figure 10.14. Note that the raster has one pixel with NA in both layers, and another pixel with NA in the first layer only.\nplot(round(u, 2), text_values = TRUE, col = hcl.colors(4, \"Spectral\"))\nFigure 10.14: Sample raster\nNow, let's try using st_as_sf to transform the small raster u to a polygon layer p:\np = st_as_sf(u)\nThe resulting polygonal layer p has eight polygons, even though the raster u had nine pixels. This is because one of the pixels had NA in all layers, and therefore it was not converted to a polygon:\np ## Simple feature collection with 8 features and 2 fields ## Geometry type: POLYGON ## Dimension:     XY ## Bounding box:  xmin: 727089 ymin: 3610119 xmax: 729867.9 ymax: 3612898 ## Projected CRS: WGS 84 / UTM zone 36N ##   2000-02-01 2000-03-01                       geometry ## 1        0.7        0.7 POLYGON ((727089 3612898, 7... ## 2        0.7        0.8 POLYGON ((728015.3 3612898,... ## 3        0.6        0.7 POLYGON ((728941.6 3612898,... ## 4        0.8        0.9 POLYGON ((727089 3611972, 7... ## 5        0.7        0.8 POLYGON ((728015.3 3611972,... ## 6         NA        0.7 POLYGON ((728941.6 3611972,... ## 7        0.8        0.8 POLYGON ((727089 3611046, 7... ## 8        0.7        0.8 POLYGON ((728941.6 3611046,...\nThe resulting layer p, which has eight features and two attributes, is shown in Figure 10.15:\nplot(p)\nFigure 10.15: Polygon layer created from a raster\nLet's try the merge=TRUE option. Since merging is only affected by the first layer, it makes sense to transform each of the layers separately:\np1 = st_as_sf(u[,,,1], merge = TRUE) p2 = st_as_sf(u[,,,2], merge = TRUE)\nAs a result, we have two separate polygonal layers p1 and p2:\np1 ## Simple feature collection with 4 features and 1 field ## Geometry type: POLYGON ## Dimension:     XY ## Bounding box:  xmin: 727089 ymin: 3610119 xmax: 729867.9 ymax: 3612898 ## Projected CRS: WGS 84 / UTM zone 36N ##   NDVI                       geometry ## 1  0.7 POLYGON ((727089 3612898, 7... ## 2  0.6 POLYGON ((728941.6 3612898,... ## 3  0.8 POLYGON ((727089 3611972, 7... ## 4  0.7 POLYGON ((728941.6 3611046,...\np2 ## Simple feature collection with 6 features and 1 field ## Geometry type: POLYGON ## Dimension:     XY ## Bounding box:  xmin: 727089 ymin: 3610119 xmax: 729867.9 ymax: 3612898 ## Projected CRS: WGS 84 / UTM zone 36N ##   NDVI                       geometry ## 1  0.7 POLYGON ((727089 3612898, 7... ## 2  0.8 POLYGON ((728015.3 3612898,... ## 3  0.7 POLYGON ((728941.6 3612898,... ## 4  0.9 POLYGON ((727089 3611972, 7... ## 5  0.8 POLYGON ((727089 3611046, 7... ## 6  0.8 POLYGON ((728941.6 3611046,...\nFigure 10.16 displays both p1 and p2:\nplot(p1, main = paste0(colnames(p)[1], \" (\", nrow(p1), \")\")) plot(p2, main = paste0(colnames(p)[2], \" (\", nrow(p2), \")\"))\nFigure 10.16: Polygon layer created from a raster\n\nDo you think there will there be a difference if we use connect8=TRUE?\n\n\n\n15.4.2 Segmentation\nOne example of a use case of raster to polygon conversion is the delineation of distinct inter-connected groups of pixels, sharing the same (or a similar) value. This operationis also known as segmentation. In it's simplest form—detecting segments with exactly the same value—segmentation can be done using st_as_sf with merge=TRUE.\nFor example, we can derive segments of NDVI&gt;0.2 in the reclassified NDVI raster l_rec_focal from Section 9.4.3. First, let's recreate the l_rec_focal raster:\nl = read_stars(\"landsat_04_10_2000.tif\") red = l[,,,3,drop=TRUE] nir = l[,,,4,drop=TRUE] ndvi = (nir - red) / (nir + red) names(ndvi) = \"NDVI\" l_rec = ndvi l_rec[l_rec &lt; 0.2] = 0 l_rec[l_rec &gt;= 0.2] = 1 get_neighbors = function(m, pos) {   i = (pos[1]-1):(pos[1]+1)   j = (pos[2]-1):(pos[2]+1)   as.vector(t(m[i, j])) } focal2 = function(r, fun, ...) {   template = r   input = t(template[[1]])   output = matrix(NA, nrow = nrow(input), ncol = ncol(input))   for(i in 2:(nrow(input) - 1)) {     for(j in 2:(ncol(input) - 1)) {       v = get_neighbors(input, c(i, j))       output[i, j] = fun(v, ...)     }   }   template[[1]] = t(output)   return(template) } l_rec_focal = focal2(l_rec, max)\nSegments in the l_rec_focal raster represent distinct continuous areas with NDVI&gt;0.2����&gt;0.2. To detect them, we can convert the raster to polygons using st_as_sf with the merge=TRUE:\npol = st_as_sf(l_rec_focal, merge = TRUE)\nThen, we filter only those segments where raster value was 1:\npol = pol[pol$NDVI == 1, ]\nThe result is a polygon layer where each feature represents a single continuous area where NDVI&gt;0.2����&gt;0.2:\npol ## Simple feature collection with 536 features and 1 field ## Geometry type: POLYGON ## Dimension:     XY ## Bounding box:  xmin: 663975 ymin: 3459405 xmax: 687645 ymax: 3488145 ## Projected CRS: WGS 84 / UTM zone 36N ## First 10 features: ##    NDVI                       geometry ## 1     1 POLYGON ((675615 3488145, 6... ## 2     1 POLYGON ((676455 3488145, 6... ## 5     1 POLYGON ((681945 3488145, 6... ## 8     1 POLYGON ((684765 3488145, 6... ## 12    1 POLYGON ((687195 3488145, 6... ## 13    1 POLYGON ((686985 3488085, 6... ## 17    1 POLYGON ((686505 3488145, 6... ## 18    1 POLYGON ((665865 3488145, 6... ## 23    1 POLYGON ((685545 3488145, 6... ## 26    1 POLYGON ((686835 3487905, 6...\nFigure 10.17 shows the segments on top of the NDVI raster:\nplot(ndvi, breaks = \"equal\", col = hcl.colors(11, \"Spectral\"), reset = FALSE) plot(st_geometry(pol), add = TRUE)\nFigure 10.17: Segments with NDVI&gt;0.2\n\nWhat is the exact number of segments in pol? If we ran the st_as_sf function on l_rec instead of l_rec_focal, do you think the number of segments would be higher or lower?"
  },
  {
    "objectID": "ch10.html#raster-points",
    "href": "ch10.html#raster-points",
    "title": "15  Combining rasters and vector layers",
    "section": "15.5 Raster → Points",
    "text": "15.5 Raster → Points\nThe Raster→Points transformation is done using function st_as_sf with the as_points=TRUE option. Pixel centers—except for pixels with NA in all layers—become points. Just like in a conversion to polygons (Section 10.3), the attribute table of the resulting point layer contains the raster values.\nFor example, here is how we can transform the small raster u to a point layer:\np = st_as_sf(u, as_points = TRUE)\nOnce again (Section 10.3), the resulting point layer p has eight points, even though the raster has nine pixels, because one of the pixels had NA in all layers and therefore was not converted to a point36:\np ## Simple feature collection with 8 features and 2 fields ## Geometry type: POINT ## Dimension:     XY ## Bounding box:  xmin: 727552.1 ymin: 3610583 xmax: 729404.8 ymax: 3612435 ## Projected CRS: WGS 84 / UTM zone 36N ##   2000-02-01 2000-03-01                 geometry ## 1        0.7        0.7 POINT (727552.1 3612435) ## 2        0.7        0.8 POINT (728478.4 3612435) ## 3        0.6        0.7 POINT (729404.8 3612435) ## 4        0.8        0.9 POINT (727552.1 3611509) ## 5        0.7        0.8 POINT (728478.4 3611509) ## 6         NA        0.7 POINT (729404.8 3611509) ## 7        0.8        0.8 POINT (727552.1 3610583) ## 8        0.7        0.8 POINT (729404.8 3610583)\nThe point layer p is shown in Figure 10.18:\nplot(p, pch = 16, cex = 3)\nFigure 10.18: Point layer created from a raster"
  },
  {
    "objectID": "ch10.html#distance-to-nearest-point",
    "href": "ch10.html#distance-to-nearest-point",
    "title": "15  Combining rasters and vector layers",
    "section": "15.6 Distance to nearest point",
    "text": "15.6 Distance to nearest point\nAnother example of a spatial operator involving a raster and a vector layer is the calculation of a raster of distances to nearest point. For example, we may be interested in mapping the distances to the nearest meteorological stations in Israel, to evaluate where coverage is too sparse and reliable meteorological data are missing.\nWe are already familiar with the st_distance function for calculating distances (Section 8.3.2.3). However, st_distance expects two vector layers as input, not a vector layer and a raster. Therefore, to calculate a raster of distances from nearest point, we first need to convert the \"template\" raster to a point later (Section 10.4). In this example, our template for the distances raster, hereby named grid, is the 926×926926×926 m2�2 raster r_avg (though we could use any other raster template). The NDVI attribute is discarded, since it is irrelevant for the distance calculation:\ngrid = st_as_sf(r_avg, as_points = TRUE) grid$NDVI = NULL grid ## Simple feature collection with 32858 features and 0 fields ## Geometry type: POINT ## Dimension:     XY ## Bounding box:  xmin: 616395.6 ymin: 3262292 xmax: 770162.2 ymax: 3692097 ## Projected CRS: WGS 84 / UTM zone 36N ## First 10 features: ##                    geometry ## 1  POINT (757193.9 3691171) ## 2  POINT (758120.2 3691171) ## 3  POINT (759046.5 3691171) ## 4  POINT (753488.7 3690245) ## 5    POINT (754415 3690245) ## 6  POINT (755341.3 3690245) ## 7  POINT (756267.6 3690245) ## 8  POINT (757193.9 3690245) ## 9  POINT (758120.2 3690245) ## 10 POINT (759046.5 3690245)\nLet's also import the point layer of meteorological stations (Section 7.4):\nrainfall = read.csv(\"rainfall.csv\") rainfall = st_as_sf(rainfall, coords = c(\"x_utm\", \"y_utm\"), crs = 32636)\nNow we can use st_distance to calculate pairwise distances between every grid point (grid) and meteorological station (rainfall):\ndistance = st_distance(grid, rainfall)\nThe result is a distance matrix named distance. Its rows correspond to grid points and its columns correspond to rainfall points. This is a very large matrix, due to the large number of grid points:\ndim(distance) ## [1] 32858   169\nIn this example, we are interested in the minimal distance, per grid point, among the distances to the 169 meteorological stations. Therefore we apply the min function on the distance matrix rows (Section 4.5), which refer to grid points. The result is a numeric vector of minimal distances, which we assign as an attribute in grid:\ngrid$distance = apply(distance, 1, min)\nIt is also convenient to convert the distances from from m� to km��. Since apply returns numeric, thus \"losing\" the units of measurement, we first need to convert the distances to units and only then make the conversion:\ngrid$distance = set_units(grid$distance, \"m\") grid$distance = set_units(grid$distance, \"km\")\nThe grid point layer now has a distance attribute, with the distance to nearest meteorological station, in km��:\ngrid ## Simple feature collection with 32858 features and 1 field ## Geometry type: POINT ## Dimension:     XY ## Bounding box:  xmin: 616395.6 ymin: 3262292 xmax: 770162.2 ymax: 3692097 ## Projected CRS: WGS 84 / UTM zone 36N ## First 10 features: ##                    geometry      distance ## 1  POINT (757193.9 3691171) 15.72760 [km] ## 2  POINT (758120.2 3691171) 16.40858 [km] ## 3  POINT (759046.5 3691171) 17.11260 [km] ## 4  POINT (753488.7 3690245) 12.55666 [km] ## 5    POINT (754415 3690245) 13.14112 [km] ## 6  POINT (755341.3 3690245) 13.76317 [km] ## 7  POINT (756267.6 3690245) 14.41792 [km] ## 8  POINT (757193.9 3690245) 15.10113 [km] ## 9  POINT (758120.2 3690245) 15.80911 [km] ## 10 POINT (759046.5 3690245) 16.53868 [km]\nThe result is shown in Figure 10.19:\nplot(grid)\nFigure 10.19: Distance to nearest meteorological station (point grid)\nTo have the results back as a raster, we can rasterize the points (Section 10.2). We use the same raster which we started with as template—this guarantees that every point corresponds exactly to one pixel:\ndistance = st_rasterize(grid[, \"distance\"], r_avg)\nThe final raster with distances to the nearest stations is shown in Figure 10.20. The locations of the meteorological stations and contour lines are shown on top of the raster, to emphasize the distance gradient:\nplot(distance, breaks = \"equal\", col = terrain.colors(10), reset = FALSE) plot(st_geometry(rainfall), add = TRUE, pch = 3, cex = 0.4) contour(distance, add = TRUE)\nFigure 10.20: Distance to nearest meteorological station (raster)"
  },
  {
    "objectID": "ch10.html#raster-lines-contours",
    "href": "ch10.html#raster-lines-contours",
    "title": "15  Combining rasters and vector layers",
    "section": "15.7 Raster → Lines (contours)",
    "text": "15.7 Raster → Lines (contours)\nWe already saw how a raster can be converted to polygons (Section 10.3) or to points (Section 10.4), which simply results in a layer of cell outlines or centroids, respectively. Another common transformation is to calculate contour lines of equal raster values.\nTo illustrate contour lines calculation, we will reconstruct the Haifa DEM at 90 m� resolution (Section 9.3):\ndem1 = read_stars(\"_book/data/srtm_43_06.tif\") dem2 = read_stars(\"_book/data/srtm_44_06.tif\") dem = st_mosaic(dem1, dem2) names(dem) = \"elevation\" dem = dem[, 5687:6287, 2348:2948] dem = st_warp(src = dem, crs = 32636, cellsize = 90)\nRaster contours can be calculated using the st_contour function. The function can accept a breaks argument, which determines the break points where contours are generated. Another parameter named contour_lines determines whether contours are returned as a line layer (TRUE) or polygon layer (FALSE, the default).\nTo decide on breaks values we are interested in, it is useful to examine the range of values in the raster:\nrange(dem[[1]], na.rm = TRUE) ## [1] -14 541\nAccording to the above result, we will use breaks from -100 to 600 meters, in steps of 50 meters:\nb = seq(-100, 600, 50) dem_contour = st_contour(dem, breaks = b, contour_lines = TRUE)\nThe result, dem_contour, is a LINESTRING layer with one feature per contour line:\ndem_contour ## Simple feature collection with 612 features and 1 field ## Geometry type: LINESTRING ## Dimension:     XY ## Bounding box:  xmin: 678607.1 ymin: 3602252 xmax: 710107.1 ymax: 3658412 ## Projected CRS: WGS 84 / UTM zone 36N ## First 10 features: ##    elevation                       geometry ## 1        150 LINESTRING (704500.1 365832... ## 2        150 LINESTRING (704680.1 365832... ## 3        200 LINESTRING (707113.3 365841... ## 4        100 LINESTRING (701332.1 365823... ## 5        350 LINESTRING (708883.5 365841... ## 6        150 LINESTRING (706102.1 365802... ## 7        100 LINESTRING (702198.3 365823... ## 8         50 LINESTRING (700342.1 365746... ## 9        200 LINESTRING (704932.1 365719... ## 10       450 LINESTRING (709117.1 365728...\nThe contours are shown in Figure 10.21:\nplot(dem, breaks = b, col = terrain.colors(length(b)-1), key.pos = 4, reset = FALSE) plot(st_geometry(dem_contour), add = TRUE)\nFigure 10.21: Contour lines"
  },
  {
    "objectID": "ch10.html#extracting-raster-values",
    "href": "ch10.html#extracting-raster-values",
    "title": "15  Combining rasters and vector layers",
    "section": "15.8 Extracting raster values",
    "text": "15.8 Extracting raster values\n\n15.8.1 Introduction\nIt is often necessary to \"extract\" raster values according to overlapping vector features, such as points, lines or polygons (Figure 10.22). For example, given an NDVI raster, we may be interested to calculate the NDVI value observed in particular point locations, or the average NDVI observed over an administrative area polygon.\nFigure 10.22: Extracting raster values (http://rpubs.com/etiennebr/visualraster)\nWhen extracting values to liner or polygons, it is common to summarize the values that the geometry intersects with, using a function such as mean. That way, the values can \"fit\" into a new attribute or attributes. Extracting to points is simpler, since each geometry corresponds to one pixel, so there is nothing to summarize.\nExtracting to points can be accompished with the st_extract function, while extracting to lines or to polygons can be accomplished with the aggregate function. In the next few sections, we will see examples of the most common extract scenarios:\n\nFrom single-band raster to points (Section 10.7.2)\nFrom multi-band raster to points (Section 10.7.3)\nFrom single-band raster to polygons (Section 10.7.4)\nFrom multi-band raster to polygons (Section 10.7.5)\n\n\n\n15.8.2 Extracting to points: single-band\n\n15.8.2.1 NDVI in meteorological stations\nRaster values can be extracted to points using the st_extract function. The function accepts:\n\nx—a stars raster\npts—a sf point layer\n\nand returns a new sf layer, with an additional attribute containing the values from x from the matching pixel for each point in pts.\nFor example, we can determine the average NDVI (r_avg) in the pixel where each meteorological station (rainfall) falls in, as follows:\nx = st_extract(r_avg, rainfall) x ## Simple feature collection with 169 features and 1 field ## Geometry type: POINT ## Dimension:     XY ## Bounding box:  xmin: 629301.4 ymin: 3270290 xmax: 761589.2 ymax: 3681163 ## Projected CRS: WGS 84 / UTM zone 36N ## First 10 features: ##         NDVI                 geometry ## 1  0.4357747 POINT (696533.1 3660837) ## 2  0.3544532 POINT (697119.1 3656748) ## 3  0.3196541 POINT (696509.3 3652434) ## 4  0.4238691 POINT (696541.7 3641332) ## 5  0.3745760 POINT (697875.3 3630156) ## 6         NA POINT (687006.2 3633330) ## 7  0.4580601 POINT (689553.7 3626282) ## 8  0.3732974 POINT (694694.5 3624388) ## 9  0.5153764 POINT (686489.5 3619716) ## 10 0.4521039 POINT (683148.4 3616846)\nThe extracted values can be attached to the rainfall layer, using assignment:\nrainfall$NDVI = x$NDVI\nAs a result, the rainfall layer now has an additional attribuite named NDVI:\nrainfall ## Simple feature collection with 169 features and 13 fields ## Geometry type: POINT ## Dimension:     XY ## Bounding box:  xmin: 629301.4 ymin: 3270290 xmax: 761589.2 ymax: 3681163 ## Projected CRS: WGS 84 / UTM zone 36N ## First 10 features: ##       num altitude sep oct nov dec jan feb mar apr may              name ## 1  110050       30 1.2  33  90 117 135 102  61  20 6.7 Kfar Rosh Hanikra ## 2  110351       35 2.3  34  86 121 144 106  62  23 4.5              Saar ## 3  110502       20 2.7  29  89 131 158 109  62  24 3.8             Evron ## 4  111001       10 2.9  32  91 137 152 113  61  21 4.8       Kfar Masrik ## 5  111650       25 1.0  27  78 128 136 108  59  21 4.7     Kfar Hamakabi ## 6  120202        5 1.5  27  80 127 136  95  49  19 2.7        Haifa Port ## 7  120630      450 1.9  36  93 161 166 128  71  21 4.9  Haifa University ## 8  120750       30 1.6  31  91 163 170 146  76  22 4.9             Yagur ## 9  120870      210 1.1  32  93 147 147 109  61  16 4.3        Nir Etzyon ## 10 121051       20 1.8  32  85 147 142 102  56  13 4.5         En Carmel ##                    geometry      NDVI ## 1  POINT (696533.1 3660837) 0.4357747 ## 2  POINT (697119.1 3656748) 0.3544532 ## 3  POINT (696509.3 3652434) 0.3196541 ## 4  POINT (696541.7 3641332) 0.4238691 ## 5  POINT (697875.3 3630156) 0.3745760 ## 6  POINT (687006.2 3633330)        NA ## 7  POINT (689553.7 3626282) 0.4580601 ## 8  POINT (694694.5 3624388) 0.3732974 ## 9  POINT (686489.5 3619716) 0.5153764 ## 10 POINT (683148.4 3616846) 0.4521039\nThe attribute values are shown as text labels in Figure 10.23:\nplot(r_avg, breaks = \"equal\", col = hcl.colors(11, \"Spectral\"), reset = FALSE) text(st_coordinates(rainfall), as.character(round(rainfall$NDVI, 2)), cex = 0.5)\nFigure 10.23: Raster values extracted to points\nNow that we know the average NDVI for each meteorological station location, we can examine, for instance, the association between NDVI and rainfall in December (Figure 10.24):\nplot(rainfall$dec, rainfall$NDVI, xlab = \"Rainfall (mm)\", ylab = \"NDVI\")\nFigure 10.24: Average NDVI as function of average rainfall in December\n\nWhat is the number and proportion of rainfall points that have an NA value in the NDVI attribute? Why did those stations get NA?\n\n\n\n\n15.8.3 Extracting to points: multi-band\nAs another example, we can extract the NDVI values for different dates from the multi-band raster r. For simplicity, let's create a subset rainfall1, with just three meteorological stations from the rainfall layer:\nsel = c(\"Horashim\", \"Beer Sheva\", \"Yotveta\") rainfall1 = rainfall[rainfall$name %in% sel, ] rainfall1 ## Simple feature collection with 3 features and 13 fields ## Geometry type: POINT ## Dimension:     XY ## Bounding box:  xmin: 671364.1 ymin: 3307819 xmax: 717491.2 ymax: 3648872 ## Projected CRS: WGS 84 / UTM zone 36N ##        num altitude sep oct nov dec jan feb mar apr  may       name ## 77  212168      825 3.4  37 122 202 238 190 114  38 12.2   Horashim ## 141 251690      280 0.5   9  18  38  48  40  29   9  3.6 Beer Sheva ## 168 345005       80 0.4   2   2   6   5   4   5   2  0.4    Yotveta ##                     geometry      NDVI ## 77  POINT (717491.2 3648872) 0.5041712 ## 141 POINT (671364.1 3458877) 0.1699691 ## 168 POINT (700626.3 3307819)        NA\nNow, let's extract the NDVI values from a subset with the first five bands of r, into rainfall1:\nrainfall1 = st_extract(r[,,,1:5], rainfall1)\nThis time, since extraction from a multi-band raster took place, the result is a stars object and not an sf object. Moreover, this is a special type of a stars object, with one dimension for the vector geometries (geometry) and another dimension for the raster layers (time):\nrainfall1 ## stars object with 2 dimensions and 1 attribute ## attribute(s): ##      NDVI         ##  Min.   :0.1092   ##  1st Qu.:0.1300   ##  Median :0.1564   ##  Mean   :0.2614   ##  3rd Qu.:0.4442   ##  Max.   :0.5396   ##  NA's   :1        ## dimension(s): ##          from to offset delta                refsys point ## geometry    1  3     NA    NA WGS 84 / UTM zone 36N  TRUE ## time        1  5     NA    NA                  Date    NA ##                                                                          values ## geometry POINT (717491 3648872), POINT (671364 3458877), POINT (700626 3307819) ## time                                                  2000-02-01,...,2000-06-01\nTo work with the data, it is often more convenient to transform the stars object to an sf layer. This can be done with st_as_sf:\nrainfall1 = st_as_sf(rainfall1) rainfall1 ## Simple feature collection with 3 features and 5 fields ## Geometry type: POINT ## Dimension:     XY ## Bounding box:  xmin: 671364.1 ymin: 3307819 xmax: 717491.2 ymax: 3648872 ## Projected CRS: WGS 84 / UTM zone 36N ##   2000-02-01 2000-03-01 2000-04-01 2000-05-01 2000-06-01 ## 1     0.4314     0.4485     0.5396     0.5234     0.4772 ## 2         NA     0.1587     0.1618     0.1541     0.1489 ## 3     0.1440     0.1254     0.1251     0.1122     0.1092 ##                   geometry ## 1 POINT (717491.2 3648872) ## 2 POINT (671364.1 3458877) ## 3 POINT (700626.3 3307819)\nIn case we are interested only in the extracted values, not the geometries, the latter can be discarded with st_drop_geometry:\nrainfall1 = st_drop_geometry(rainfall1) rainfall1 ##   2000-02-01 2000-03-01 2000-04-01 2000-05-01 2000-06-01 ## 1     0.4314     0.4485     0.5396     0.5234     0.4772 ## 2         NA     0.1587     0.1618     0.1541     0.1489 ## 3     0.1440     0.1254     0.1251     0.1122     0.1092\nFinally, we can rely on the fact that row and column order matches the vector layer feature and raster layer order, respectively, to assign names. For example, rainfall1 rows correspond to the selected station names, therefore we can assign the station names into rownames:\nrownames(rainfall1) = sel rainfall1 ##            2000-02-01 2000-03-01 2000-04-01 2000-05-01 2000-06-01 ## Horashim       0.4314     0.4485     0.5396     0.5234     0.4772 ## Beer Sheva         NA     0.1587     0.1618     0.1541     0.1489 ## Yotveta        0.1440     0.1254     0.1251     0.1122     0.1092\n\nColumns in rainfall1 are already named according to image dates. Where did these names come from?\n\nThe analogous operation in ArcGIS is the \"Extract Multi Values to Points\" tool (Figures 10.25–10.26).\nFigure 10.25: \"Extract Multi Values to Points\" tool in ArcGIS\nFigure 10.26: \"Extract Multi Values to Points\" tool in ArcGIS\n\n\n15.8.4 Extracting to polygons: single-band\nExtracting raster values to polygons (or to lines) is different from extracting to points. When extracting raster values to polygons (or to lines), each vector feature is potentially associated with more than one pixel value (Figure 10.22). Moreover, the number of pixels may vary between features. For example, a large polygon may cover many more pixels than a small polygon. Therefore, it is often convenient to summarize the raster values per polygon feauture by applying a function on them and obtaining a single number, such as the average. This is done by aggregating the raster based on a vector layer using function aggregate37.\nWhen summarizing raster values per polygon feature, the aggregate function accepts38:\n\nx—The stars raster to summarize\nby—An sf layer determining areas\nFUN—The function to be applied on each set of extracted pixel values from x\n\nFor example, we can calculate the average (mean) NDVI value (r_avg) per administrative area (nafot) as follows. Any additional arguments, such as na.rm=TRUE, are passed to the function, in this case so that mean excludes NA values:\nndvi_nafot = aggregate(r_avg, nafot, mean, na.rm = TRUE)\nThe result is, again (Section 10.7.3), a special type of a stars object, with just one dimension for the vector geometries:\nndvi_nafot ## stars object with 1 dimensions and 1 attribute ## attribute(s): ##      NDVI         ##  Min.   :0.1263   ##  1st Qu.:0.3376   ##  Median :0.3845   ##  Mean   :0.3554   ##  3rd Qu.:0.4001   ##  Max.   :0.4317   ## dimension(s): ##          from to offset delta                refsys point ## geometry    1 15     NA    NA WGS 84 / UTM zone 36N FALSE ##                                                                     values ## geometry POLYGON ((739780.1 3686007,...,...,POLYGON ((674110.3 3549169,...\nIt can be converted to the more familiar sf structure using st_as_sf:\nndvi_nafot = st_as_sf(ndvi_nafot)\nNow we have a polygon layer of the administrative areas, with an NDVI attribute containing the average NDVI value according to the r_avg raster:\nndvi_nafot ## Simple feature collection with 15 features and 1 field ## Geometry type: POLYGON ## Dimension:     XY ## Bounding box:  xmin: 620662.1 ymin: 3263494 xmax: 770624.4 ymax: 3691834 ## Projected CRS: WGS 84 / UTM zone 36N ## First 10 features: ##         NDVI                       geometry ## 1  0.4208889 POLYGON ((739780.1 3686007,... ## 2  0.3845475 POLYGON ((672273.8 3518394,... ## 3  0.3093089 POLYGON ((745560 3649860, 7... ## 4  0.3874892 POLYGON ((702283.1 3628046,... ## 5  0.4317472 POLYGON ((702725.9 3630513,... ## 6  0.4051780 POLYGON ((759304.4 3691202,... ## 7  0.3949261 POLYGON ((701391.6 3631170,... ## 8  0.4124200 POLYGON ((706537.1 3602188,... ## 9  0.3937370 POLYGON ((692687.3 3583974,... ## 10 0.3468470 POLYGON ((672841.5 3544808,...\nThe result can be visualized as follows (Figure 10.27):\nplot(ndvi_nafot, pal = function(x) hcl.colors(x, \"Spectral\"), reset = FALSE, key.pos = 4) text(st_coordinates(st_centroid(ndvi_nafot)), as.character(round(ndvi_nafot$NDVI, 2)))\nFigure 10.27: Average NDVI per \"Nafa\"\n\n\n15.8.5 Extracting to polygons: multi-band\nExtracting raster values from a multi-band raster works the same way as extracting from a multi-band raster. The difference is that the resulting stars object has two dimensions: one for the geometries and one for the input raster bands.\nFor example, the following expression calculates the average r value for each nafot feature over five months starting from 2000-02-01, based on the first five layers of r:\nndvi_nafot = aggregate(r[,,,1:5], nafot, mean, na.rm = TRUE) ndvi_nafot ## stars object with 2 dimensions and 1 attribute ## attribute(s): ##      NDVI         ##  Min.   :0.1083   ##  1st Qu.:0.3148   ##  Median :0.4408   ##  Mean   :0.4023   ##  3rd Qu.:0.4873   ##  Max.   :0.5906   ## dimension(s): ##          from to offset delta                refsys point ## geometry    1 15     NA    NA WGS 84 / UTM zone 36N FALSE ## time        1  5     NA    NA                  Date    NA ##                                                                     values ## geometry POLYGON ((739780.1 3686007,...,...,POLYGON ((674110.3 3549169,... ## time                                             2000-02-01,...,2000-06-01\nWhen transformed to an sf layer, the values extracted from each raster layer are placed in separate attributes:\nndvi_nafot = st_as_sf(ndvi_nafot) ndvi_nafot ## Simple feature collection with 15 features and 5 fields ## Geometry type: POLYGON ## Dimension:     XY ## Bounding box:  xmin: 620662.1 ymin: 3263494 xmax: 770624.4 ymax: 3691834 ## Projected CRS: WGS 84 / UTM zone 36N ## First 10 features: ##    2000-02-01 2000-03-01 2000-04-01 2000-05-01 2000-06-01 ## 1   0.5061847  0.5270644  0.5319248  0.4432705  0.3478298 ## 2   0.4692342  0.4604232  0.4568367  0.3757161  0.3096993 ## 3   0.5395486  0.5398781  0.4491296  0.3011687  0.2440450 ## 4   0.5616802  0.5826193  0.5475469  0.3978501  0.3062216 ## 5   0.4718739  0.4840480  0.4993016  0.4304506  0.3796581 ## 6   0.5518223  0.5851435  0.5905808  0.4905159  0.2675451 ## 7   0.4593590  0.4520991  0.4388301  0.3730517  0.3468060 ## 8   0.5061774  0.5087029  0.4809899  0.3971890  0.3509476 ## 9   0.5104205  0.4970106  0.4763348  0.3797437  0.3421975 ## 10  0.4709000  0.4675206  0.4407848  0.3536589  0.2844259 ##                          geometry ## 1  POLYGON ((739780.1 3686007,... ## 2  POLYGON ((672273.8 3518394,... ## 3  POLYGON ((745560 3649860, 7... ## 4  POLYGON ((702283.1 3628046,... ## 5  POLYGON ((702725.9 3630513,... ## 6  POLYGON ((759304.4 3691202,... ## 7  POLYGON ((701391.6 3631170,... ## 8  POLYGON ((706537.1 3602188,... ## 9  POLYGON ((692687.3 3583974,... ## 10 POLYGON ((672841.5 3544808,...\nThe result reflects the average NDVI, for the first five months in the MODIS NDVI series, for each \"Nafa\" (Figure 10.28):\nplot(ndvi_nafot, pal = function(x) hcl.colors(x, \"Spectral\"))\nFigure 10.28: NDVI per \"Nafa\" for five months"
  },
  {
    "objectID": "ch11.html#aims",
    "href": "ch11.html#aims",
    "title": "16  Processing spatio-temporal data",
    "section": "16.1 Aims",
    "text": "16.1 Aims\nOur aims in this chapter are:\n\nLearn to work with list objects\nPresent several characteristics of spatio-temporal data\nDemonstrate spatio-temporal data processing:\n\nAggregation of raster spatio-temporal data\nReshaping of vector trajectory data\n\n\nWe will use the following R packages:\n\nstars\ndplyr\ndata.table\nsf\nunits"
  },
  {
    "objectID": "ch11.html#lists",
    "href": "ch11.html#lists",
    "title": "16  Processing spatio-temporal data",
    "section": "16.2 Lists",
    "text": "16.2 Lists\n\n16.2.1 The list class\nA list is a collection of objects of any class. There are no restrictions as for the class and dimensions of each list element. Lists are therefore the most flexible of the base R data structures.\nA list can be created by passing individual objects to the list function. For example, the following expression creates a list object named x, which contains the three vectors c(1,3), c(4,5,6) and 8:\nx = list(c(1, 3), c(4, 5, 6), 8)\nUsing the class function we can make sure that x is indeed a list:\nclass(x) ## [1] \"list\"\nPrinting a list shows the internal objects it contains. In an unnamed list, the elements are marked with indices inside double square brackets ([[):\nx ## [[1]] ## [1] 1 3 ##  ## [[2]] ## [1] 4 5 6 ##  ## [[3]] ## [1] 8\nList element names can be accessed with names:\nnames(x) ## NULL\nand modified by assignment to names:\nnames(x) = c(\"a\", \"b\", \"c\")\nAs a result of the above expression, x is now a named list. When printing a named list, elements are marked with $ symbols followed by element names:\nx ## $a ## [1] 1 3 ##  ## $b ## [1] 4 5 6 ##  ## $c ## [1] 8\n\n\n16.2.2 List subsetting\nThe [ operator gives a list subset, as a new list. Numeric indices or element names (in case the list is named) can be used. For example, either of the following expression returns a list with just the first two elements of x:\nx[1:2] ## $a ## [1] 1 3 ##  ## $b ## [1] 4 5 6\nx[c(\"a\", \"b\")] ## $a ## [1] 1 3 ##  ## $b ## [1] 4 5 6\nEither of the following expressions returns a list with just the third element of x:\nx[3] ## $c ## [1] 8\nx[\"c\"] ## $c ## [1] 8\nThe [[ or $ operators return the contents of an individual list element (Figure 11.1). For example, either of the following expressions returns the second element of x, which is a numeric vector:\nx[[2]] ## [1] 4 5 6\nx[[\"b\"]] ## [1] 4 5 6\nx$b ## [1] 4 5 6\nNote that the [[ operator can be used with either numeric or character indices (in quotes), while the $ operator only works with character indices (without quotes).\nFigure 11.1: The difference between the [ and [[ operators when subsetting a list (http://r4ds.had.co.nz/vectors.html)\nList element names can be removed by assigning NULL to the names property:\nnames(x) = NULL\n\n\n16.2.3 The lapply function\nOne of the most useful functions when working with list objects is lapply. The lapply function is used to \"do something\" with each list element, getting back a matching list of results. The lapply function:\n\n\"Splits\" a list to individual elements\nCalls a function on each element\nCombines the results back to a new list\n\nThe lapply function is therefore conceptually similar to apply (Section 4.5), only operating on list elements, rather than the rows or the columns of a matrix or a data.frame (Figure 11.2).\nFigure 11.2: apply and lapply\nFor example, the following expressions apply the mean (Figure 11.2), range and is.na functions on each element in the list x. In each case, the returned object from lapply a new list with the results:\nlapply(x, mean) ## [[1]] ## [1] 2 ##  ## [[2]] ## [1] 5 ##  ## [[3]] ## [1] 8\nlapply(x, range) ## [[1]] ## [1] 1 3 ##  ## [[2]] ## [1] 4 6 ##  ## [[3]] ## [1] 8 8\nlapply(x, is.na) ## [[1]] ## [1] FALSE FALSE ##  ## [[2]] ## [1] FALSE FALSE FALSE ##  ## [[3]] ## [1] FALSE\n\n\n16.2.4 The split function\nThe split function can be used to split a data.frame to a list of data.frame subsets, according to unique values of the given vector (Figure 11.3).\nFigure 11.3: The split function\nFor the next example, let's ctreate a small data.frame named dat:\ndat = data.frame(   y = c(3, 5, 1, 4, 5),   g = c(\"f\", \"m\", \"f\", \"f\", \"m\") ) dat ##   y g ## 1 3 f ## 2 5 m ## 3 1 f ## 4 4 f ## 5 5 m\nand a vector named a which matches the number of rows in dat:\na = c(\"a\", \"a\", \"b\", \"a\", \"b\") a ## [1] \"a\" \"a\" \"b\" \"a\" \"b\"\nUsing split, we can \"split\" dat to a list of subsets, with the corresponding rows to all levels in the vector a (Figure 11.3):\nsplit(dat, a) ## $a ##   y g ## 1 3 f ## 2 5 m ## 4 4 f ##  ## $b ##   y g ## 3 1 f ## 5 5 m\n\nWhat type of object will be the result of split(dat, dat$y)? How many elements will it have?\n\n\n\n16.2.5 The do.call function\nThe do.call function can be used to pass a list of function arguments to another function. For example, instead of the following hypothetical function call of f with arguments a, b, c and d:\nf(a, b, c, d)\nwe can use the following equivalent expression with do.call:\ndo.call(f, list(a, b, c, d))\nThe do.call function is useful when we want to call a function with a large or variable number of arguments—passed as list elements—without having to specify them by name.\nFor example, suppose we want to combine all internal vectors in the list x to a single vector, using the c function. Instead of specifying each and every element:\nc(x[[1]], x[[2]], x[[3]]) ## [1] 1 3 4 5 6 8\nwe can use do.call:\ndo.call(c, x) ## [1] 1 3 4 5 6 8\nNow that we know how to work with list objects, we move on to defining spatio-temporal data (Section 11.2) and using the list class for spatio-temporal data processing (Sections 11.3–11.4)."
  },
  {
    "objectID": "ch11.html#spatio-temporal-data",
    "href": "ch11.html#spatio-temporal-data",
    "title": "16  Processing spatio-temporal data",
    "section": "16.3 Spatio-temporal data",
    "text": "16.3 Spatio-temporal data\nIt can be argued that all data are spatio-temporal, since they were measured in certain location(s) and time(s), even if the locations and times were not recorded and/or are irrelevant for analysis. However, we usually refer to data as spatio-temporal when the locations and times of observation were recorded and are relevant for analysis. Here are some examples of spatio-temporal data:\n\nTime-series of satellite images\nTemperature measurements in meteorological stations over time\nVoting results in administrative units during several election campaigns\nMovement tracks of people or animals, with or without associated measurements (heart rate, activity type, etc.)\nSpatial pattern of epidemic disease outbreak\nVolcanic eruption event locations over time\n\nMethods and tools for processing and analyzing spatio-temporal data are generally less developed than methods for working with spatial or temporal data. R has numeroud specialized packages for analyzing spatio-temporal data. The important ones are listed in the Handling and Analyzing Spatio-Temporal Data task view.\nConceptually, we can classify types of spatio-temporal data according to the arrangement of observations in space-time (Figure 11.4). For example, spatio-temporal data may form a regular or irregular \"grid,\" depending on whether the observations were repeatedly measured at the same locations and times or whether each observations has a unique space-time \"stamp.\" Trajectories are distinguised by the fact that observations usually refer to an individual object (or few objects) observed in consecutive times.\nFigure 11.4: Space-time dataset types (https://www.jstatsoft.org/article/view/v051i07)\nFor example, meteorological data (Figure 11.5) and satellite data (Figure 11.6) form regular grids of points or pixels collectively measured at the same time.\nFigure 11.5: Grid layout: PM point measurements (https://edzer.github.io/UseR2016/)\nFigure 11.6: Grid layout: Satellite image time series (Appel, Marius, and Edzer Pebesma. \"On-Demand Processing of Data Cubes from Satellite Image Collections with the gdalcubes Library.\" Data 4, no. 3 (2019): 92.)\nTweet (Figure 11.7) and disease case (Figure 11.8) locations comprise irregular grids, since each observation is associated with a unique location and timestamp.\nFigure 11.7: Irregular layout: Tweets (http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9671.2012.01359.x/abstract)\nFigure 11.8: Irregular layout: Coral disease cases (http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0004993)\nFlickr image records per user over time (Figure 11.9) and storm tracks (Figure 11.10) are examples of trajectory data.\nFigure 11.9: Trajectory: Flickr user paths\nFigure 11.10: Trajectory: Storm paths (https://www.r-spatial.org/r/2017/08/28/nest.html)"
  },
  {
    "objectID": "ch11.html#aggregation-of-spatio-temporal-rasters",
    "href": "ch11.html#aggregation-of-spatio-temporal-rasters",
    "title": "16  Processing spatio-temporal data",
    "section": "16.4 Aggregation of spatio-temporal rasters",
    "text": "16.4 Aggregation of spatio-temporal rasters\n\n16.4.1 11.3.1 Introduction\nProcessing and visualization of spatio-temporal data are challenging, because of their three-dimensional nature. One of the basic approaches for working with spatio-temporal data is to simplify them using aggregation, in the spatial and/or temporal dimension, to help with visualization and exploratory analysis.\nTo demonstrate spatio-temporal aggregation, let's go back to the MOD13A3_2000_2019.tif raster, which is an example of spatio-temporal data forming a regular grid (Figure 11.4):\nlibrary(stars) r = read_stars(\"MOD13A3_2000_2019.tif\") dates = read.csv(\"MOD13A3_2000_2019_dates2.csv\") borders = st_read(\"israel_borders.shp\") names(r) = \"NDVI\" dates$date = as.Date(dates$date) r = st_set_dimensions(r, \"band\", values = dates$date, names = \"time\") r = st_warp(r, crs = 32636) r = r[borders]\n\n\n16.4.2 Aggregating time periods\nTo aggregate a raster on the temporal dimension (the raster layer), we need to:\n\nSplit the raster to subsets of raster layers (e.g., images captured in the different seasons)\nUse raster algebra to summarize each subset into a single layer (e.g., mean per pixel)\nCombine the results into a new multi-band raster (e.g., seasonal mean images)\n\nWe already learned how to subset raster layers using a numeric vector of indices (Section 6.2). Combined with which (Section 2.4.2), we can use this method to subset raster layers using a logical vector specifying which layers to keep. For example, here how we can get a subset of just the NDVI images taken in January:\nr[,,,which(dates$month == 1)] ## stars object with 3 dimensions and 1 attribute ## attribute(s), summary of first 1e+05 cells: ##      NDVI        ##  Min.   :-0.18   ##  1st Qu.: 0.12   ##  Median : 0.35   ##  Mean   : 0.33   ##  3rd Qu.: 0.50   ##  Max.   : 0.89   ##  NA's   :60579   ## dimension(s): ##      from  to  offset    delta                refsys point ## x      80 246  542754  926.305 WGS 84 / UTM zone 36N    NA ## y      14 478 3704603 -926.305 WGS 84 / UTM zone 36N    NA ## time    1  19      NA       NA                  Date    NA ##                                                   values x/y ## x                                                   NULL [x] ## y                                                   NULL [y] ## time [2001-01-01,2001-02-01),...,[2019-01-01,2019-02-01)\n\nHow can we create a subset of MOD13A3_2000_2019.tif with just the images taken during spring? How can we then calculate the \"average\" spring NDVI image?\n\nNow, let's use the same method to create a seasonal summary of average NDVI images, including each of the four seasons. We would like to create a raster named season_means, having 4 layers, where each layer is the average NDVI, excluding NA, per season:\n\n\"winter\"\n\"spring\"\n\"summer\"\n\"fall\"\n\nFirst, we create a vector of season names:\nseasons = c(\"winter\", \"spring\", \"summer\", \"fall\")\nThen, iterating on the season names with a for loop, for each season we:\n\nSubset the layers captured in that season only\nCalculate the mean NDVI per pixel\nCollect the result into a list\n\nThe following code section initializes an empty list named season_means, then runs a for loop that goes over the seasons and calculates seasons means:\nseason_means = list() for(i in seasons) {   sel = which(dates$season == i)   s = r[,,,sel]   season_means[[i]] = st_apply(s, 1:2, mean, na.rm = TRUE) }\nWhen the for loop ends, we get a list of seasonal mean rasters, named season_means.\n(Note that passing a vector of indices does not work inside a for loop, which is why we are creating a vector named sel as an intermediate step. This is a bug in the current version of stars which is resolved in the development version.)\nNext, we combine the list elements to a multi-band raster with do.call and c. The additional along=3 parameter makes sure the layers are \"stacked\" to form a third dimension:\nseason_means$along = 3 season_means = do.call(c, season_means)\nThis is basically a shortcut to the following alternative code, without using do.call, in which case we need to specify each and every one of the list items:\nseason_means = c(   season_means[[1]],   season_means[[2]],   season_means[[3]],   season_means[[4]],   along = 3 )\nEither way, we now have a four-band raster named season_mean with the seasonal means in r:\nseason_means ## stars object with 3 dimensions and 1 attribute ## attribute(s): ##      mean         ##  Min.   :-0.20    ##  1st Qu.: 0.11    ##  Median : 0.18    ##  Mean   : 0.23    ##  3rd Qu.: 0.33    ##  Max.   : 0.82    ##  NA's   :179251   ## dimension(s): ##         from  to  offset    delta                refsys point          values ## x         80 246  542754  926.305 WGS 84 / UTM zone 36N    NA            NULL ## y         14 478 3704603 -926.305 WGS 84 / UTM zone 36N    NA            NULL ## new_dim    1   4      NA       NA                    NA    NA winter,...,fall ##         x/y ## x       [x] ## y       [y] ## new_dim\nAll we have left to do is set the attribute name (\"NDVI\") and dimension names:\nnames(season_means) = \"NDVI\" season_means = st_set_dimensions(season_means, names = c(\"x\", \"y\", \"season\"))\nHere is the modified season_means raster:\nseason_means ## stars object with 3 dimensions and 1 attribute ## attribute(s): ##      NDVI         ##  Min.   :-0.20    ##  1st Qu.: 0.11    ##  Median : 0.18    ##  Mean   : 0.23    ##  3rd Qu.: 0.33    ##  Max.   : 0.82    ##  NA's   :179251   ## dimension(s): ##        from  to  offset    delta                refsys point          values ## x        80 246  542754  926.305 WGS 84 / UTM zone 36N    NA            NULL ## y        14 478 3704603 -926.305 WGS 84 / UTM zone 36N    NA            NULL ## season    1   4      NA       NA                    NA    NA winter,...,fall ##        x/y ## x      [x] ## y      [y] ## season\nNote that we do not need to set the layer dimention values, (i.e., the season names), since they are automatically populated with the names of the list elements in do.call.\nThe resulting season means raster is shown in Figure 11.11:\nplot(season_means, breaks = \"equal\", col = hcl.colors(11, \"Spectral\"))\nFigure 11.11: Average NDVI per season\nIn case we need to summarize the seasonal NDVI in a different way, all we have to do is replace the aggregation function. For example, we can decide to have NA instead of less reliable pixels where &gt;10% of values are missing. In such case, instead of the previous function mean, we use a custom function named f_NA:\nf_NA = function(x) {if(mean(is.na(x)) &gt; 0.1) NA else mean(x, na.rm = TRUE)}\nThe aggregation code are exactly the same as in the last example, except for using f_NA—instead of mean—inside st_apply:\nseason_means = list() for(i in seasons) {   sel = which(dates$season == i)   s = r[,,,sel]   season_means[[i]] = st_apply(s, 1:2, f_NA) } season_means$along = 3 season_means = do.call(c, season_means) names(season_means) = \"NDVI\" season_means = st_set_dimensions(season_means, names = c(\"x\", \"y\", \"season\"))\nThe result is similar, only with some pixels replaced with NA:\nplot(season_means, breaks = \"equal\", col = hcl.colors(11, \"Spectral\"))\nFigure 11.12: Average NDVI per season, pixels with &gt;25% NA excluded\nThe pattern of NA values can ne visualized by applying is.na on the result (Figure 11.13):\nplot(is.na(season_means)[borders], col = c(\"grey90\", \"red\"))\nFigure 11.13: Location of pixels with &gt;10% NA per season\n\nWhat is the purpose of the [borders] part in the above expression? What happens without it?\n\n\n\n16.4.3 Aggregating the \"x\" dimension\nOur second example demonstrates aggregation in one of the spatial dimensions, rather than in the layers dimension (Section 11.3.2). In this example, we will summarize the west-east gradient, i.e., the \"x\" dimension or raster columns, into a single value. That way, we will be able to visualize how NDVI changes acrosss the remaining spatial dimension \"y\", i.e., the north-south gradient, and over time.\nFirst, let's aggregate the \"x\" dimension for specific points in time, to visualize the north-south NDVI gradient during two time points only. This can be done by applying the mean function on all rows for particular layers, such as layers 1 and 7:\nx = st_apply(r[,,,1], \"y\", mean, na.rm = TRUE)[[1]]   # Winter y = st_apply(r[,,,7], \"y\", mean, na.rm = TRUE)[[1]]   # Summer\nThe resulting vectors can be plotted as follows (Figure 11.14):\nplot(x, type = \"l\", col = \"blue\", xlab = \"Row\", ylab = \"NDVI\") lines(y, col = \"red\")\nFigure 11.14: North-south NDVI gradient in two different months: February 2000 (blue) and August 2000 (red)\nNow we are going to repeat the operation for all layers of r, rather than two specific layers 1 and 7. In other words, we will calculate the north-south gradient for all time points (layers) in the raster r. We will create a raster s, where each column will contain the row means of one layer of r (Figure 11.15).\nFigure 11.15: Raster row means\nRaster s is going to have:\n\nThe same number of rows as r, i.e., 167 rows\nAs many columns as r layers, i.e., 233 columns\n\nTo create s, we aggregate on dimensions \"y\" and \"time\", so that we calculate the mean of each \"y\" and \"time\" combination, i.e., each row. Consequently, all values along \"x\" are averaged:\ns = st_apply(r, c(\"y\", \"time\"), mean, na.rm = TRUE) names(s) = \"NDVI\"\nThe resulting stars object s has two dimensions, \"y\" and \"time\", and its values are the average NDVI values for entire rows:\ns ## stars object with 2 dimensions and 1 attribute ## attribute(s): ##      NDVI          ##  Min.   :-0.1011   ##  1st Qu.: 0.1096   ##  Median : 0.1948   ##  Mean   : 0.2299   ##  3rd Qu.: 0.3214   ##  Max.   : 0.7447   ##  NA's   :733       ## dimension(s): ##      from  to  offset    delta                refsys point ## y      14 478 3704603 -926.305 WGS 84 / UTM zone 36N    NA ## time    1 233      NA       NA                  Date    NA ##                         values ## y                         NULL ## time 2000-02-01,...,2019-06-01\nThe arrangement of s is very convenient in case we want to work with the data as a matrix or as a data.frame. For example, transforming s to a data.frame results in a table where the NDVI value in each y and time combination is in a separate row:\ndat = as.data.frame(s) head(dat) ##         y       time      NDVI ## 1 3692561 2000-02-01       NaN ## 2 3691634 2000-02-01       NaN ## 3 3690708 2000-02-01       NaN ## 4 3689782 2000-02-01 0.1570333 ## 5 3688855 2000-02-01 0.2565000 ## 6 3687929 2000-02-01 0.3022000\nHowever, the s object does not have \"spatial\" x-y dimensions, which means that—striktly speaking—s is not a spatial raster. Therefore, it can't be simply displayed with plot:\nplot(s) ## Error in plot.stars(s): no raster, no features geometries: no default plot method set up yet!\nIn order to visualize s, we can modify its metadata so that:\n\nThe \"time\" and \"y\" dimensions are specified in the same units, e.g., in an arbitrary system with a resolution of 3×13×1, using the offset and delta parameters\nThe \"time\" and \"y\" dimensions are identified as \"spatial\" [y] and [x] dimensions, respectively, using the xy parameter\n\nIn the following code section, the first two expressions set the arbitrary coordinate system while the third expression identifies the dimensions as \"spatial\":\ns = st_set_dimensions(s, \"time\", offset = 0, delta = 3) s = st_set_dimensions(s, \"y\", offset = 0, delta = -1) s = st_set_dimensions(s, xy = c(\"time\", \"y\"))\nNow the s object can be plotted (Figure 11.16):\nplot(s, breaks = \"equal\", col = hcl.colors(11, \"Spectral\"), axes = TRUE, reset = FALSE) contour(s, add = TRUE)\nFigure 11.16: Row means of r over time"
  },
  {
    "objectID": "ch11.html#processing-trajectory-data",
    "href": "ch11.html#processing-trajectory-data",
    "title": "16  Processing spatio-temporal data",
    "section": "16.5 Processing trajectory data",
    "text": "16.5 Processing trajectory data\n\n16.5.1 The storms dataset\nAs another example of how list, split and do.call can be useful when processing spatio-temporal data, we will transform a table of point locations over time to a line layer of trajectories. Our input data for this example is a data.frame object named storms (package dplyr):\nlibrary(dplyr) storms = as.data.frame(storms) vars = c(\"name\", \"year\", \"month\", \"day\", \"hour\", \"long\", \"lat\") storms = storms[, vars]\nThe storms table contains recorded storm locations at consecutive points over time:\nhead(storms) ##   name year month day hour  long  lat ## 1  Amy 1975     6  27    0 -79.0 27.5 ## 2  Amy 1975     6  27    6 -79.0 28.5 ## 3  Amy 1975     6  27   12 -79.0 29.5 ## 4  Amy 1975     6  27   18 -79.0 30.5 ## 5  Amy 1975     6  28    0 -78.8 31.5 ## 6  Amy 1975     6  28    6 -78.7 32.4\nFor each storm record, we have the storm name (name), time (year, month, day, hour) and location (long, lat). The majority of storm records are given in consecutive 6-hour intervals (0, 6, 12, 18).\n\n\n16.5.2 Setting storm IDs\nTo distinguish between individual storm tracks we need to have a unique ID variable. Our first pick could be to use storm name (name). However, storm name is not a good fit for an ID, because it is not unique. Many of the storm names are repeated for different storms in different years. For example, a storm named \"Josephine\" appears in several different years between 1984 and 2008:\nstorms$year[storms$name == \"Josephine\"] ##   [1] 1984 1984 1984 1984 1984 1984 1984 1984 1984 1984 1984 1984 1984 1984 1984 ##  [16] 1984 1984 1984 1984 1984 1984 1984 1984 1984 1984 1984 1984 1984 1984 1984 ##  [31] 1984 1984 1984 1984 1984 1984 1984 1984 1984 1984 1984 1990 1990 1990 1990 ##  [46] 1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 ##  [61] 1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 ##  [76] 1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 ##  [91] 1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 1996 1996 ## [106] 1996 1996 1996 1996 1996 1996 1996 1996 1996 1996 1996 1996 1996 2002 2002 ## [121] 2002 2002 2002 2002 2002 2002 2008 2008 2008 2008 2008 2008 2008 2008 2008 ## [136] 2008 2008 2008 2008 2008 2008 2008 2008\nConsequently, we could choose to derive an ID that combines year (year) and storm name (name), so that each name/year combination is unique. However, a storm can be spread over two different years, e.g., if it starts in December and ends in January of the following year! For example, the storm named \"Zeta\" starts in December 30^th 2005 and ends in January 6^th 2006:\nhead(storms[storms$name == \"Zeta\", ]) ##      name year month day hour  long  lat ## 7372 Zeta 2005    12  30    0 -35.6 23.9 ## 7373 Zeta 2005    12  30    6 -36.1 24.2 ## 7374 Zeta 2005    12  30   12 -36.6 24.7 ## 7375 Zeta 2005    12  30   18 -37.0 25.2 ## 7376 Zeta 2005    12  31    0 -37.3 25.6 ## 7377 Zeta 2005    12  31    6 -37.6 25.7\ntail(storms[storms$name == \"Zeta\", ]) ##      name year month day hour  long  lat ## 7397 Zeta 2006     1   5    6 -46.6 21.9 ## 7398 Zeta 2006     1   5   12 -47.3 22.2 ## 7399 Zeta 2006     1   5   18 -47.9 22.7 ## 7400 Zeta 2006     1   6    0 -48.4 23.0 ## 7401 Zeta 2006     1   6    6 -49.0 23.1 ## 7402 Zeta 2006     1   6   12 -49.6 23.1\nUsing a name/year combination as an ID will wrongly split the storm in two parts: \"Zeta 2005\" and \"Zeta 2006\".\nWhat we really need is a unique ID for each consecutive sequence of storm names, assuming that no two consecutive storms will be given the same name. This type of IDs can be automatically generated using function rleid (Run Length Encoding ID) from the data.table package:\nlibrary(data.table) storms$id = rleid(storms$name)\nThe storms table now has a new column named id, which contains a unique index for each storm:\nhead(storms) ##   name year month day hour  long  lat id ## 1  Amy 1975     6  27    0 -79.0 27.5  1 ## 2  Amy 1975     6  27    6 -79.0 28.5  1 ## 3  Amy 1975     6  27   12 -79.0 29.5  1 ## 4  Amy 1975     6  27   18 -79.0 30.5  1 ## 5  Amy 1975     6  28    0 -78.8 31.5  1 ## 6  Amy 1975     6  28    6 -78.7 32.4  1\nFor example, using the id we can determine how many individual storms are there in the storms table (knowing that rleid produces IDs of consecutive numbers):\nmax(storms$id) ## [1] 425\n\n\n16.5.3 To points\nOur next step to to convert the storms table to a spatial point layer of the observed locations. The storms table can be converted to a point layer with st_as_sf, using the long and lat columns (Section 7.4):\nlibrary(sf) pnt = st_as_sf(storms, coords = c(\"long\", \"lat\"), crs = 4326)\nThe resulting object pnt is an sf point layer with 10010 points:\npnt ## Simple feature collection with 10010 features and 6 fields ## Geometry type: POINT ## Dimension:     XY ## Bounding box:  xmin: -109.3 ymin: 7.2 xmax: -6 ymax: 51.9 ## Geodetic CRS:  WGS 84 ## First 10 features: ##    name year month day hour id           geometry ## 1   Amy 1975     6  27    0  1   POINT (-79 27.5) ## 2   Amy 1975     6  27    6  1   POINT (-79 28.5) ## 3   Amy 1975     6  27   12  1   POINT (-79 29.5) ## 4   Amy 1975     6  27   18  1   POINT (-79 30.5) ## 5   Amy 1975     6  28    0  1 POINT (-78.8 31.5) ## 6   Amy 1975     6  28    6  1 POINT (-78.7 32.4) ## 7   Amy 1975     6  28   12  1   POINT (-78 33.3) ## 8   Amy 1975     6  28   18  1     POINT (-77 34) ## 9   Amy 1975     6  29    0  1 POINT (-75.8 34.4) ## 10  Amy 1975     6  29    6  1   POINT (-74.8 34)\nThe point layer is shown in Figure 11.17:\nplot(pnt)\nFigure 11.17: The storms points\n\n\n16.5.4 Points to lines\nTo transform a point layer of object locations over time to a line layer of trajectories, we go through the following steps:\n\nSplit the point layer to subsets of points for each storm\nSort each group of points chronologically, earliest to latest\nTransform each of the point sequences to a line\nCombine the separate lines back to a single line layer\n\nFirst, we split the point layer by storm ID (id), using the split function (Section 11.1.4). Remember that the sf class is a special case of a data.frame (Section 7.1.4), which is why split works with sf the same way as with a data.frame:\nlines = split(pnt, pnt$id)\nWe now have a list of point sequences per storm. For example, the first list element contains 30 points for the first storm Amy in 1975:\nlines[[1]] ## Simple feature collection with 30 features and 6 fields ## Geometry type: POINT ## Dimension:     XY ## Bounding box:  xmin: -79 ymin: 27.5 xmax: -51.6 ymax: 44.5 ## Geodetic CRS:  WGS 84 ## First 10 features: ##    name year month day hour id           geometry ## 1   Amy 1975     6  27    0  1   POINT (-79 27.5) ## 2   Amy 1975     6  27    6  1   POINT (-79 28.5) ## 3   Amy 1975     6  27   12  1   POINT (-79 29.5) ## 4   Amy 1975     6  27   18  1   POINT (-79 30.5) ## 5   Amy 1975     6  28    0  1 POINT (-78.8 31.5) ## 6   Amy 1975     6  28    6  1 POINT (-78.7 32.4) ## 7   Amy 1975     6  28   12  1   POINT (-78 33.3) ## 8   Amy 1975     6  28   18  1     POINT (-77 34) ## 9   Amy 1975     6  29    0  1 POINT (-75.8 34.4) ## 10  Amy 1975     6  29    6  1   POINT (-74.8 34)\nSorting the points, or making sure they are aready sorted, is essential to connect storm track points in chronological order. To sort the points we write a function that accepts a data.frame (or an sf) object and returns a sorted one based on the year, month, day and hour columns, in that order, using order (Section 4.6.2). The function is then applied on each storm, separately, using lapply (Section 11.1.3):\nf = function(x) x[order(x$year, x$month, x$day, x$hour), ] lines = lapply(lines, f)\nNow each list element is a series of points in chronological order:\nlines[[1]] ## Simple feature collection with 30 features and 6 fields ## Geometry type: POINT ## Dimension:     XY ## Bounding box:  xmin: -79 ymin: 27.5 xmax: -51.6 ymax: 44.5 ## Geodetic CRS:  WGS 84 ## First 10 features: ##    name year month day hour id           geometry ## 1   Amy 1975     6  27    0  1   POINT (-79 27.5) ## 2   Amy 1975     6  27    6  1   POINT (-79 28.5) ## 3   Amy 1975     6  27   12  1   POINT (-79 29.5) ## 4   Amy 1975     6  27   18  1   POINT (-79 30.5) ## 5   Amy 1975     6  28    0  1 POINT (-78.8 31.5) ## 6   Amy 1975     6  28    6  1 POINT (-78.7 32.4) ## 7   Amy 1975     6  28   12  1   POINT (-78 33.3) ## 8   Amy 1975     6  28   18  1     POINT (-77 34) ## 9   Amy 1975     6  29    0  1 POINT (-75.8 34.4) ## 10  Amy 1975     6  29    6  1   POINT (-74.8 34)\nThe first sequence is shown on Figure 11.18. We can clearly see how the storm spans two different months and several different days (month and day panels).\nplot(lines[[1]])\nFigure 11.18: The storms points\n\n\n16.5.5 Trajectory data\nOur next step is to combine all POINT geometries to a single MULTIPOINT geometry. We use st_combine to do that (Section 8.3.4.3):\nlines = lapply(lines, st_combine)\nEach lines element is now a MULTIPOINT geometry (sfc):\nlines[[1]] ## Geometry set for 1 feature  ## Geometry type: MULTIPOINT ## Dimension:     XY ## Bounding box:  xmin: -79 ymin: 27.5 xmax: -51.6 ymax: 44.5 ## Geodetic CRS:  WGS 84 ## MULTIPOINT ((-79 27.5), (-79 28.5), (-79 29.5),...\n\nWhy do you think all point attributes are lost as a result of applying st_combine?\n\nThe way the first list item looks when plotted is shown in Figure 11.19:\nplot(lines[[1]])\nFigure 11.19: A point trajectory converted to MULTIPOINT\nNext, each of the MULTIPOINT geometries can be cast (Section 8.3.4.4) to LINESTRING. Note that the additional fixed to parameter is passed from lapply to each of the st_cast function calls:\nlines = lapply(lines, st_cast, to = \"LINESTRING\")\nPrinting the first lines element reveals that the geometry type was indeed changed from MULTIPOINT to LINESTRING:\nlines[[1]] ## Geometry set for 1 feature  ## Geometry type: LINESTRING ## Dimension:     XY ## Bounding box:  xmin: -79 ymin: 27.5 xmax: -51.6 ymax: 44.5 ## Geodetic CRS:  WGS 84 ## LINESTRING (-79 27.5, -79 28.5, -79 29.5, -79 3...\nAccordingly, plotting shows a line instead of points (Figure 11.20):\nplot(lines[[1]])\nFigure 11.20: The LINESTRING created from MULTIPOINT\nAt this stage, we have a list of 425 individual LINESTRING geometries, one for each storm. The list can be combined back to an sfc geometry column with do.call (Section 11.1.5):\ngeometry = do.call(c, lines)\nUnlike in the previous example (Section 11.3.2) where we had a list of four elements (season_means), doing the above without do.call is not a viable option as we would have to type 425 arguments:\ngeometry = c(   lines[[1]],   lines[[2]],   ...,   lines[[425]] )\nHere is the combined sfc:\ngeometry ## Geometry set for 425 features  ## Geometry type: LINESTRING ## Dimension:     XY ## Bounding box:  xmin: -109.3 ymin: 7.2 xmax: -6 ymax: 51.9 ## Geodetic CRS:  WGS 84 ## First 5 geometries: ## LINESTRING (-79 27.5, -79 28.5, -79 29.5, -79 3... ## LINESTRING (-69.8 22.4, -71.1 21.9, -72.5 21.6,... ## LINESTRING (-48.9 34.9, -49.1 35.2, -48.9 35.3,... ## LINESTRING (-72.8 26, -73 26.3, -73.4 26, -73.2... ## LINESTRING (-58 23, -58.1 23.7, -58.2 24.3, -58...\nWe now have a geometry column with one line per individual storm (Figure 11.21):\nplot(geometry)\nFigure 11.21: The LINESTRING geometries for all storms combined\nWhat if we want to get back the other attributes of each storm, such as its name? To do that, we first need to attach storm IDs back to the geometries. We can rely on the fact that list names contain the original variable that was used for splitting the point layer, namely the storm IDs:\nlayer = st_sf(geometry, data.frame(id = names(lines)))\nThe line layer is now an sf object with one attribute id:\nlayer ## Simple feature collection with 425 features and 1 field ## Geometry type: LINESTRING ## Dimension:     XY ## Bounding box:  xmin: -109.3 ymin: 7.2 xmax: -6 ymax: 51.9 ## Geodetic CRS:  WGS 84 ## First 10 features: ##    id                       geometry ## 1   1 LINESTRING (-79 27.5, -79 2... ## 2   2 LINESTRING (-69.8 22.4, -71... ## 3   3 LINESTRING (-48.9 34.9, -49... ## 4   4 LINESTRING (-72.8 26, -73 2... ## 5   5 LINESTRING (-58 23, -58.1 2... ## 6   6 LINESTRING (-88.4 26.9, -88... ## 7   7 LINESTRING (-80 32.8, -79 3... ## 8   8 LINESTRING (-62.9 26.9, -64... ## 9   9 LINESTRING (-97 25.7, -97.4... ## 10 10 LINESTRING (-90.4 25.3, -91...\nPlotting the layer shows the ID values per storm (Figure 11.22):\nplot(layer)\nFigure 11.22: The storms line layer with storm ID\nIn case we need to, we can join other pieces of information from the original storms table back to layer, using aggregation (why?) followed by an ordinary join (Section 4.6) using the common column id.\n\n\n16.5.6 Line length\nNow that each storm is represented by a line, we can calculate spatial properties of the trajectories. For example, line length reflects the overall distance that each storm has traveled. The line lengths can be calculated using the st_length function (Section 8.3.2):\nlayer$length = st_length(layer) layer ## Simple feature collection with 425 features and 2 fields ## Geometry type: LINESTRING ## Dimension:     XY ## Bounding box:  xmin: -109.3 ymin: 7.2 xmax: -6 ymax: 51.9 ## Geodetic CRS:  WGS 84 ## First 10 features: ##    id                       geometry        length ## 1   1 LINESTRING (-79 27.5, -79 2... 3510012.0 [m] ## 2   2 LINESTRING (-69.8 22.4, -71... 3177598.3 [m] ## 3   3 LINESTRING (-48.9 34.9, -49... 1469864.8 [m] ## 4   4 LINESTRING (-72.8 26, -73 2... 2148308.5 [m] ## 5   5 LINESTRING (-58 23, -58.1 2... 3226682.2 [m] ## 6   6 LINESTRING (-88.4 26.9, -88... 1600143.2 [m] ## 7   7 LINESTRING (-80 32.8, -79 3... 2145905.6 [m] ## 8   8 LINESTRING (-62.9 26.9, -64... 2392209.7 [m] ## 9   9 LINESTRING (-97 25.7, -97.4...  455724.8 [m] ## 10 10 LINESTRING (-90.4 25.3, -91...  953824.1 [m]\nLengths can be transformed to km�� units for convenience:\nlibrary(units) layer$length = set_units(layer$length, \"km\") layer ## Simple feature collection with 425 features and 2 fields ## Geometry type: LINESTRING ## Dimension:     XY ## Bounding box:  xmin: -109.3 ymin: 7.2 xmax: -6 ymax: 51.9 ## Geodetic CRS:  WGS 84 ## First 10 features: ##    id                       geometry         length ## 1   1 LINESTRING (-79 27.5, -79 2... 3510.0120 [km] ## 2   2 LINESTRING (-69.8 22.4, -71... 3177.5983 [km] ## 3   3 LINESTRING (-48.9 34.9, -49... 1469.8648 [km] ## 4   4 LINESTRING (-72.8 26, -73 2... 2148.3085 [km] ## 5   5 LINESTRING (-58 23, -58.1 2... 3226.6822 [km] ## 6   6 LINESTRING (-88.4 26.9, -88... 1600.1432 [km] ## 7   7 LINESTRING (-80 32.8, -79 3... 2145.9056 [km] ## 8   8 LINESTRING (-62.9 26.9, -64... 2392.2097 [km] ## 9   9 LINESTRING (-97 25.7, -97.4...  455.7248 [km] ## 10 10 LINESTRING (-90.4 25.3, -91...  953.8241 [km]\nThe result can be visualized using mapview as follows:\nlibrary(mapview) layer$length = drop_units(layer$length) mapview(layer, zcol = \"length\")"
  },
  {
    "objectID": "ch12.html#aims",
    "href": "ch12.html#aims",
    "title": "17  Spatial interpolation of point data",
    "section": "17.1 Aims",
    "text": "17.1 Aims\nOur aims in this chapter are:\n\nCalculate an empirical variogram\nFit a variogram model\nInterpolate using three methods:\n\nInverse Distance Weighted (IDW) interpolation\nOrdinary Kriging (OK)\nUniversal Kriging (UK)\n\nEvaluate interpolation accuracy using Leave-One-Out Cross Validation\n\nWe will use the following R packages:\n\nsf\nstars\ngstat\nautomap"
  },
  {
    "objectID": "ch12.html#what-is-spatial-interpolation",
    "href": "ch12.html#what-is-spatial-interpolation",
    "title": "17  Spatial interpolation of point data",
    "section": "17.2 What is spatial interpolation?",
    "text": "17.2 What is spatial interpolation?\n\n17.2.1 Interpolation models\nSpatial interpolation is the prediction of a given phenomenon in unmeasured locations (Figures 12.1–12.2). For that, we need a spatial interpolation model—a set of procedures to calculate predicted values of the variable of interest, given calibration data.\nFigure 12.1: Spatial interpolation (Input elevation point data, Interpolated elevation surface) (http://desktop.arcgis.com/en/arcmap/10.3/tools/spatial-analyst-toolbox/understanding-interpolation-analysis.htm)\nFigure 12.2: Spatial interpolation (Point locations of ozone monitoring stations, Interpolated prediction surface) (http://desktop.arcgis.com/en/arcmap/10.3/tools/spatial-analyst-toolbox/understanding-interpolation-analysis.htm)\nCalibrarion data usually include:\n\nField measurements—available for a limited number of locations, for example: rainfall data from meteorological stations\nCovariates—available for each and every location within the area of interest, for example: elevation from a DEM\n\nSpatial interpolation models can be divided into two general categories:\n\nDeterministic models—Models using arbitrary parameter values, for example: IDW\nStatistical models—Models using parameters chosen objectively based on the data, for example: Kriging\n\nKeep in mind that data structure does not imply meaning. It is technically possible to interpolate any numeric variable measured in a set of points, however it does not always make sense to do so. For example, it does not make sense to spatially interpolate point data when they refer to a localized phenomenon, such as amount of emissions per power plant (Figure 12.3).\nFigure 12.3: CO2��2 emissions from power plants: a localized phenomenon (https://edzer.github.io/UseR2016/)\nConveresly, it does not make sense to sum up point measurements of a continuous phenomenon (Figure 12.4).\nFigure 12.4: PM10��10 measurements: a continuous phenomenon (https://edzer.github.io/UseR2016/)\n\n\n17.2.2 12.1.2 The weighted average principle\nMany of the commonly used interpolation methods, including the ones we learn about in this Chapter (Nearest Neighbor, IDW, Kriging), are based on the same principle, where a predicted value is a weighted average of neighboring points. Weight are usually inveresely related to distance, i.e., as distance increases the weight (importance) of the point decreases. The predicted value for a particular point is calculated as a weighted average of measured values in other points (Equation (12.1)):\n[^Z(s0)=∑ni=1w(si)Z(si)∑ni=1w(si)(12.1)(12.1)�^(�0)=∑�=1��(��)�(��)∑�=1��(��)]{.underline}\nwhere:\n\n^Z(s0)�^(�0) is the predicted value at location s0�0\nw(si)�(��) is the weight of measured point i�\nZ(si)�(��) is the value of measured point i�\n\nThe weight w(si)�(��) of each measured point is a function of distance (Figure 12.5) from the predicted point.\nFigure 12.5: Distances between predicted point and all measured points (http://desktop.arcgis.com/en/arcmap/10.3/tools/spatial-analyst-toolbox/how-kriging-works.htm)\nIn IDW, the weight is the inverse of distance to the power of p� (Equation (12.2)):\nw(si)=1d(s0,si)p(12.2)(12.2)�(��)=1�(�0,��)�\nwhere:\n\nw(si)�(��) is the weight of measured point i�\nd(s0,si)�(�0,��) is the distance between predicted point s0�0 and measured point si��\n\nThe default value for p� is usually p=2�=2 (Equation (12.3)):\nw(si)=1d(s0,si)2(12.3)(12.3)�(��)=1�(�0,��)2\nThe p� parameter basically determines how steeply does weight increase with proximity. As a result, p� determines whether weights are more or less equally distributed among neighbors (low p�) or whether one point (the nearest) has overwhelmingly high weight and thus the predicted value will be strongly influenced by that point (high p�). In other words, when p� approaches zero, the predicted result will approach a uniform surface which is just an average of all measured points. When p� approaches infinity, the predicted result will approach nearest neighbor interpolation, which is the simplest spatial interpolation method there is: every predicted point gets the value of the nearest measured point (Figures 12.6–12.7).\nFigure 12.6: Spatial interpolation of annual rainfall using IDW with p=0.25�=0.25, p=2�=2 and p=16�=16\nFigure 12.7: Nearest Neighbor interpolation (left) and Voronoi polygons (right)\nIn Kriging, the weight is a particular function of distance known as the variogram model (Figure 12.8). The variogram model is fitted to characterize the autocorrelation structure in the measured data, based on the empirical variogram.\nFigure 12.8: Variogram models: spherical (left) and exponential (right) (http://desktop.arcgis.com/en/arcmap/10.3/tools/spatial-analyst-toolbox/how-kriging-works.htm)\nThere are two frequently used kriging methods: Ordinary Kriging (OK) and Universal Kriging (UK). Adding up the Inverse Distance Weighted (IDW) interpolation, we now mentioned three interpolation methods. We are going to cover those three methods (Figure 12.9), mostly from the practical point of view, in the next three sections (Sections 12.2–12.4).\nFigure 12.9: Spatial interpolation of annual rainfall using IDW, OK and UK\nFor the examples, we will load the rainfall.csv file (Section 4.4.3), calculate the annual column (Section 4.5) and convert it to a point layer (Section 7.4):\nlibrary(sf) rainfall = read.csv(\"rainfall.csv\") rainfall = st_as_sf(rainfall, coords = c(\"x_utm\", \"y_utm\"), crs = 32636) m = c(\"sep\", \"oct\", \"nov\", \"dec\", \"jan\", \"feb\", \"mar\", \"apr\", \"may\") rainfall$annual = apply(st_drop_geometry(rainfall[, m]), 1, sum)\nWe will also use a 1×11×1 km2��2 DEM raster of the area of interest (Sections 9.1, 9.3 and 10.1):\nlibrary(stars) dem1 = read_stars(\"srtm_43_06.tif\") dem2 = read_stars(\"srtm_44_06.tif\") dem = st_mosaic(dem1, dem2) borders = st_read(\"israel_borders.shp\") grid = st_as_sfc(st_bbox(borders)) grid = st_as_stars(grid, dx = 1000, dy = 1000) dem = st_warp(src = dem, grid, method = \"average\", use_gdal = TRUE) dem = dem[borders] names(dem) = \"elev_1km\"\nFinally, we subset the dem to include only the area to the north of 31 degrees latitude, where meteorological station density is relatively high:\ny = st_as_sf(dem, as_points = TRUE) y$lat = st_coordinates(st_transform(y, 4326))[,2] y = st_rasterize(y[, \"lat\"], dem) y[y &lt; 31] = NA y[!is.na(y)] = 1 y = st_as_sf(y, merge = TRUE) dem = dem[y]\nNext, we extract the elevation values (Section 10.7.2):\nrainfall = st_join(rainfall, st_as_sf(dem))\nand subset those stations that coincide with the raster:\nrainfall = rainfall[!is.na(rainfall$elev_1km), ]\nFigure 12.10 shows the dem elevation raster and the rainfall point layer:\nplot(dem, breaks = \"equal\", col = terrain.colors(11), reset = FALSE) plot(st_geometry(rainfall), add = TRUE)\nFigure 12.10: Rainfall data points and elevation raster"
  },
  {
    "objectID": "ch12.html#inverse-distance-weighted-interpolation",
    "href": "ch12.html#inverse-distance-weighted-interpolation",
    "title": "17  Spatial interpolation of point data",
    "section": "17.3 Inverse Distance Weighted interpolation",
    "text": "17.3 Inverse Distance Weighted interpolation\n\n17.3.1 The gstat object\nTo interpolate, we first need to create an object of class gstat, using a function of the same name: gstat. A gstat object contains all necessary information to conduct spatial interpolation, namely:\n\nThe model definition\nThe calibration data\n\nBased on its arguments, the gstat function \"understands\" what type of interpolation model we want to use:\n\nNo variogram model → IDW\nVariogram model, no covariates → Ordinary Kriging\nVariogram model, with covariates → Universal Kriging\n\nThe complete decision tree of gstat, including several additional methods which we are not going to use, is shown in Figure 12.11.\nFigure 12.11: gstat predict methods (Applied Spatial Data Analysis with R, 2013)\nWe are going to use three parameters of the gstat function:\n\nformula—The prediction \"formula\" specifying the dependent and the independent variables (covariates)\ndata—The calibration data\nmodel—The variogram model\n\nKeep in mind that we need to specify parameter names, because these three parameters are not the first three in the gstat function definition.\nFor example, to interpolate using the IDW method we create the following gstat object, specifying just the formula (Section 12.2.2 below) and data:\nlibrary(gstat) g = gstat(formula = annual ~ 1, data = rainfall)\n\n\n17.3.2 Working with formula objects\nIm R, formula objects are used to specify relation between objects, in particular—the role of different data columns in statistical models. A formula object is created using the ~ operator, which separates names of dependent variables (to the left of the ~ symbol) and independent variables (to the right of the ~ symbol). Writing 1 to the right of the ~ symbol, as in ~ 1, means that there are no independent variables39.\nFor example, in the following expression we create a formula object named f:\nf = annual ~ 1 f ## annual ~ 1\nChecking the class shows that f is indeed a formula object:\nclass(f) ## [1] \"formula\"\nWe can also convert character values to formula using the as.formula function. For example:\nf = as.formula(\"annual ~ 1\") class(f) ## [1] \"formula\"\nThe as.formula function is particularly useful when we want to construct different formulas as part of a for loop (Section 12.3.3).\n\n\n17.3.3 Making predictions\nNow that our model is defined, we can use the predict function to actually interpolate, i.e., to calculate predicted values. The predict function accepts:\n\nA raster—stars object, such as dem\nA model—gstat object, such as g\n\nThe raster serves for two purposes:\n\nSpecifying the locations where we want to make predictions (in all methods)\nSpecifying covariate values (in Universal Kriging only)\n\nFor example, the following expression interpolates annual values according to the model defined in g and the raster template defined in dem:\nz = predict(g, dem) ## [inverse distance weighted interpolation]\nThe resulting stars object has two attributes:\n\nvar1.pred—the predicted values\nvar1.var—the variance (for Kriging only)\n\nFor example:\nz ## stars object with 2 dimensions and 2 attributes ## attribute(s): ##    var1.pred        var1.var      ##  Min.   : 42.85   Min.   : NA     ##  1st Qu.:356.01   1st Qu.: NA     ##  Median :495.20   Median : NA     ##  Mean   :466.49   Mean   :NaN     ##  3rd Qu.:552.16   3rd Qu.: NA     ##  Max.   :946.31   Max.   : NA     ##  NA's   :20108    NA's   :39933   ## dimension(s): ##   from  to  offset delta                       refsys point values x/y ## x    1 153  616965  1000 +proj=utm +zone=36 +datum...    NA   NULL [x] ## y    1 261 3691819 -1000 +proj=utm +zone=36 +datum...    NA   NULL [y]\nWe can subset just the first attribute and rename it to \"annual\":\nz = z[\"var1.pred\",,] names(z) = \"annual\"\nThe interpolated annual rainfall raster, using IDW, is shown in Figure 12.12:\nb = seq(0, 1200, 100) plot(z, breaks = b, col = hcl.colors(length(b)-1, \"Spectral\"), reset = FALSE) plot(st_geometry(rainfall), pch = 3, add = TRUE) contour(z, breaks = b, add = TRUE)\nFigure 12.12: Predicted annual rainfall using Inverse Distance Weighted (IDW) interpolation"
  },
  {
    "objectID": "ch12.html#ordinary-kriging",
    "href": "ch12.html#ordinary-kriging",
    "title": "17  Spatial interpolation of point data",
    "section": "17.4 Ordinary Kriging",
    "text": "17.4 Ordinary Kriging\n\n17.4.1 Annual rainfall example\nKriging methods require a variogram model. The variogram model is an objective way to quantify the autocorrelation pattern in the data, and assign weights accordingly when making predictions (Section 12.1.2).\nAs a first step, we can calculate and examine the empirical variogram using the variogram function. The function requires two arguments:\n\nformula—Specifies the dependent variable and the covariates, just like in gstat\ndata—The point layer with the dependent variable and covariates as point attributes\n\nFor example, the following expression calculates the empirical variogram of annual, with no covariates:\nv_emp_ok = variogram(annual ~ 1, rainfall)\nUsing plot to examine it we can examine the variogram (Figure 12.13):\nplot(v_emp_ok)\nFigure 12.13: Empirical variogram\nThere are several ways to fit a variogram model to an empirical variogram. We will use the simplest one—automatic fitting using function autofitVariogram from package automap:\nlibrary(automap) v_mod_ok = autofitVariogram(annual ~ 1, as(rainfall, \"Spatial\"))\nThe function chooses the best fitting type of model, and also fine tunes its parameters. (Use show.vgms() to display variogram model types.) Note that the autofitVariogram function does not work on sf objects, which is why we convert the object to a SpatialPointsDataFrame (package sp).\nThe fitted model can be plotted with plot (Figure 12.14):\nplot(v_mod_ok)\nFigure 12.14: Variogram model\nThe resulting object is actually a list with several components, including the empirical variogram and the fitted variogram model. The $var_model component of the resulting object contains the actual model:\nv_mod_ok$var_model ##   model      psill    range kappa ## 1   Nug   451.9177     0.00     0 ## 2   Ste 23223.8370 34604.87     2\nThe variogram model can then be passed to the gstat function, and we can carry on with the Ordinary Kriging interpolation:\ng = gstat(formula = annual ~ 1, model = v_mod_ok$var_model, data = rainfall) z = predict(g, dem) ## [using ordinary kriging]\nAgain, we will subset the predicted values attribute and rename it:\nz = z[\"var1.pred\",,] names(z) = \"annual\"\nThe Ordinary Kriging predictions are shown in Figure 12.15:\nb = seq(0, 1200, 100) plot(z, breaks = b, col = hcl.colors(length(b)-1, \"Spectral\"), reset = FALSE) plot(st_geometry(rainfall), pch = 3, add = TRUE) contour(z, breaks = b, add = TRUE)\nFigure 12.15: Predicted annual rainfall using Ordinary Kriging\n\n\n17.4.2 Elevation example\nAnother example: suppose that we did not have a DEM for Israel, but only the elevation measurements at the meteorological stations. How can we produce an elevation raster using Ordinary Kriging?\nFirst, we prepare the gstat object:\nv = autofitVariogram(altitude ~ 1, as(rainfall, \"Spatial\")) g = gstat(formula = altitude ~ 1, model = v$var_model, data = rainfall)\nThen, we interpolate:\nz = predict(g, dem) ## [using ordinary kriging] z = z[\"var1.pred\",,] names(z) = \"elevation\"\nThe predicted elevation raster is shown in Figure 12.16:\nb = seq(-500, 1200, 100) plot(z, breaks = b, col = hcl.colors(length(b)-1, \"Spectral\"), reset = FALSE) plot(st_geometry(rainfall), pch = 3, add = TRUE) contour(z, breaks = b, add = TRUE)\nFigure 12.16: Ordinary Kriging prediction of elevation\n\n\n17.4.3 Monthly rainfall example\nIn the next example we use kriging inside a for loop, to make a series of predictions for different variables. Specifically, we will use Ordinary Kriging to predict monthly rainfall, i.e., sep through may columns in the rainfall layer.\nIn each for loop \"round,\" the formula is going to be re-defined according to the current month i. For example:\ni = \"may\" as.formula(paste0(i, \" ~ 1\")) ## may ~ 1\nFirst, we set up a vector with the column names of the variables we wish to interpolate, and a list where we \"collect\" the results:\nm = c(\"sep\", \"oct\", \"nov\", \"dec\", \"jan\", \"feb\", \"mar\", \"apr\", \"may\") result = list()\nSecond, we specify the for loop, as follows:\nfor(i in m) {   f = as.formula(paste0(i, \" ~ 1\"))   v = autofitVariogram(f, as(rainfall, \"Spatial\"))   g = gstat(formula = f, model = v$var_model, data = rainfall)   z = predict(g, dem)   z = z[\"var1.pred\",,]   result[[i]] = z } ## [using ordinary kriging] ## [using ordinary kriging] ## [using ordinary kriging] ## [using ordinary kriging] ## [using ordinary kriging] ## [using ordinary kriging] ## [using ordinary kriging] ## [using ordinary kriging] ## [using ordinary kriging]\nFinally, we combine the list of results per month into a single multi-band raster, using do.call and c (Section 11.3.2):\nresult$along = 3 result = do.call(c, result)\nThe interpolated montly rainfall amounts are shown in Figure 12.17:\nplot(result, breaks = \"equal\", col = hcl.colors(11, \"Spectral\"), key.pos = 4)\nFigure 12.17: Monthly rainfall predictions using Ordinary Kriging"
  },
  {
    "objectID": "ch12.html#universal-kriging",
    "href": "ch12.html#universal-kriging",
    "title": "17  Spatial interpolation of point data",
    "section": "17.5 Universal Kriging",
    "text": "17.5 Universal Kriging\nUniversal Kriging interpolation uses a model with one or more independent variables, i.e., covariates. The covariates need to be known for both:\n\nThe point layer, as an attribute such as elev_1km in rainfall\nThe predicted locations, as raster values such as dem values\n\nThe formula now specifies the name(s) of the covariate(s) to the right of the ~ symbol, separated by + if there are more than one. Also, we are using a subset of rainfall where elev_1km values were present:\nv_emp_uk = variogram(annual ~ elev_1km, rainfall) v_mod_uk = autofitVariogram(annual ~ elev_1km, as(rainfall, \"Spatial\"))\nComparing the Ordinary Kriging and Universal Kriging variogram models (Figure 12.18):\nplot(v_emp_ok, model = v_mod_ok$var_model, ylim = c(0, 25000), main = \"OK\") plot(v_emp_uk, model = v_mod_uk$var_model, ylim = c(0, 25000), main = \"UK\")\nFigure 12.18: OK and UK variogram models\nNext we create a gstat object, where the formula contains the covariate and the corresponding variogram model:\ng = gstat(formula = annual ~ elev_1km, model = v_mod_uk$var_model, data = rainfall)\nRemember that all of the variables that appear in the formula need to be present in the data. In this case we have two variables: a dependent variable (annual) and an independent variable (elev_1km).\nNow we can make predictions:\nz = predict(g, dem) ## [using universal kriging]\nand then subset and rename:\nz = z[\"var1.pred\",,] names(z) = \"annual\"\nUniversal Kriging predictions are shown in Figure 12.19:\nb = seq(0, 1200, 100) plot(z, breaks = b, col = hcl.colors(length(b)-1, \"Spectral\"), reset = FALSE) plot(st_geometry(rainfall), pch = 3, add = TRUE) contour(z, breaks = b, add = TRUE)\nFigure 12.19: Predicted annual rainfall using Universal Kriging"
  },
  {
    "objectID": "ch12.html#cross-validation",
    "href": "ch12.html#cross-validation",
    "title": "17  Spatial interpolation of point data",
    "section": "17.6 Cross-validation",
    "text": "17.6 Cross-validation\nIn Sections 12.2–12.4, we have calculated annual rainfall surfaces using three different methods: IDW, Ordinary Kriging and Universal Kriging. Although it is useful to examine and compare the results graphically (Figures 12.12, 12.15, and 12.19), we also need an objective way to evaluate interolation accuracy. That way, we can objectively choose the most accurate method among the various interpolation methods there are.\nPlainly speaking, to evaluate prediction accuracy we need to compare the predicted values with measured data in the same location. Since measured data are often sparse and expensive to produce, it makes little sense to collect more data merely for the sake of accuracy assessment. Instead, the available data are usually split to two parst, called training and test data. The training data are used to fit the model, while the test data are used to calculate prediction accuracy. The procedure is called cross validation. A specific, commonly used, type of cross validation is Leave-One-Out Cross Validation where all observations consecutively take the role of test data while the remaning observations take the role of training data. The separation of training and test data is important because evaluating a model based on the same data used to fit it gives the model an \"unfair\" advantage and therefore overestimates accuracy.\nIn Leave-One-Out Cross Validation we:\n\nTake out one point out of the calibration data\nMake a prediction for that point\nRepeat for all points\n\nIn the end, what we get is a table with an observed value and a predicted value for all points.\nWe can run Leave-One-Out Cross Validation using the gstat.cv function, which accepts a gstat object:\ncv = gstat.cv(g)\nThe gstat.cv function returns an object of class SpatialPointsDataFrame (package sp), which we can convert to an sf object with st_as_sf:\ncv = st_as_sf(cv) cv ## Simple feature collection with 160 features and 6 fields ## Geometry type: POINT ## Dimension:     XY ## Bounding box:  xmin: 629301.4 ymin: 3435503 xmax: 761589.2 ymax: 3681163 ## CRS:           NA ## First 10 features: ##    var1.pred var1.var observed    residual       zscore fold ## 1   632.4635 951.6388    582.8 -49.6635206 -1.609909413    1 ## 2   614.6876 915.2690    608.5  -6.1876076 -0.204525930    2 ## 3   598.0584 907.7003    614.7  16.6415723  0.552361147    3 ## 4   609.5275 774.8052    562.7 -46.8274602 -1.682303431    4 ## 5   701.0174 913.6664    682.8 -18.2174496 -0.602689676    5 ## 6   610.4985 740.2553    705.5  95.0014929  3.491722024    6 ## 7   645.4771 824.3927    610.4 -35.0771011 -1.221677615    7 ## 8   581.9809 976.5117    583.3   1.3190893  0.042211954    8 ## 9   654.7017 705.9952    709.8  55.0983188  2.073659677    9 ## 10  654.6546 706.8918    654.8   0.1454226  0.005469598   10 ##                    geometry ## 1  POINT (697119.1 3656748) ## 2  POINT (696509.3 3652434) ## 3  POINT (696541.7 3641332) ## 4  POINT (697875.3 3630156) ## 5  POINT (689553.7 3626282) ## 6  POINT (694694.5 3624388) ## 7  POINT (686489.5 3619716) ## 8  POINT (683148.4 3616846) ## 9  POINT (696489.3 3610221) ## 10 POINT (693025.1 3608449)\nThe result of gstat.cv has the following attributes:\n\nvar1.pred—Predicted value\nvar1.var—Variance (only for Kriging)\nobserved—Observed value\nresidual—Observed-Predicted\nzscore—Z-score (only for Kriging)\nfold—Cross-validation ID\n\nA bubble plot is convenient to examine the residuals, since it shows positive and negative values in different color (Figure 12.20):\nbubble(as(cv[, \"residual\"], \"Spatial\"))\nFigure 12.20: Cross-validation residuals\nUsing the \"predicted\" and \"observed\" columns we can calculate prediction accuracy indices, such as the Root Mean Square Error (RMSE) (Equation (12.4)):\nRMSE=√∑ni=1(predi−obsi)2n(12.4)(12.4)����=∑�=1�(�����−����)2�\nwhere predi����� and obsi���� are predicted and observed values for point i�, respectively.\nFor example:\nsqrt(sum((cv$var1.pred - cv$observed)^2) / nrow(cv)) ## [1] 36.21741"
  },
  {
    "objectID": "ch13.html#introduction",
    "href": "ch13.html#introduction",
    "title": "\n18  High Frequency Data\n",
    "section": "\n18.1 Introduction",
    "text": "18.1 Introduction\nHigh-frequency ocean data for temperature, pH, and dissolved oxygen can be obtained through the use of oceanographic sondes. Sondes are instruments that are deployed in the ocean to collect continuous measurements of various parameters. When combined, temperature, pH, and dissolved oxygen data collected can provide valuable insights into oceanic conditions and help researchers study processes such as ocean warming, ocean acidification, and oxygen dynamics.\nIn this chapter, we will delve into the intricacies of processing and visualizing high frequency data. Our dataset has been sourced from the Pemba Channel, specifically in the vicinity of upwelling events. Our aim is to impart knowledge on the techniques required to analyze and interpret this data effectively. We will adopt a formal approach throughout this chapter to ensure that our readers gain a comprehensive understanding of the subject matter."
  },
  {
    "objectID": "ch13.html#packages-needed",
    "href": "ch13.html#packages-needed",
    "title": "\n18  High Frequency Data\n",
    "section": "\n18.2 Packages needed",
    "text": "18.2 Packages needed\nLet’s read the functions in the working directory.The require() function is used to load R packages into the current R session. When the require(tidyverse) code is executed, it checks if the tidyverse package is already installed. If it is not installed, it installs the package and then loads it into the session. If the package is already installed, it simply loads it.\n\nCoderequire(tidyverse)\nrequire(patchwork)\n\n\nThen we define our color codes that we are going to use\n\nCodemy.colors = colorRampPalette(c(\"#5E85B8\",\"#EDF0C0\",\"#C13127\"))\n\n\nmy.colors2 = colorRampPalette(colors = c(\"#8220EF\", \"#000096\", \"#0000CC\", \"#446CEA\",\n                                         \"#1F90FF\", \"#00BFFA\",\"#9FD2FE\", \"#D3F5FF\", \n                                         \"#FFFFC7\", \"#FFAA01\", \"#FF6E01\",\n                            \"#FE0000\", \"#C80000\", \"#9F2323\"))\n\nrecolor = c(\"#8220EF\", \"#000094\", \"#0000CC\", \"#4169E2\", \"#1F90FF\", \"#00C1FE\", \n            \"#9FD2FE\", \"#D3F5FF\", \"#FDFFC8\",  \"#FEE131\", \"#FFAA01\", \"#FF6E01\",\n            \"#FE0000\", \"#C80000\", \"#9F2323\", \"#FF69B4\")"
  },
  {
    "objectID": "ch13.html#dataset",
    "href": "ch13.html#dataset",
    "title": "\n18  High Frequency Data\n",
    "section": "\n18.3 Dataset",
    "text": "18.3 Dataset\nWe are going to use moa_high_frequency.csv, which is a high frequency dataset. This dataset has three columns and a brief description of each column:\n\ndate: This column appears to contain date and time values represented as dttm (DateTime) data type. It includes specific timestamps such as “2022-10-08 22:00:00” and “2022-10-08 22:30:00”.\nvariables: This column is of character (chr) data type represents variable measured, which are Temperature, pH and dissolved oxygen.\ndata: This column is of numeric (dbl) data type and contains the actual data values corresponding to the measurements of Temperature, pH and dissolved oxygen.\n\nAs the data is in a comma-separated file format, we can easily import it into the session from the working directory by using the read_csv function from the readr package.\n\nCodedata = read_csv(\"data/moa_high_frequency.csv\")\n\n\nThe code data = read_csv(\"data/moa_high_frequency.csv\") read and load the high-frequency data from the CSV file into the data variable, and you can further analyze and manipulate the data using R’s data manipulation and analysis capabilities.\nWe can concise have a summary of a dataset, including its structure and contents with glimpse function. It is particularly useful for large datasets with many variables.\n\nCodedata |&gt;\n  glimpse()\n\nRows: 25,887\nColumns: 3\n$ date      &lt;dttm&gt; 2022-10-08 22:00:00, 2022-10-08 22:00:00, 2022-10-08 22:30:…\n$ variables &lt;chr&gt; \"temperature\", \"ph\", \"temperature\", \"ph\", \"temperature\", \"ph…\n$ data      &lt;dbl&gt; 26.520000, 8.061103, 26.520000, 8.061375, 26.500000, 8.05981…\n\n\nThis summary provides an overview of the dataset’s structure and the information contained within each column. This dataset contains 25,887 rows and 3 columns. We can use this information to quickly assess the quality and relevance of the dataset for analysis purposes\nFurther, we can print the data frame and show the first and last five records of the dataset. The FSA::headtail() function is not a built-in function in R. To use the headtail() function with the data object, you would need to ensure that the “FSA” package is installed and loaded.\n\nCodedata |&gt;\n  FSA::headtail() |&gt;\n  gt::gt()\n\n\n\n\n\n\nTABLE 18.1.  The first and last five records of the datase \n  \ndate\n      variables\n      data\n    \n\n\n2022-10-08 22:00:00\ntemperature\n26.520000\n\n\n2022-10-08 22:00:00\nph\n8.061103\n\n\n2022-10-08 22:30:00\ntemperature\n26.520000\n\n\n2023-04-06 04:00:00\ndo\n6.970000\n\n\n2023-04-06 04:30:00\ndo\n6.830000\n\n\n2023-04-06 05:00:00\ndo\n6.840000\n\n\n\n\n\n\n\nWe notice that the variables names must be edited, By applying these transformations using the mutate() function and the str_replace() function, the variables names are updated, reflecting the desired replacements.\n\nCodedata = data  |&gt;\n  mutate(\n    variables = str_replace(string = variables, pattern = \"do\", replacement = \"Dissolved Oxygen\"),\n    variables = str_replace(string = variables, pattern = \"temperature\", replacement = \"Temperature\"),\n    variables = str_replace(string = variables, pattern = \"ph\", replacement = \"pH\")\n  )"
  },
  {
    "objectID": "ch13.html#visualizing-high-frequency-data",
    "href": "ch13.html#visualizing-high-frequency-data",
    "title": "\n18  High Frequency Data\n",
    "section": "\n18.4 Visualizing high frequency data",
    "text": "18.4 Visualizing high frequency data\nOnce this high frequency dataset has been tidied, it is important to visualize the data to gain insights and identify patterns. Visualizing high frequency data can be challenging due to the sheer volume of data points. However, there are several techniques that can be used to effectively visualize this type of data.\nOne approach is to use a line chart to plot the data over time. This allows for easy identification of trends and patterns in the data.\nFigure 18.1\n\nCodedata |&gt;\n  ggplot(aes(x = date, y = data)) +\n  geom_path() +\n  facet_wrap(~variables, scales = \"free_y\", nrow = 2)+\n  annotate(geom = \"rect\", xmin = lubridate::dmy_hms(011222000000), xmax = lubridate::dmy_hms(010223000000), ymin =Inf, ymax = Inf, fill = \"red\", alpha = .2 )+\n  scale_x_datetime(breaks = \"month\", labels = scales::label_date_short())+\n  theme_bw(base_size = 14)+\n  theme(axis.title.x = element_blank())\n\n\n\nFIGURE 18.1. Line plot dissolved oxygen, pH and temperature over a period\n\n\n\n\nAnother technique is to use a heat map or density plot to visualize the distribution of the data. This can be particularly useful when dealing with large datasets.\n\nCodeph = data |&gt;\n  filter(variables == \"pH\") |&gt;\n  mutate(hour = lubridate::hour(date), date = lubridate::as_date(date)) |&gt;\n  filter(date &lt; dmy(010423))|&gt;\n  ggplot(aes(x = date, y = hour, z = data)) +\n  metR::geom_contour_fill()+\n  metR::geom_contour2(aes(label = ..level..), breaks = 8.05, skip = 10)+\n  scale_fill_gradientn(colours = oce::oce.colors9A(120), trans = scales::modulus_trans(p = 3), name = \"pH\")+\n  scale_y_reverse(breaks = seq(0,24,2), expand = c(0, NA), name = \"Hours\")+\n  scale_x_date(date_breaks = \"10 day\", labels = scales::label_date_short(), expand = c(0, NA))+\n  theme_bw(base_size = 14)+\n  theme(axis.title.x = element_blank())\n\nph\n\n\n\nFIGURE 18.2. Hovmoller diagram of hour variation of pH over a period\n\n\n\n\nIt is also important to consider the scale of the visualization when dealing with high frequency data. Choosing an appropriate scale can help to highlight important features of the data while avoiding visual clutter.\n\nCodetemp = data |&gt;\n  filter(variables == \"Temperature\") |&gt;\n  mutate(hour = lubridate::hour(date), date = lubridate::as_date(date)) |&gt;\n  filter(date &lt; dmy(010423)) |&gt;\n  ggplot(aes(x = date, y = hour, z = data)) +\n  metR::geom_contour_fill()+  \n  metR::geom_contour2(aes(label = ..level..), breaks = 28, skip = 0)+\n  scale_fill_gradientn(colours = my.colors2(120), trans = scales::modulus_trans(p = 0.001), name = \"SST\")+\n  scale_y_reverse(breaks = seq(0,24,2), expand = c(0, NA), name = \"Hours\")+\n  scale_x_date(date_breaks = \"10 day\", labels = scales::label_date_short(), expand = c(0, NA))+\n  theme_bw(base_size = 14)+\n  theme(axis.title.x = element_blank())\n\ntemp\n\n\n\nFIGURE 18.3. Hovmoller diagram of hour variation of temperature over a period\n\n\n\n\n\nCodedo = data |&gt;\n  filter(variables == \"Dissolved Oxygen\") |&gt;\n  mutate(hour = lubridate::hour(date), date = lubridate::as_date(date)) |&gt;\n  filter(date &lt; dmy(010423) & hour &gt;7 & hour &lt; 21) |&gt;\n  ggplot(aes(x = date, y = hour, z = data)) +\n  metR::geom_contour_fill()+  \n  metR::geom_contour2(aes(label = ..level..), breaks = 28, skip = 0)+\n  scale_fill_gradientn(colours = recolor, trans = scales::modulus_trans(p = 2.1), name = \"DO\")+\n  scale_y_reverse(breaks = seq(0,24,2), expand = c(0, NA), name = \"Hours\")+\n  scale_x_date(date_breaks = \"10 day\", labels = scales::label_date_short(), expand = c(0, NA))+\n  theme_bw(base_size = 14)+\n  theme(axis.title.x = element_blank())\n\ndo\n\n\n\nFIGURE 18.4. Hovmoller diagram of hour variation of oxygen over a period\n\n\n\n\nOverall, the importance of visualizing high frequency data after tidying cannot be overstated, as it is a critical component of gaining insights and making informed decisions. To see the pattern of the three variables, we can use the patchwork package, which provides a simple and flexible way to arrange and combine multiple plots into a single layout. The package allows to arrange the plots horizontally or vertically using the + operator or the / operators, respectively. For example, here I arranged the heatmap of ph, temperature and dissolved oxygen vertically. T that I have just plotted w\nHere’s a step-by-step guide on how to use patchwork\n\nCode(ph + theme(axis.text.x = element_blank())) / \n  (temp + theme(axis.text.x = element_blank())) / \n  do\n\n# ggsave(\"f:/2023/Oceanography_soaf/figures/moa.png\", width = 8, height = 6, dpi = 300)\n\n\n\nFIGURE 18.5. Hovmoller diagram of hour variation of pH, temperature, and dissolved oxygen over a period"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bache, Stefan Milton, and Hadley Wickham. 2022. Magrittr: A\nForward-Pipe Operator for r. https://CRAN.R-project.org/package=magrittr.\n\n\nMüller, Kirill, and Hadley Wickham. 2022. Tibble: Simple Data\nFrames. https://CRAN.R-project.org/package=tibble.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data\nAnalysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\n———. 2019. Stringr: Simple, Consistent Wrappers for Common String\nOperations. https://CRAN.R-project.org/package=stringr.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the tidyverse.”\nJournal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, and Jennifer Bryan. 2022. Readxl: Read Excel\nFiles. https://CRAN.R-project.org/package=readxl.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2022.\nDplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, and Maximilian Girlich. 2022. Tidyr: Tidy Messy\nData. https://CRAN.R-project.org/package=tidyr.\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2022. Readr: Read\nRectangular Text Data. https://CRAN.R-project.org/package=readr."
  }
]